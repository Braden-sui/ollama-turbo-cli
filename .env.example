# Ollama Turbo Configuration
OLLAMA_API_KEY=your_api_key_here_from_ollama_settings_keys
OLLAMA_MODEL=gpt-oss:120b
OLLAMA_HOST=https://ollama.com

# Optional Configuration
MAX_CONVERSATION_HISTORY=10
RETRY_ATTEMPTS=3
STREAM_BY_DEFAULT=true
LOG_LEVEL=INFO

# Mem0 Configuration (optional but recommended)
# Set MEM0_API_KEY to enable long-term memory
MEM0_API_KEY=YOUR-MEM0-API
MEM0_USER_ID=Your-User-ID
MEM0_AGENT_ID=ollama-turbo
MEM0_APP_ID=ollama-cli
# Preferred Mem0 API version (default: v2; falls back automatically if unsupported)
MEM0_VERSION=v2
# Advanced (optional)
# MEM0_ORG_ID=
# MEM0_PROJECT_ID=

# Mem0 Runtime Knobs
# Enable/disable Mem0 integration (default: 1)
MEM0_ENABLED=1
# Debug logs for Mem0 timings/events (default: 0)
MEM0_DEBUG=0
# Max number of hits to inject per turn (default: 3)
MEM0_MAX_HITS=3
# Hard timeout for pre-query search in ms (default: 200)
MEM0_SEARCH_TIMEOUT_MS=200
# HTTP timeouts in ms (best-effort; SDK-dependent)
MEM0_TIMEOUT_CONNECT_MS=1000
MEM0_TIMEOUT_READ_MS=2000
# Background add worker queue size (default: 256)
MEM0_ADD_QUEUE_MAX=256
# Circuit breaker: failures to trip and cooldown in ms
MEM0_BREAKER_THRESHOLD=3
MEM0_BREAKER_COOLDOWN_MS=60000
# Graceful shutdown flush timeout in ms
MEM0_SHUTDOWN_FLUSH_MS=3000
# Tiny thread pool for Mem0 search workers
MEM0_SEARCH_WORKERS=2

# Sandbox execution
SHELL_TOOL_ALLOW=1
SHELL_TOOL_CONFIRM=1
SHELL_TOOL_ALLOWLIST=git status,git log,git diff,ls,dir,cat,type,python -V,python --version,pip list
SHELL_TOOL_MAX_OUTPUT=131072
SHELL_TOOL_ROOT=/ABSOLUTE/PATH/TO/PROJECT

# Web access
SANDBOX_NET=allowlist
SANDBOX_NET_ALLOW=api.mem0.ai,*.wikipedia.org,api.github.com,*.githubusercontent.com,pypi.org,files.pythonhosted.org
SANDBOX_BLOCK_PRIVATE_IPS=1
SANDBOX_ALLOW_HTTP=1
SANDBOX_MAX_DOWNLOAD_MB=5
SANDBOX_RATE_PER_HOST=12
SANDBOX_HTTP_CACHE_TTL_S=600
SANDBOX_HTTP_CACHE_MB=200
SANDBOX_NET_MODE=proxy

# Global tool â†’ LLM injection cap
TOOL_CONTEXT_MAX_CHARS=12000

