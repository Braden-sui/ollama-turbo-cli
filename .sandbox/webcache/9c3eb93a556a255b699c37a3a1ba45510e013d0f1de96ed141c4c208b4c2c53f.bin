<!DOCTYPE html><html lang="en" class="__className_9a6492 __variable_9a6492 __variable_3c557b __variable_ff0bc9 __variable_a59715 ssr-theme-applied light" data-initial-theme="system" data-theme="light" style="color-scheme: light; --theme: &quot;light&quot;;"><head><style><!----> <!--?lit$3861756153$-->.osano-cm-window{font-smooth:always;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothingz:auto;--fade-transition-time:700ms;--slide-transition-time:400ms;display:block;font-family:Helvetica,Arial,Hiragino Sans GB,STXihei,Microsoft YaHei,WenQuanYi Micro Hei,Hind,MS Gothic,Apple SD Gothic Neo,NanumBarunGothic,sans-serif;font-size:16px;left:0;line-height:1;position:absolute;top:0;width:100%;z-index:2147483638}.osano-cm-window--context_amp{height:100%}.osano-visually-hidden{height:1px;left:-10000px;margin:-1px;opacity:0;overflow:hidden;position:absolute;width:1px}.osano-cm-button{border-radius:.25em;border-style:solid;border-width:thin;cursor:pointer;flex:1 1 auto;font-size:1em;font-weight:700;line-height:1;margin:.125em;min-width:6em;padding:.5em .75em;transition-duration:.2s;transition-property:background-color;transition-timing-function:ease-out}.osano-cm-button--type_icon{border-radius:50%;height:1em;line-height:0;min-width:1em;width:1em}.osano-cm-button:focus,.osano-cm-button:hover{outline:none}.osano-cm-close{stroke-width:1px;border-radius:50%;border-style:solid;border-width:2px;box-sizing:initial;cursor:pointer;height:20px;justify-content:center;line-height:normal;margin:.5em;min-height:20px;min-width:20px;order:0;outline:none;overflow:hidden;padding:0;text-decoration:none;transform:rotate(0deg);transition-duration:.2s;transition-property:transform,color,background-color,stroke,stroke-width;transition-timing-function:ease-out;width:20px;z-index:2}.osano-cm-close:focus,.osano-cm-close:hover{stroke-width:2px;transform:rotate(90deg)}.ccpa-opt-out-icon{display:flex;flex:1 1 auto}.ccpa-opt-out-icon svg{max-width:40px}.osano-cm-link{cursor:pointer;text-decoration:underline;transition-duration:.2s;transition-property:color;transition-timing-function:ease-out}.osano-cm-link:active,.osano-cm-link:hover{outline:none}.osano-cm-link:focus{font-weight:700;outline:none}.osano-cm-link--type_feature,.osano-cm-link--type_purpose,.osano-cm-link--type_specialFeature,.osano-cm-link--type_specialPurpose{cursor:help;display:block;-webkit-text-decoration:dashed;text-decoration:dashed}.osano-cm-link--type_denyAll{display:block;text-align:right}[dir=rtl] .osano-cm-link--type_denyAll{text-align:left}.osano-cm-link--type_vendor{display:block}.osano-cm-vendor-link{font-size:.75em}.osano-cm-list-item{margin:0}.osano-cm-list-item--type_term{border-top-style:solid;border-top-width:1px;font-size:.875rem;font-weight:400;margin-bottom:.25em;margin-top:.5em;padding:.5em .75rem 0;position:relative;top:-1px}.osano-cm-list-item--type_description{font-size:.75rem;font-weight:lighter;padding:0 .75rem}.osano-cm-list{list-style-position:outside;list-style-type:none;margin:0;padding:0}.osano-cm-list__list-item{text-indent:0}.osano-cm-list--type_description{margin:0 -1em}.osano-cm-list:first-of-type .osano-cm-list__list-item:first-of-type{border-top-width:0;margin-top:0;padding-top:0}.osano-cm-toggle{align-items:center;display:flex;flex-direction:row-reverse;justify-content:flex-start;margin:.25em 0;pointer-events:auto;position:relative}.osano-cm-toggle__label{margin:0 .5em 0 0}[dir=rtl] .osano-cm-toggle__label{margin:0 0 0 .5em}.osano-cm-toggle__switch{border-radius:14px;border-style:solid;border-width:2px;box-sizing:initial;color:#0000;display:block;flex-shrink:0;height:18px;line-height:0;margin:0;position:relative;text-indent:-9999px;transition-duration:.2s;transition-property:background-color;transition-timing-function:ease-out;width:40px}.osano-cm-toggle__switch:hover{cursor:pointer}.osano-cm-toggle__switch:after{border-radius:9px;border-width:0;height:18px;left:0;top:0;width:18px}.osano-cm-toggle__switch:before{border-radius:16px;border-width:2px;bottom:-6px;box-sizing:border-box;left:-6px;right:-6px;top:-6px}.osano-cm-toggle__switch:after,.osano-cm-toggle__switch:before{border-style:solid;content:"";margin:0;position:absolute;transform:translateX(0);transition-duration:.3s;transition-property:transform,left,border-color;transition-timing-function:ease-out}.osano-cm-toggle__switch:after:active,.osano-cm-toggle__switch:before:active{transition-duration:.1s}.osano-cm-toggle__switch:after:active{width:26px}.osano-cm-toggle__switch:before:active{width:34px}[dir=rtl] .osano-cm-toggle__switch:after{left:100%;transform:translateX(-100%)}.osano-cm-toggle__input{height:1px;left:-10000px;margin:-1px;opacity:0;overflow:hidden;position:absolute;width:1px}[dir=rtl] .osano-cm-toggle__input{left:0;right:-10000px}.osano-cm-toggle__input:disabled{cursor:default}.osano-cm-toggle--type_checkbox .osano-cm-toggle__switch{border-radius:4px;border-style:solid;border-width:1px;height:22px;width:22px}.osano-cm-toggle--type_checkbox .osano-cm-toggle__switch:after{background-color:#0000!important;border-bottom-width:2px;border-left-width:2px;border-radius:0;content:none;height:6px;left:3px;top:3px;transform:rotate(-45deg);transition-property:color;transition-timing-function:ease-out;width:12px}.osano-cm-toggle--type_opt-out .osano-cm-toggle__switch{border-radius:4px;border-style:solid;border-width:1px;height:22px;width:22px}.osano-cm-toggle--type_opt-out .osano-cm-toggle__switch:after,.osano-cm-toggle--type_opt-out .osano-cm-toggle__switch:before{background-color:#0000!important;border-bottom-width:1px;border-radius:0;border-top-width:1px;content:none;height:0;left:-3px;top:7px;transition-property:color;transition-timing-function:ease-out;width:12px}.osano-cm-toggle--type_opt-out .osano-cm-toggle__switch:after{transform:translate(50%,50%) rotate(-45deg)}.osano-cm-toggle--type_opt-out .osano-cm-toggle__switch:before{transform:translate(50%,50%) rotate(45deg)}.osano-cm-toggle__input:checked+.osano-cm-toggle__switch:after{left:100%;transform:translateX(-100%)}[dir=rtl] .osano-cm-toggle__input:checked+.osano-cm-toggle__switch:after{left:0;transform:translateX(0)}.osano-cm-toggle__input:disabled+.osano-cm-toggle__switch{cursor:default}.osano-cm-toggle--type_checkbox .osano-cm-toggle__input:checked+.osano-cm-toggle__switch:after{content:"";left:3px;top:3px;transform:rotate(-45deg)}.osano-cm-toggle--type_opt-out .osano-cm-toggle__input:checked+.osano-cm-toggle__switch:after,.osano-cm-toggle--type_opt-out .osano-cm-toggle__input:checked+.osano-cm-toggle__switch:before{content:"";left:-1px;top:9px}.osano-cm-toggle--type_opt-out .osano-cm-toggle__input:checked+.osano-cm-toggle__switch:after{transform:translate(50%,50%) rotate(-45deg)}.osano-cm-toggle--type_opt-out .osano-cm-toggle__input:checked+.osano-cm-toggle__switch:before{transform:translate(50%,50%) rotate(45deg)}.osano-cm-toggle--type_checkbox .osano-cm-toggle__input:disabled+.osano-cm-toggle__switch,.osano-cm-toggle--type_opt-out .osano-cm-toggle__input:disabled+.osano-cm-toggle__switch{opacity:.3}.osano-cm-widget{background:none;border:none;bottom:12px;cursor:pointer;height:40px;opacity:.9;outline:none;padding:0;position:fixed;transition:transform .1s linear 0s,opacity .4s linear 0ms,visibility 0ms linear 0ms;visibility:visible;width:40px;z-index:2147483636}.osano-cm-widget--position_right{right:12px}.osano-cm-widget--position_left{left:12px}.osano-cm-widget:focus{outline:solid;outline-offset:.2rem}.osano-cm-widget:focus,.osano-cm-widget:hover{opacity:1;transform:scale(1.1)}.osano-cm-widget--hidden{opacity:0;transition-delay:0ms,0ms,.4s;visibility:hidden}.osano-cm-widget--hidden:focus,.osano-cm-widget--hidden:hover{opacity:0;transform:scale(1)}.osano-cm-dialog{align-items:center;box-sizing:border-box;font-size:1em;line-height:1.25;overflow:auto;padding:1.5em;position:fixed;transition-delay:0ms,0ms;transition-duration:.7s,0ms;transition-property:opacity,visibility;visibility:visible;z-index:2147483637}.osano-cm-dialog--hidden{opacity:0;transition-delay:0ms,.7s;visibility:hidden}.osano-cm-dialog--type_bar{box-sizing:border-box;display:flex;flex-direction:column;left:0;right:0}.osano-cm-dialog--type_bar .osano-cm-button{flex:none;margin:.125em auto;width:80%}@media screen and (min-width:768px){.osano-cm-dialog--type_bar{flex-direction:row}.osano-cm-dialog--type_bar .osano-cm-button{flex:1 1 100%;margin:.25em .5em;width:auto}}.osano-cm-dialog--type_box{flex-direction:column;max-height:calc(100vh - 2em);max-width:20em;width:calc(100vw - 2em)}.osano-cm-dialog__close{position:absolute;right:0;top:0}.osano-cm-dialog__list{margin:.5em 0 0;padding:0}.osano-cm-dialog__list .osano-cm-item{display:flex;margin-top:0}.osano-cm-dialog__list .osano-cm-item:last-child{margin-bottom:0}.osano-cm-dialog__list .osano-cm-toggle{flex-direction:row}[dir=rtl] .osano-cm-dialog__list .osano-cm-toggle{flex-direction:row-reverse}.osano-cm-dialog__list .osano-cm-label{white-space:nowrap}[dir=ltr] .osano-cm-dialog__list .osano-cm-label{margin-left:.375em}[dir=rtl] .osano-cm-dialog__list .osano-cm-label{margin-right:.375em}.osano-cm-dialog__buttons{display:flex;flex-wrap:wrap}.osano-cm-dialog--type_bar .osano-cm-dialog__content{flex:5;margin-bottom:.25em;width:100%}@media screen and (min-width:768px){.osano-cm-dialog--type_bar .osano-cm-dialog__content{max-height:30vh}}.osano-cm-dialog--type_box .osano-cm-dialog__content{display:flex;flex-direction:column;flex-grow:.0001;transition:flex-grow 1s linear}.osano-cm-dialog--type_bar .osano-cm-dialog__list{display:flex;flex-direction:column;flex-wrap:wrap;justify-content:flex-start;margin:.75em auto}@media screen and (min-width:376px){.osano-cm-dialog--type_bar .osano-cm-dialog__list{flex-direction:row}}@media screen and (min-width:768px){.osano-cm-dialog--type_bar .osano-cm-dialog__list{margin:.5em 0 0 auto}[dir=rtl] .osano-cm-dialog--type_bar .osano-cm-dialog__list{margin:.5em auto 0 0}}[dir=ltr] .osano-cm-dialog--type_bar .osano-cm-dialog__list .osano-cm-item{margin-right:.5em}[dir=rtl] .osano-cm-dialog--type_bar .osano-cm-dialog__list .osano-cm-item{margin-left:.5em}.osano-cm-dialog--type_bar .osano-cm-dialog__list .osano-cm-label{padding-top:0}.osano-cm-dialog--type_bar .osano-cm-dialog__buttons{flex:1;justify-content:flex-end;margin:0;width:100%}@media screen and (min-width:768px){.osano-cm-dialog--type_bar .osano-cm-dialog__buttons{margin:0 0 0 .5em;max-width:30vw;min-width:16em;position:-webkit-sticky;position:sticky;top:0;width:auto}[dir=rtl] .osano-cm-dialog--type_bar .osano-cm-dialog__buttons{margin:0 .5em 0 0}}.osano-cm-dialog--type_box .osano-cm-dialog__buttons{margin:.5em 0 0}.osano-cm-dialog--type_bar.osano-cm-dialog--position_top{top:0}.osano-cm-dialog--type_bar.osano-cm-dialog--position_bottom{bottom:0}.osano-cm-dialog--type_box.osano-cm-dialog--position_top-left{left:1em;top:1em}.osano-cm-dialog--type_box.osano-cm-dialog--position_top-right{right:1em;top:1em}.osano-cm-dialog--type_box.osano-cm-dialog--position_bottom-left{bottom:1em;left:1em}.osano-cm-dialog--type_box.osano-cm-dialog--position_bottom-right{bottom:1em;right:1em}.osano-cm-dialog--type_box.osano-cm-dialog--position_center{left:50%;top:50%;transform:translate(-50%,-50%)}.osano-cm-dialog--context_amp{height:100%;position:relative}.osano-cm-content__message{margin-bottom:1em;padding-bottom:1.5em;word-break:break-word}.osano-cm-drawer-links{margin:.5em 0 0}.osano-cm-drawer-links__link{display:block}.osano-cm-storage-policy{display:inline-block}.osano-cm-usage-list__list{list-style-position:inside;list-style-type:disc}:export{fadeTransitionTime:.7s;slideTransitionTime:.4s}.osano-cm-info-dialog{height:100vh;left:0;position:fixed;top:0;transition-delay:0ms,0ms;transition-duration:.2s,0ms;transition-property:opacity,visibility;visibility:visible;width:100vw;z-index:2147483638}.osano-cm-info-dialog--hidden{opacity:0;transition-delay:0ms,.2s;visibility:hidden}.osano-cm-header{margin:0 0 -1em;padding:1em 0;position:-webkit-sticky;position:sticky;top:0;z-index:1}.osano-cm-info{animation:delay-overflow .4s;bottom:0;box-shadow:0 0 2px 2px #ccc;box-sizing:border-box;max-width:20em;overflow-x:visible;overflow-y:visible;position:fixed;top:0;transition-duration:.4s;transition-property:transform;width:100%}.osano-cm-info--position_left{left:0;transform:translate(-100%)}.osano-cm-info--position_right{right:0;transform:translate(100%)}.osano-cm-info--open{animation:none;overflow-x:hidden;overflow-y:auto;transform:translate(0)}.osano-cm-info--do_not_sell{animation:none;height:-webkit-fit-content;height:-moz-fit-content;height:fit-content;left:50%;position:fixed;right:auto;top:50%;transform:translate(-50%,-50%);transition:none}.osano-cm-info--do_not_sell .osano-cm-close{order:-1}.osano-cm-info--do_not_sell .osano-cm-header{box-sizing:initial;display:block;flex:none}.osano-cm-info-views{align-items:flex-start;display:flex;flex-direction:row;flex-wrap:nowrap;height:100%;transition-duration:.4s;transition-property:transform;width:100%}[dir=rtl] .osano-cm-info-views{flex-direction:row-reverse}.osano-cm-info-views__view{box-sizing:border-box;flex-shrink:0;width:100%}.osano-cm-info-views--position_0>:not(:first-of-type){max-height:100%;overflow:hidden}.osano-cm-info-views--position_1{transform:translateX(-100%)}.osano-cm-info-views--position_1>:not(:nth-of-type(2)){max-height:100%;overflow:hidden}.osano-cm-info-views--position_2{transform:translateX(-200%)}.osano-cm-info-views--position_2>:not(:nth-of-type(3)){max-height:100%;overflow:hidden}.osano-cm-info--do_not_sell .osano-cm-info-views{height:-webkit-fit-content;height:-moz-fit-content;height:fit-content}.osano-cm-view{height:0;padding:0 .75em 1em;transition-delay:.4s;transition-duration:0ms;transition-property:height,visibility;visibility:hidden;width:100%}.osano-cm-view__button{font-size:.875em;margin:1em 0 0;width:100%}.osano-cm-view--active{height:auto;transition-delay:0ms;visibility:visible}.osano-cm-description{font-size:.75em;font-weight:300;line-height:1.375;margin:1em 0 0}.osano-cm-description:first-child{margin:0}.osano-cm-description:last-of-type{margin-bottom:1em}.osano-cm-drawer-toggle .osano-cm-label{font-size:.875em;line-height:1.375em;margin:0 auto 0 0}[dir=rtl] .osano-cm-drawer-toggle .osano-cm-label{margin:0 0 0 auto}.osano-cm-info-dialog-header{align-items:center;display:flex;flex-direction:row-reverse;left:auto;min-height:3.25em;position:-webkit-sticky;position:sticky;top:0;width:100%;z-index:1}[dir=rtl] .osano-cm-info-dialog-header{flex-direction:row}.osano-cm-info-dialog-header__header{align-items:center;display:flex;flex:1 1 auto;font-size:1em;justify-content:flex-start;margin:0;order:1;padding:1em .75em}.osano-cm-info-dialog-header__description{font-size:.75em;line-height:1.375}.osano-cm-back,.osano-cm-info-dialog-header__close{position:relative}.osano-cm-back{flex:0 1 auto;margin:0 0 0 .5em;min-width:0;order:2;width:auto;z-index:2}[dir=rtl] .osano-cm-back{margin:0 .5em 0 0}.osano-cm-powered-by{align-items:center;display:flex;flex-direction:column;font-weight:700;justify-content:center;margin:1em 0}.osano-cm-powered-by__link{font-size:.625em;outline:none;text-decoration:none}.osano-cm-powered-by__link:focus,.osano-cm-powered-by__link:hover{text-decoration:underline}@keyframes delay-overflow{0%{overflow-x:hidden;overflow-y:auto}}.osano-cm-drawer-iab-button-container{display:flex;gap:.5em;justify-content:center;margin-bottom:2em}.osano-cm-illustrations__list>.osano-cm-list-item--type_description{padding:.2rem 1rem}.osano-cm-expansion-panel{border-bottom:1px solid #0000001a;display:block;font-size:.75em;margin:0 -1.5em 1em;padding:1.5em 1.5em 0}.osano-cm-expansion-panel--expanded{border-bottom:none}.osano-cm-expansion-panel--empty,.osano-cm-expansion-panel--empty:not([open]){border-bottom:1px solid #0000001a;padding-bottom:0}.osano-cm-expansion-panel__body{background-color:#0000001a;line-height:1.25;list-style:none;margin:0 -1.5em;max-height:0;overflow:hidden;padding:0 1.5em;transition-delay:0ms,0ms,0ms,.3s;transition-duration:.3s,.3s,.3s,0s;transition-property:max-height,padding-top,padding-bottom,visibility;transition-timing-function:ease-out;visibility:hidden}.osano-cm-expansion-panel__toggle{cursor:pointer;display:block;line-height:1.25;margin:0 auto 1em 0;outline:none;position:relative}.osano-cm-expansion-panel__toggle:active,.osano-cm-expansion-panel__toggle:focus,.osano-cm-expansion-panel__toggle:hover{outline:none}[dir=rtl] .osano-cm-expansion-panel__toggle{margin:0 0 1em auto}.osano-cm-expansion-panel--expanded .osano-cm-expansion-panel__body{max-height:none;padding:1.25em 1.5em 1em;transition-delay:0ms,0ms,0ms,0ms;visibility:visible}.osano-cm-cookie-disclosure__title,.osano-cm-script-disclosure__title{border:0;clear:both;display:block;flex:0 1 30%;font-size:1em;font-weight:700;line-height:1.375;margin:0 0 .5em;padding:0}.osano-cm-cookie-disclosure__description,.osano-cm-script-disclosure__description{flex:0 1 70%;font-size:1em;line-height:1.375;margin:0 0 .5em;padding:0}.osano-cm-disclosure{border-bottom:none;display:block;font-size:.75em;margin:0 -1.5em 1em;padding:1.5em 1.5em 0}.osano-cm-disclosure--collapse{border-bottom:1px solid #0000001a;padding-bottom:1em}.osano-cm-disclosure--empty,.osano-cm-disclosure--empty:not([open]){border-bottom:1px solid #0000001a;padding-bottom:0}.osano-cm-disclosure__list{background-color:#0000001a;line-height:1.25;list-style:none;margin:0 -1.5em;padding:1.25em 1.5em 1em}.osano-cm-disclosure__list:empty{border:none;padding:0 1.5em}.osano-cm-disclosure__list:first-of-type{margin-top:1em;padding:1.25em 1.5em 1em}.osano-cm-disclosure__list:first-of-type:empty{padding:1.75em 1.5em .75em}.osano-cm-disclosure__list:not(:first-of-type):not(:empty){border-top:1px solid #0000001a}.osano-cm-disclosure__list:empty+.osano-cm-disclosure__list:not(:empty){border:none;padding:0 1.5em}.osano-cm-disclosure__list:not(:empty)~.osano-cm-disclosure__list:empty+.osano-cm-disclosure__list:not(:empty){border-top:1px solid #0000001a}.osano-cm-disclosure__list>.osano-cm-list-item{line-height:1.25}.osano-cm-disclosure__list>.osano-cm-list-item:not(:first-of-type){border-top:1px solid #0000001a;margin:1em -1.25em 0;padding:1em 1.25em 0}.osano-cm-disclosure__toggle{cursor:pointer;display:block;font-weight:700;line-height:1.25;margin:0 auto 0 0;outline:none;position:relative}.osano-cm-disclosure__toggle:focus,.osano-cm-disclosure__toggle:hover{text-decoration:underline}[dir=rtl] .osano-cm-disclosure__toggle{margin:0 0 0 auto}.osano-cm-disclosure--loading .osano-cm-disclosure__list{height:0;line-height:0;max-height:0}.osano-cm-disclosure--loading .osano-cm-disclosure__list>*{display:none}.osano-cm-disclosure--loading .osano-cm-disclosure__list:after{animation-duration:1s;animation-iteration-count:infinite;animation-name:osano-load-scale;animation-timing-function:ease-in-out;border-radius:100%;content:"";display:block;height:1em;position:relative;top:-.125em;transform:translateY(-50%);width:1em}.osano-cm-disclosure--collapse .osano-cm-disclosure__list{display:none}.osano-cm-disclosure--collapse .osano-cm-disclosure__list:after{content:none}.osano-cm-cookie-disclosure,.osano-cm-script-disclosure{display:flex;flex-wrap:wrap;margin:0}.osano-cm-cookie-disclosure__description:last-of-type,.osano-cm-cookie-disclosure__title:last-of-type,.osano-cm-script-disclosure__description:last-of-type,.osano-cm-script-disclosure__title:last-of-type{margin-bottom:0}@keyframes osano-load-scale{0%{transform:translateY(-50%) scale(0)}to{opacity:0;transform:translateY(-50%) scale(1)}} .osano-cm-window { direction: <!--?lit$3861756153$-->ltr; text-align: <!--?lit$3861756153$-->left; } .osano-cm-dialog { background: <!--?lit$3861756153$-->#ffffff; color: <!--?lit$3861756153$-->#000000; } .osano-cm-dialog__close { color: <!--?lit$3861756153$-->#000000; stroke: <!--?lit$3861756153$-->#000000; } .osano-cm-dialog__close:focus { background-color: <!--?lit$3861756153$-->#000000; border-color: <!--?lit$3861756153$-->#000000; stroke: <!--?lit$3861756153$-->#ffffff; } .osano-cm-dialog__close:hover { stroke: <!--?lit$3861756153$-->#141414; } .osano-cm-dialog__close:focus:hover { stroke: <!--?lit$3861756153$-->#ebebeb; } .osano-cm-info-dialog { background: <!--?lit$3861756153$-->rgba(0,0,0,0.45); } .osano-cm-header, .osano-cm-info-dialog-header { background: <!--?lit$3861756153$-->#fff; background: linear-gradient( 180deg, <!--?lit$3861756153$-->#fff 2.5em, <!--?lit$3861756153$-->rgba(255,255,255,0) 100% ); } .osano-cm-info { background: <!--?lit$3861756153$-->#fff; color: <!--?lit$3861756153$-->#000; } .osano-cm-link-separator::before { content: '|'; padding: 0 0.5em; } .osano-cm-close { display: flex; background-color: transparent; border-color: transparent; } .osano-cm-info-dialog-header__close { color: <!--?lit$3861756153$-->#000; stroke: <!--?lit$3861756153$-->#000; } .osano-cm-info-dialog-header__close:focus { background-color: <!--?lit$3861756153$-->#000; border-color: <!--?lit$3861756153$-->#000; stroke: <!--?lit$3861756153$-->#fff; } .osano-cm-info-dialog-header__close:hover { stroke: <!--?lit$3861756153$-->#141414; } .osano-cm-info-dialog-header__close:focus:hover { stroke: <!--?lit$3861756153$-->#ebebeb; } .osano-cm-disclosure__list:first-of-type::after { background-color: <!--?lit$3861756153$-->#1c17ff; } .osano-cm-disclosure__toggle, .osano-cm-expansion-panel__toggle { color: <!--?lit$3861756153$-->#1c17ff; } .osano-cm-disclosure__toggle:hover, .osano-cm-disclosure__toggle:active, .osano-cm-expansion-panel__toggle:hover, .osano-cm-expansion-panel__toggle:active { color: <!--?lit$3861756153$-->#1c17ff; } .osano-cm-disclosure__toggle:focus, .osano-cm-expansion-panel__toggle:focus { color: <!--?lit$3861756153$-->#302bff; } .osano-cm-button { background-color: <!--?lit$3861756153$-->#1c17ff; border-color: <!--?lit$3861756153$-->#ffffff; color: <!--?lit$3861756153$-->#ffffff; } .osano-cm-button--type_deny { background-color: <!--?lit$3861756153$-->#989; border-color: <!--?lit$3861756153$-->#fff; color: <!--?lit$3861756153$-->#fff; } .osano-cm-button:focus, .osano-cm-button:hover { background-color: <!--?lit$3861756153$-->#302bff; } .osano-cm-button--type_deny:focus, .osano-cm-button--type_deny:hover { background-color: <!--?lit$3861756153$-->#857485; } .osano-cm-link { color: <!--?lit$3861756153$-->#1c17ff; } .osano-cm-link:hover, .osano-cm-link:active { color: <!--?lit$3861756153$-->#1c17ff; } .osano-cm-link:focus { color: <!--?lit$3861756153$-->#302bff; } .osano-cm-toggle__switch { background-color: <!--?lit$3861756153$-->#d2cfff; } .osano-cm-toggle__switch::after { background-color: <!--?lit$3861756153$-->#ffffff; border-color: <!--?lit$3861756153$-->#ffffff; } .osano-cm-toggle__switch::before { border-color: transparent; } .osano-cm-toggle__input:checked + .osano-cm-toggle__switch { background-color: <!--?lit$3861756153$-->#1c17ff; border-color: <!--?lit$3861756153$-->#1c17ff; } .osano-cm-toggle__input:checked + .osano-cm-toggle__switch::after, .osano-cm-toggle__input:checked + .osano-cm-toggle__switch::before { border-color: <!--?lit$3861756153$-->#f4f4f4; } .osano-cm-toggle__input:focus + .osano-cm-toggle__switch, .osano-cm-toggle__input:hover + .osano-cm-toggle__switch { background-color: <!--?lit$3861756153$-->#bebbeb; border-color: <!--?lit$3861756153$-->#bebbeb; } .osano-cm-toggle__input:focus + .osano-cm-toggle__switch::before { border-color: <!--?lit$3861756153$-->#bebbeb; } .osano-cm-toggle__input:checked:focus + .osano-cm-toggle__switch, .osano-cm-toggle__input:checked:hover + .osano-cm-toggle__switch { background-color: <!--?lit$3861756153$-->#302bff; border-color: <!--?lit$3861756153$-->#302bff; } .osano-cm-toggle__input:checked:focus + .osano-cm-toggle__switch::before { border-color: <!--?lit$3861756153$-->#302bff; } .osano-cm-toggle__input:disabled + .osano-cm-toggle__switch, .osano-cm-toggle__input:disabled:focus + .osano-cm-toggle__switch, .osano-cm-toggle__input:disabled:hover + .osano-cm-toggle__switch { background-color: <!--?lit$3861756153$-->#928fbf; border-color: <!--?lit$3861756153$-->#928fbf; } .osano-cm-toggle__input:disabled + .osano-cm-toggle__switch::after, .osano-cm-toggle__input:disabled:focus + .osano-cm-toggle__switch::after, .osano-cm-toggle__input:disabled:hover + .osano-cm-toggle__switch::after { background-color: <!--?lit$3861756153$-->#bfbfbf; border-color: <!--?lit$3861756153$-->#bfbfbf; } .osano-cm-toggle__input:disabled + .osano-cm-toggle__switch::before, .osano-cm-toggle__input:disabled:focus + .osano-cm-toggle__switch::before, .osano-cm-toggle__input:disabled:hover + .osano-cm-toggle__switch::before { border-color: transparent; } .osano-cm-toggle__input:disabled:checked + .osano-cm-toggle__switch, .osano-cm-toggle__input:disabled:checked:focus + .osano-cm-toggle__switch, .osano-cm-toggle__input:disabled:checked:hover + .osano-cm-toggle__switch { background-color: <!--?lit$3861756153$-->#5c57ff; border-color: <!--?lit$3861756153$-->#5c57ff; } .osano-cm-toggle__input:disabled:checked + .osano-cm-toggle__switch::after, .osano-cm-toggle__input:disabled:checked:focus + .osano-cm-toggle__switch::after, .osano-cm-toggle__input:disabled:checked:hover + .osano-cm-toggle__switch::after { background-color: <!--?lit$3861756153$-->#b4b4b4; border-color: <!--?lit$3861756153$-->#b4b4b4; } .osano-cm-toggle__input:disabled:checked + .osano-cm-toggle__switch::before, .osano-cm-toggle__input:disabled:checked:focus + .osano-cm-toggle__switch::before, .osano-cm-toggle__input:disabled:checked:hover + .osano-cm-toggle__switch::before { border-color: transparent; } .osano-cm-widget__outline { fill: <!--?lit$3861756153$-->#fff; stroke: <!--?lit$3861756153$-->#29246a; } .osano-cm-widget__dot { fill: <!--?lit$3861756153$-->#37cd8f; } </style><link rel="preload" as="image" imagesrcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=16&amp;q=100 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=32&amp;q=100 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=48&amp;q=100 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=64&amp;q=100 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=96&amp;q=100 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=128&amp;q=100 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=256&amp;q=100 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=384&amp;q=100 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=640&amp;q=100 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=750&amp;q=100 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=828&amp;q=100 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=1080&amp;q=100 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=1200&amp;q=100 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=1920&amp;q=100 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=2048&amp;q=100 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=3840&amp;q=100 3840w" imagesizes="202px" fetchpriority="high"><link rel="stylesheet" href="/_next/static/css/4be8eca88fd30eee.css" data-precedence="next"><link rel="stylesheet" href="/_next/static/css/ecdb34faee2e34d6.css" data-precedence="next"><link rel="stylesheet" href="/_next/static/css/13e7e6ab504d5b67.css" data-precedence="next"><link rel="preload" as="script" fetchpriority="low" href="/_next/static/chunks/webpack-da8dce98ca778df3.js"><script src="https://js.hsleadflows.net/leadflows.js" type="text/javascript" id="LeadFlows-8231564" crossorigin="anonymous" data-leadin-portal-id="8231564" data-leadin-env="prod" data-loader="hs-scriptloader" data-hsjs-portal="8231564" data-hsjs-env="prod" data-hsjs-hublet="na1"></script><script data-hsjs-hublet="na1" data-hsjs-env="prod" data-hsjs-portal="8231564" data-loader="hs-scriptloader" data-hs-ignore="true" data-cookieconsent="ignore" id="cookieBanner-8231564" type="text/javascript" src="https://js.hs-banner.com/v2/8231564/banner.js"></script><script src="https://js.hs-analytics.net/analytics/1755675300000/8231564.js" type="text/javascript" id="hs-analytics"></script><script data-hsjs-hublet="na1" data-hsjs-env="prod" data-hsjs-portal="8231564" data-loader="hs-scriptloader" data-ads-env="prod" data-ads-portal-id="8231564" id="hs-ads-pixel-8231564" type="text/javascript" src="https://js.hsadspixel.net/fb.js"></script><script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-5RCSPVG"></script><script type="osano/blocked" async=""></script><script src="/_next/static/chunks/fd9d1056-796b85023105fe1d.js" async=""></script><script src="/_next/static/chunks/2117-f76e68b5020c0509.js" async=""></script><script src="/_next/static/chunks/main-app-be3d940aef6bd3eb.js" async=""></script><script src="/_next/static/chunks/9da6db1e-0b0f4d5c93f833e0.js" async=""></script><script src="/_next/static/chunks/app/layout-5888a3b8110b9399.js" async=""></script><script src="/_next/static/chunks/52774a7f-9140b4e323efe0ba.js" async=""></script><script src="/_next/static/chunks/8667-3b0d6e3f612cdd6b.js" async=""></script><script src="/_next/static/chunks/7354-1d7870310d1419aa.js" async=""></script><script src="/_next/static/chunks/app/(frontend)/layout-ddd476d9ab06f775.js" async=""></script><script src="/_next/static/chunks/08ffe114-ffac5ea08ea3b971.js" async=""></script><script src="/_next/static/chunks/8dc5345f-e183b54149bccc09.js" async=""></script><script src="/_next/static/chunks/3627521c-c9e8a1c79edd7561.js" async=""></script><script src="/_next/static/chunks/d8f92815-98c12721cfa98058.js" async=""></script><script src="/_next/static/chunks/07115393-01c29098453ff257.js" async=""></script><script src="/_next/static/chunks/3204862b-d5e6e14a59dedbca.js" async=""></script><script src="/_next/static/chunks/2972-1470746df2c539e6.js" async=""></script><script src="/_next/static/chunks/9291-ed64be53c7295d4f.js" async=""></script><script src="/_next/static/chunks/8076-55c63a69d26d910d.js" async=""></script><script src="/_next/static/chunks/466-1359f016cbb61606.js" async=""></script><script src="/_next/static/chunks/1101-229bbbbd02dcbc67.js" async=""></script><script src="/_next/static/chunks/5757-36d92f2a990b34ca.js" async=""></script><script src="/_next/static/chunks/6951-4bc4103c43c657d3.js" async=""></script><script src="/_next/static/chunks/4715-00d7cff39ed87d8f.js" async=""></script><script src="/_next/static/chunks/9111-1485527d67ca5f72.js" async=""></script><script src="/_next/static/chunks/6591-75e707f3cacea3a0.js" async=""></script><script src="/_next/static/chunks/8098-38edba19209c5d5b.js" async=""></script><script src="/_next/static/chunks/9316-8148cc88da752ca5.js" async=""></script><script src="/_next/static/chunks/9948-7ca7d72282f7a143.js" async=""></script><script src="/_next/static/chunks/app/(frontend)/(shared-layout)/layout-bcb0bfbab41a9d1e.js" async=""></script><script src="/_next/static/chunks/6327-3a5eadbcfae2ffed.js" async=""></script><script src="/_next/static/chunks/9159-699bd0d299123ab9.js" async=""></script><script src="/_next/static/chunks/3617-d8a85a8d442778a6.js" async=""></script><script src="/_next/static/chunks/3335-40843fa911f88e9b.js" async=""></script><script src="/_next/static/chunks/4089-02ac20c12970a615.js" async=""></script><script src="/_next/static/chunks/45-4c9a51cfc08d6de3.js" async=""></script><script src="/_next/static/chunks/9339-8fe59c1ff0393aeb.js" async=""></script><script src="/_next/static/chunks/4853-48e50bc54d42e954.js" async=""></script><script src="/_next/static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js" async=""></script><script src="/_next/static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/page-9a6749d88c6c9981.js" async=""></script><link rel="preload" href="/scripts/consent-data-layer.js" as="script"><link rel="preload" href="https://cmp.osano.com/AzZbUITh61cXz4HqL/fee5a4ca-0585-49b2-b317-c6a8cf7b66fa/osano.js" as="script"><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=UA-155294302-1" as="script"><link rel="preload" href="https://js.hs-scripts.com/8231564.js" as="script"><link rel="preload" href="https://tag.clearbitscripts.com/v1/pk_5add668ab6de5cdc81322cba801cf429/tags.js" as="script"><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" nomodule=""></script><script id="consent-data-layer" src="/scripts/consent-data-layer.js"></script><script id="osano-cmp-script" src="https://cmp.osano.com/AzZbUITh61cXz4HqL/fee5a4ca-0585-49b2-b317-c6a8cf7b66fa/osano.js"></script><style type="text/css">.rfm-marquee-container {
  overflow-x: hidden;
  display: flex;
  flex-direction: row;
  position: relative;
  width: var(--width);
  transform: var(--transform);
}
.rfm-marquee-container:hover div {
  animation-play-state: var(--pause-on-hover);
}
.rfm-marquee-container:active div {
  animation-play-state: var(--pause-on-click);
}

.rfm-overlay {
  position: absolute;
  width: 100%;
  height: 100%;
}
.rfm-overlay::before, .rfm-overlay::after {
  background: linear-gradient(to right, var(--gradient-color), rgba(255, 255, 255, 0));
  content: "";
  height: 100%;
  position: absolute;
  width: var(--gradient-width);
  z-index: 2;
  pointer-events: none;
  touch-action: none;
}
.rfm-overlay::after {
  right: 0;
  top: 0;
  transform: rotateZ(180deg);
}
.rfm-overlay::before {
  left: 0;
  top: 0;
}

.rfm-marquee {
  flex: 0 0 auto;
  min-width: var(--min-width);
  z-index: 1;
  display: flex;
  flex-direction: row;
  align-items: center;
  animation: scroll var(--duration) linear var(--delay) var(--iteration-count);
  animation-play-state: var(--play);
  animation-delay: var(--delay);
  animation-direction: var(--direction);
}
@keyframes scroll {
  0% {
    transform: translateX(0%);
  }
  100% {
    transform: translateX(-100%);
  }
}

.rfm-initial-child-container {
  flex: 0 0 auto;
  display: flex;
  min-width: auto;
  flex-direction: row;
  align-items: center;
}

.rfm-child {
  transform: var(--transform);
}</style><script id="vwoCode">window._vwo_code || (function() {
            var account_id=758750,
            version=2.1,
            settings_tolerance=2000,
            hide_element='body',
            hide_element_style = 'opacity:0 !important;filter:alpha(opacity=0) !important;background:none !important',
            /* DO NOT EDIT BELOW THIS LINE */
            f=false,w=window,d=document,v=d.querySelector('#vwoCode'),cK='_vwo_'+account_id+'_settings',cc={};try{var c=JSON.parse(localStorage.getItem('_vwo_'+account_id+'_config'));cc=c&&typeof c==='object'?c:{}}catch(e){}var stT=cc.stT==='session'?w.sessionStorage:w.localStorage;code={use_existing_jquery:function(){return typeof use_existing_jquery!=='undefined'?use_existing_jquery:undefined},library_tolerance:function(){return typeof library_tolerance!=='undefined'?library_tolerance:undefined},settings_tolerance:function(){return cc.sT||settings_tolerance},hide_element_style:function(){return'{'+(cc.hES||hide_element_style)+'}'},hide_element:function(){if(performance.getEntriesByName('first-contentful-paint')[0]){return''}return typeof cc.hE==='string'?cc.hE:hide_element},getVersion:function(){return version},finish:function(e){if(!f){f=true;var t=d.getElementById('_vis_opt_path_hides');if(t)t.parentNode.removeChild(t);if(e)(new Image).src='https://dev.visualwebsiteoptimizer.com/ee.gif?a='+account_id+e}},finished:function(){return f},addScript:function(e){var t=d.createElement('script');t.type='text/javascript';if(e.src){t.src=e.src}else{t.text=e.text}d.getElementsByTagName('head')[0].appendChild(t)},load:function(e,t){var i=this.getSettings(),n=d.createElement('script'),r=this;t=t||{};if(i){n.textContent=i;d.getElementsByTagName('head')[0].appendChild(n);if(!w.VWO||VWO.caE){stT.removeItem(cK);r.load(e)}}else{var o=new XMLHttpRequest;o.open('GET',e,true);o.withCredentials=!t.dSC;o.responseType=t.responseType||'text';o.onload=function(){if(t.onloadCb){return t.onloadCb(o,e)}if(o.status===200){_vwo_code.addScript({text:o.responseText})}else{_vwo_code.finish('&e=loading_failure:'+e)}};o.onerror=function(){if(t.onerrorCb){return t.onerrorCb(e)}_vwo_code.finish('&e=loading_failure:'+e)};o.send()}},getSettings:function(){try{var e=stT.getItem(cK);if(!e){return}e=JSON.parse(e);if(Date.now()>e.e){stT.removeItem(cK);return}return e.s}catch(e){return}},init:function(){if(d.URL.indexOf('__vwo_disable__')>-1)return;var e=this.settings_tolerance();w._vwo_settings_timer=setTimeout(function(){_vwo_code.finish();stT.removeItem(cK)},e);var t;if(this.hide_element()!=='body'){t=d.createElement('style');var i=this.hide_element(),n=i?i+this.hide_element_style():'',r=d.getElementsByTagName('head')[0];t.setAttribute('id','_vis_opt_path_hides');v&&t.setAttribute('nonce',v.nonce);t.setAttribute('type','text/css');if(t.styleSheet)t.styleSheet.cssText=n;else t.appendChild(d.createTextNode(n));r.appendChild(t)}else{t=d.getElementsByTagName('head')[0];var n=d.createElement('div');n.style.cssText='z-index: 2147483647 !important;position: fixed !important;left: 0 !important;top: 0 !important;width: 100% !important;height: 100% !important;background: white !important;';n.setAttribute('id','_vis_opt_path_hides');n.classList.add('_vis_hide_layer');t.parentNode.insertBefore(n,t.nextSibling)}var o='https://dev.visualwebsiteoptimizer.com/j.php?a='+account_id+'&u='+encodeURIComponent(d.URL)+'&vn='+version;if(w.location.search.indexOf('_vwo_xhr')!==-1){this.addScript({src:o})}else{this.load(o+'&x=true')}}};w._vwo_code=code;code.init();})();</script><link rel="preload" href="/_next/static/media/2d50ea7139ed0078-s.p.woff2" as="font" crossorigin="" type="font/woff2"><link rel="preload" href="/_next/static/media/88203632e8260aeb-s.p.otf" as="font" crossorigin="" type="font/otf"><link rel="preload" href="/_next/static/media/9001b2c46b4f4e76-s.p.otf" as="font" crossorigin="" type="font/otf"><link rel="preload" href="/_next/static/media/bb3ef058b751a6ad-s.p.woff2" as="font" crossorigin="" type="font/woff2"><link rel="preload" href="https://i.ytimg.com/vi/Uh9bYiVrW_s/hqdefault.jpg" as="image"><script src="/_vercel/speed-insights/script.js" defer="" data-sdkn="@vercel/speed-insights/next" data-sdkv="1.1.0" data-sample-rate="0.4" data-route="/learn/series/[seriesSlug]/[chapterSlug]/"></script><script type="text/javascript">(function(){function _vwo_err(e){var vwo_e=new Image;vwo_e.src="https://dev.visualwebsiteoptimizer.com/e.gif?a=758750&s=j.php&e="+encodeURIComponent(e&&e.message&&e.message.substring(0,1e3)+"&vn=")}try{var extE=0;(function(){window._VWO=window._VWO||{};var aC=window._vwo_code;if(typeof aC==='undefined'){window._vwo_mt='dupCode';return;}if(window._VWO.sCL){window._vwo_mt='dupCode';window._VWO.sCDD=true;try{if(aC){clearTimeout(window._vwo_settings_timer);var h=document.querySelectorAll('#_vis_opt_path_hides');var x=h[h.length>1?1:0];x&&x.remove();}}catch(e){}return;}window._VWO.sCL=true;;window._vwo_mt="live";var localPreviewObject={};var previewKey="_vis_preview_"+758750;var wL=window.location;;try{localPreviewObject[previewKey]=window.localStorage.getItem(previewKey);JSON.parse(localPreviewObject[previewKey])}catch(e){localPreviewObject[previewKey]=""}try{window._vwo_tm="";var getMode=function(e){var n;if(window.name.indexOf(e)>-1){n=window.name}else{n=wL.search.match("_vwo_m=([^&]*)");n=n&&atob(decodeURIComponent(n[1]))}return n&&JSON.parse(n)};var ccMode=getMode("_vwo_cc");if(window.name.indexOf("_vis_heatmap")>-1||window.name.indexOf("_vis_editor")>-1||ccMode||window.name.indexOf("_vis_preview")>-1){try{if(ccMode){window._vwo_mt=decodeURIComponent(wL.search.match("_vwo_m=([^&]*)")[1])}else if(window.name&&JSON.parse(window.name)){window._vwo_mt=window.name}}catch(e){if(window._vwo_tm)window._vwo_mt=window._vwo_tm}}else if(window._vwo_tm.length){window._vwo_mt=window._vwo_tm}else if(location.href.indexOf("_vis_opt_preview_combination")!==-1&&location.href.indexOf("_vis_test_id")!==-1){window._vwo_mt="sharedPreview"}else if(localPreviewObject[previewKey]){window._vwo_mt=JSON.stringify(localPreviewObject)}if(window._vwo_mt!=="live"){if(typeof extE!=="undefined"){extE=1}if(!getMode("_vwo_cc")){_vwo_code.load('https://dev.visualwebsiteoptimizer.com/j.php?mode=' + encodeURIComponent(window._vwo_mt) + '&a=758750&f=1&u=' + encodeURIComponent(window._vis_opt_url||document.URL)+'&eventArch=true'+'&x=true', { sL: window._vwo_code.sL }); if(window._vwo_code.sL) { prevMode = true; }}else{(function(){window._vwo_code&&window._vwo_code.finish();_vwo_ccc={u:"/j.php?a=758750&u=https%3A%2F%2Fwww.pinecone.io%2Flearn%2Fseries%2Frag%2Frerankers%2F&vn=2.1&x=true"};var s=document.createElement("script");s.src="https://app.vwo.com/visitor-behavior-analysis/dist/codechecker/cc.min.js?r="+Math.random();document.head.appendChild(s)})()}}}catch(e){var vwo_e=new Image;vwo_e.src="https://dev.visualwebsiteoptimizer.com/ee.gif?s=mode_det&e="+encodeURIComponent(e&&e.stack&&e.stack.substring(0,1e3)+"&vn=");aC&&window._vwo_code.finish()}})();
;window._vwo_cookieDomain="pinecone.io";if(window._vwo_mt === "live"){window.VWO=window.VWO||[];window._vwo_acc_id=758750;window.VWO._=window.VWO._||{};;const shouldSampleApm=false;if(shouldSampleApm){VWO.load_co=function(a){var b=document.createElement("script");b.src=a;b.crossOrigin="anonymous";b.type="text/javascript";b.fetchPriority="high";b.innerText;b.onerror=function(){_vwo_code.finish()};document.getElementsByTagName("head")[0].appendChild(b)};setTimeout((function(){VWO.load_co("https://dev.visualwebsiteoptimizer.com/cdn/apmLib-7716d5be4a4d111d8af5b28be8ff864cbr.js")}),0)}VWO._=VWO._||{};var visMeta=VWO._.visMeta;VWO._.allSettings=function(){return{dataStore:{events:{"indexCreated":{},"signup":{}}}}}();window.VWO.visUuid="D569C4EC0EF431E0F5BA22736DC16D99D|0fa3f060e845b8b3fe37eae1481d91bd";
;_vwo_code.sT=_vwo_code.finished();(function(c,a,e,d,b,z,g){if(window.VWO._&&window.VWO._.isBot)return;const cookiePrefix=window._vwoCc&&window._vwoCc.cookiePrefix||"";function f(a,b,d){var cookieName=cookiePrefix+a;e.cookie=cookieName+"="+b+"; expires="+new Date(864e5*d+ +new Date).toGMTString()+"; domain="+g+"; path=/"}const escapedCookieName=(cookiePrefix+"_vwo_uuid_v2").replace(/([.*+?^${}()|[\]\\])/g,"\\$1");const regex=new RegExp("(^|;\\s*)"+escapedCookieName+"=([^;]*)");const match=e.cookie.match(regex);-1==e.cookie.indexOf("_vis_opt_out")&&-1==d.location.href.indexOf("vwo_opt_out=1")&&(a=match?decodeURIComponent(match[2]):a,a=a.split("|"),b=new Image,g=window._vis_opt_domain||c||d.location.hostname.replace(/^www\./,""),b.src="https://dev.visualwebsiteoptimizer.com/v.gif?cd="+(window._vis_opt_cookieDays||0)+"&a=758750&d="+encodeURIComponent(d.location.hostname.replace(/^www\./,"") || c)+"&u="+a[0]+"&h="+a[1]+"&t="+z,d.vwo_iehack_queue=[b],f("_vwo_uuid_v2",a.join("|"),366))})("pinecone.io",window.VWO.visUuid,document,window,0,_vwo_code.sT);
;;window.VWO=window.VWO||[];(function(){function l(e){return e.replace(/[^\w\s-.][\w]/g,(function(e){return e.toUpperCase()})).replace(/[^\w\s-.]/g,"").replace(/ [\w]/g,(function(e){return e.toUpperCase()})).replace(/ /g,"")}function i(e,t){if("object"!=typeof t||Array.isArray(t))return s("Invalid attribute type: attribute can only be an object!");var n,r,a,o={};for(n in t)if(Object.prototype.hasOwnProperty.call(t,n)){var i="props"===(i=(i=l((i=n).slice(i.search(/[\w-.]/g)))).replace(/^(_|vwo_|\.|v_|i_|-)*/g,""))?"":i;if(!i.trim())return s("Invalid attribute name: "+n+" is not allowed as an attribute name!");40<i.length&&(i=i.slice(0,40),console.warn("Attribute name should not be greater than 40 characters!"));var c=u(t[n])||(c=t[n],"[object Array]"===toString.call(c))?JSON.stringify(t[n]):t[n];v(e)||(a=100,c=(r=c).length>a?r.slice(0,a-1)+"...":r),o[i]=c}return o}function u(e){return"[object Object]"===toString.call(e)}function s(e){console.log("%cVWO Event API Error:","font-weight:bold;",e),_vwo_err({message:e})}function c(e=-1,t=""){switch(e){case 0:return s("Event name cannot be empty!");case 1:return console.warn("Event name should not be greater than 40 characters!");case 2:return s("Invalid event name: "+t+" is not allowed as an event name!");default:return s("Invalid event name: event name can only be a string!")}}function r(e,t,n){var e=function(e,t){if("string"!=typeof e)return c();if(!(e=e.trim()))return c(0);var n=e;if(!(e=function(e){return"visitors"===(e=(e=l(e.slice(e.search(/[\w-.]/g)))).replace(/^(_|vwo_|\.|v_|i_|-)*/g,"")).toLowerCase()&&(e+="_1"),e}(e)))return c(2,n);40<e.length&&(c(1),e=e.slice(0,40));n=i(e,t);if(n)return{eventName:e,filteredAttributeObject:n}}(e,t=t||{}),r={ogName:1,source:1};if(!e)return{filteredData:void 0,cb:null};t=null;if(u(n)){"function"==typeof n.cb&&(t=n.cb),delete n.cb;var a,o={};for(a in n)r[a]&&(o[a]=n[a]);0<Object.keys(o).length&&(e.filteredAttributeObject.vwoMeta=o)}return{filteredData:e,cb:t}}function v(e){return window.VWO._&&VWO._.allSettings&&VWO._.allSettings.dataStore&&VWO._.allSettings.dataStore.events&&VWO._.allSettings.dataStore.events[e]}VWO.event=function(e,t,n){var{filteredData:e,cb:n}=r(e,t,n);if(e&&function(e){if(!window._vis_debug&&!v(e)){var t;try{t=JSON.parse(window.localStorage.getItem("vwoUnRegEvents"))||{}}catch(e){t={}}if(t[e])return!1;t[e]=!0;e=JSON.stringify(t);window.localStorage.setItem("vwoUnRegEvents",e)}return!0}(e.eventName))return window._vis_debug?void 0:((t={d:{event:{}}}).d.event.props=e.filteredAttributeObject||{},t.d.event.props.page={title:document.title,url:window._vis_opt_url||window.location.href,referrerUrl:document.referrer},t.d.event.name=e.eventName,t.d.event.time=Date.now(),e="https://dev.visualwebsiteoptimizer.com/events/"+(v(e.eventName)?"t":"t/u")+"?en="+e.eventName+"&a="+758750+"&_cu="+encodeURIComponent(document.URL),navigator.sendBeacon(e,JSON.stringify(t)),void("function"==typeof n&&n()))}})();
;for(var e=0;e<window.VWO.length;e++){exC(window.VWO[e])}window.VWO.push=function(){exC(arguments[0])};function exC(a){if(!Array.isArray(a))return;switch(a[0]){case"onVWOLoaded":var p=[];p[0]={};p[1]=[];return a[1].call(this,p);case"onVWOCampaignsLoaded":return a[1].call(this,{bucketed_campaigns:[]});case"event":return VWO.event(a[1],a[2],a[3])}}
;_vwo_code.finish()}}catch(e){_vwo_code.finish();_vwo_err(e);window.VWO.caE=1}})();
</script><meta name="viewport" content="width=device-width, initial-scale=1"><meta charset="utf-8"><title>Rerankers and Two-Stage Retrieval | Pinecone</title><meta name="description" content="Learn how to build better retrieval augmented generation (RAG) pipelines for LLMs, search, and recommendation. In this chapter we explore two-stage retrieval and the incredible accuracy of reranker models."><link rel="canonical" href="https://www.pinecone.io/learn/series/rag/rerankers/"><meta property="og:title" content="Rerankers and Two-Stage Retrieval | Pinecone"><meta property="og:description" content="Learn how to build better retrieval augmented generation (RAG) pipelines for LLMs, search, and recommendation. In this chapter we explore two-stage retrieval and the incredible accuracy of reranker models."><meta property="og:image" content="https://www.pinecone.io/api/og/?title=Rerankers%20and%20Two-Stage%20Retrieval&amp;category=Learn"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@pinecone"><meta name="twitter:creator" content="@pinecone"><meta name="twitter:title" content="Rerankers and Two-Stage Retrieval"><meta name="twitter:description" content="Learn how to build better retrieval augmented generation (RAG) pipelines for LLMs, search, and recommendation. In this chapter we explore two-stage retrieval and the incredible accuracy of reranker models."><meta name="twitter:image" content="https://www.pinecone.io/api/og/?title=Rerankers%20and%20Two-Stage%20Retrieval&amp;category=Learn"><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"><meta name="next-size-adjust"></head><body><div data-nosnippet="" class="osano-cm-window" dir="ltr"><!----> <!--?lit$3861756153$--><div hidden="" class="osano-visually-hidden"> <span id="osano-cm-aria.newWindow"><!--?lit$3861756153$-->Opens in a new window</span> <span id="osano-cm-aria.external"><!--?lit$3861756153$-->Opens an external website</span> <span id="osano-cm-aria.externalNewWindow"><!--?lit$3861756153$-->Opens an external website in a new window</span> </div> <!--?lit$3861756153$--> <div role="dialog" id="b3332840-95b8-4057-b2bd-9b1342765940" aria-label="Cookie Consent Banner" aria-describedby="b3332840-95b8-4057-b2bd-9b1342765940__label" class="osano-cm-window__dialog osano-cm-dialog osano-cm-dialog--position_bottom-left osano-cm-dialog--type_box osano-cm-dialog--hidden"> <!--?lit$3861756153$--> <button class=" osano-cm-dialog__close osano-cm-close "> <!--?lit$3861756153$--><svg width="20px" height="20px" viewBox="0 0 20 20" role="img" aria-labelledby="202b41be-ffdc-4a61-a14a-2607fe8615ae"> <title id="202b41be-ffdc-4a61-a14a-2607fe8615ae"><!---->Close this dialog<!----></title> <line role="presentation" x1="2" y1="2" x2="18" y2="18"></line> <line role="presentation" x1="2" y1="18" x2="18" y2="2"></line> </svg> </button>  <div class=" osano-cm-dialog__content osano-cm-content "> <!--?lit$3861756153$--> <span id="b3332840-95b8-4057-b2bd-9b1342765940__label" class=" osano-cm-content__message osano-cm-message "> <!--?lit$3861756153$-->This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. <!--?lit$3861756153$-->To learn more, view the following link: <!--?lit$3861756153$--> </span>  <!--?lit$3861756153$--> <!--?lit$3861756153$--><!--?lit$3861756153$--><a rel="noopener" tabindex="0" href="https://www.pinecone.io/cookies/" target="_blank" class=" osano-cm-storage-policy osano-cm-content__link osano-cm-link " aria-describedby="osano-cm-aria.newWindow"><!--?lit$3861756153$-->Cookie Policy</a><!--?--><!--?lit$3861756153$--> <!--?lit$3861756153$--> <!--?lit$3861756153$--> </div> <!--?lit$3861756153$--> </div>  <!--?lit$3861756153$--> <button id="793e4858-b261-498e-bf77-baf1b0be05d8" class="osano-cm-window__widget osano-cm-widget osano-cm-widget--position_left" title="Cookie Preferences" aria-label="Cookie Preferences"> <svg role="img" width="40" height="40" viewBox="0 0 71.85 72.23" xmlns="http://www.w3.org/2000/svg" aria-labelledby="793e4858-b261-498e-bf77-baf1b0be05d8"> <path d="m67.6 36.73a6.26 6.26 0 0 1 -3.2-2.8 5.86 5.86 0 0 0 -5.2-3.1h-.3a11 11 0 0 1 -11.4-9.5 6 6 0 0 1 -.1-1.4 9.2 9.2 0 0 1 .4-2.9 8.65 8.65 0 0 0 .2-1.6 5.38 5.38 0 0 0 -1.9-4.3 7.3 7.3 0 0 1 -2.5-5.5 3.91 3.91 0 0 0 -3.5-3.9 36.46 36.46 0 0 0 -15 1.5 33.14 33.14 0 0 0 -22.1 22.7 35.62 35.62 0 0 0 -1.5 10.2 34.07 34.07 0 0 0 4.8 17.6.75.75 0 0 0 .07.12c.11.17 1.22 1.39 2.68 3-.36.47 5.18 6.16 5.65 6.52a34.62 34.62 0 0 0 55.6-21.9 4.38 4.38 0 0 0 -2.7-4.74z" stroke-width="3" class=" osano-cm-widget__outline osano-cm-outline "></path> <path d="m68 41.13a32.37 32.37 0 0 1 -52 20.5l-2-1.56c-2.5-3.28-5.62-7.15-5.81-7.44a32 32 0 0 1 -4.5-16.5 34.3 34.3 0 0 1 1.4-9.6 30.56 30.56 0 0 1 20.61-21.13 33.51 33.51 0 0 1 14.1-1.4 1.83 1.83 0 0 1 1.6 1.8 9.38 9.38 0 0 0 3.3 7.1 3.36 3.36 0 0 1 1.2 2.6 3.37 3.37 0 0 1 -.1 1 12.66 12.66 0 0 0 -.5 3.4 9.65 9.65 0 0 0 .1 1.7 13 13 0 0 0 10.5 11.2 16.05 16.05 0 0 0 3.1.2 3.84 3.84 0 0 1 3.5 2 10 10 0 0 0 4.1 3.83 2 2 0 0 1 1.4 2z" stroke-width="3" class=" osano-cm-widget__outline osano-cm-outline "></path> <g class=" osano-cm-widget__dot osano-cm-dot "> <path d="m26.6 31.43a5.4 5.4 0 1 1 5.4-5.43 5.38 5.38 0 0 1 -5.33 5.43z"></path> <path d="m25.2 53.13a5.4 5.4 0 1 1 5.4-5.4 5.44 5.44 0 0 1 -5.4 5.4z"></path> <path d="m47.9 52.33a5.4 5.4 0 1 1 5.4-5.4 5.32 5.32 0 0 1 -5.24 5.4z"></path> </g> </svg> </button>  <!--?lit$3861756153$--><div role="dialog" aria-modal="true" id="99d294ea-f14c-417f-807b-d6b443b53aef" aria-labelledby="99d294ea-f14c-417f-807b-d6b443b53aef__label" aria-hidden="true" class=" osano-cm-window__info-dialog osano-cm-info-dialog osano-cm-info-dialog--hidden "> <!--?lit$3861756153$--><!--?lit$3861756153$--><span tabindex="0" aria-hidden="true" data-focus="first"></span><!--?--> <div role="presentation" class=" osano-cm-info-dialog__info osano-cm-info osano-cm-info--position_left "> <!--?lit$3861756153$--><div role="presentation" class=" osano-cm-info__info-dialog-header osano-cm-info-dialog-header "> <p role="heading" aria-level="1" id="99d294ea-f14c-417f-807b-d6b443b53aef__label" class=" osano-cm-info-dialog-header__header osano-cm-header "> <!--?lit$3861756153$--> </p> <!--?lit$3861756153$--> <button class=" osano-cm-info-dialog-header__close osano-cm-close "> <!--?lit$3861756153$--><svg width="20px" height="20px" viewBox="0 0 20 20" role="img" aria-labelledby="24d93d88-1922-4503-bcbb-a713d306ccd7"> <title id="24d93d88-1922-4503-bcbb-a713d306ccd7"><!---->Close Cookie Preferences<!----></title> <line role="presentation" x1="2" y1="2" x2="18" y2="18"></line> <line role="presentation" x1="2" y1="18" x2="18" y2="2"></line> </svg> </button> <!--?lit$3861756153$--> </div> <div role="presentation" class=" osano-cm-info__info-views osano-cm-info-views osano-cm-info-views--hidden osano-cm-info-views--position_0 "> <!--?lit$3861756153$--> </div> </div> <!--?lit$3861756153$--><!--?lit$3861756153$--><span tabindex="0" aria-hidden="true" data-focus="last"></span><!--?--> </div> </div><!--$--><!--/$--><script type="osano/blocked" crossorigin="anonymous"></script><script type="osano/blocked" crossorigin="anonymous"></script><script type="osano/blocked" crossorigin="anonymous"></script><script type="osano/blocked" crossorigin="anonymous"></script><script src="/_next/static/chunks/webpack-da8dce98ca778df3.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d50ea7139ed0078-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/88203632e8260aeb-s.p.otf\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/otf\"}]\n3:HL[\"/_next/static/media/9001b2c46b4f4e76-s.p.otf\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/otf\"}]\n4:HL[\"/_next/static/media/bb3ef058b751a6ad-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/4be8eca88fd30eee.css\",\"style\"]\n6:HL[\"/_next/static/css/ecdb34faee2e34d6.css\",\"style\"]\n7:HL[\"/_next/static/css/13e7e6ab504d5b67.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"8:I[12846,[],\"\"]\nc:I[4707,[],\"\"]\ne:I[36423,[],\"\"]\n11:I[11313,[\"5878\",\"static/chunks/9da6db1e-0b0f4d5c93f833e0.js\",\"3185\",\"static/chunks/app/layout-5888a3b8110b9399.js\"],\"Providers\"]\n14:I[61060,[],\"\"]\nd:[\"seriesSlug\",\"rag\",\"d\"]\n15:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L8\",null,{\"buildId\":\"RZbW7RJpYpPn3iPnbWbbx\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"learn\",\"series\",\"rag\",\"rerankers\",\"\"],\"initialTree\":[\"\",{\"children\":[\"(frontend)\",{\"children\":[\"(shared-layout)\",{\"children\":[\"learn\",{\"children\":[\"series\",{\"children\":[[\"seriesSlug\",\"rag\",\"d\"],{\"children\":[[\"chapterSlug\",\"rerankers\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"(frontend)\",{\"children\":[\"(shared-layout)\",{\"children\":[\"learn\",{\"children\":[\"series\",{\"children\":[[\"seriesSlug\",\"rag\",\"d\"],{\"children\":[[\"chapterSlug\",\"rerankers\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L9\",\"$La\",null],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/13e7e6ab504d5b67.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],\"$Lb\"],null],null]},[null,[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"(shared-layout)\",\"children\",\"learn\",\"children\",\"series\",\"children\",\"$d\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"(shared-layout)\",\"children\",\"learn\",\"children\",\"series\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"(shared-layout)\",\"children\",\"learn\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[null,\"$Lf\"],null],null]},[[null,\"$L10\"],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4be8eca88fd30eee.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ecdb34faee2e34d6.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__className_9a6492 __variable_9a6492 __variable_3c557b __variable_ff0bc9 __variable_a59715\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$L11\",null,{\"children\":[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$L12\",\"notFoundStyles\":[]}]}]}]]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L13\"],\"globalErrorComponent\":\"$14\",\"missingSlots\":\"$W15\"}]\n"])</script><script>self.__next_f.push([1,"16:I[88003,[\"991\",\"static/chunks/52774a7f-9140b4e323efe0ba.js\",\"8667\",\"static/chunks/8667-3b0d6e3f612cdd6b.js\",\"7354\",\"static/chunks/7354-1d7870310d1419aa.js\",\"4424\",\"static/chunks/app/(frontend)/layout-ddd476d9ab06f775.js\"],\"\"]\n19:\"$Sreact.suspense\"\n1a:I[82426,[\"991\",\"static/chunks/52774a7f-9140b4e323efe0ba.js\",\"8667\",\"static/chunks/8667-3b0d6e3f612cdd6b.js\",\"7354\",\"static/chunks/7354-1d7870310d1419aa.js\",\"4424\",\"static/chunks/app/(frontend)/layout-ddd476d9ab06f775.js\"],\"default\"]\n1b:I[7598,[\"991\",\"static/chunks/52774a7f-9140b4e323efe0ba.js\",\"8667\",\"static/chunks/8667-3b0d6e3f612cdd6b.js\",\"7354\",\"static/chunks/7354-1d7870310d1419aa.js\",\"4424\",\"static/chunks/app/(frontend)/layout-ddd476d9ab06f775.js\"],\"default\"]\n1c:I[1952,[\"991\",\"static/chunks/52774a7f-9140b4e323efe0ba.js\",\"8667\",\"static/chunks/8667-3b0d6e3f612cdd6b.js\",\"7354\",\"static/chunks/7354-1d7870310d1419aa.js\",\"4424\",\"static/chunks/app/(frontend)/layout-ddd476d9ab06f775.js\"],\"SpeedInsights\"]\n17:T724,document.documentElement.classList.add('theme-initializing'); ((e,t,r,s)=\u003e{document.documentElement.classList.add(\"theme-initializing\");let n=document.documentElement;function i(e){let t=\"system\"===e?window.matchMedia(\"(prefers-color-scheme: dark)\").matches?\"dark\":\"light\":e;n.setAttribute(\"data-initial-theme\",e),n.dataset.theme=t,n.classList.remove(\"light\",\"dark\"),n.classList.add(t),n.style.colorScheme=t,n.style.setProperty(\"--theme\",`\"${t}\"`)}try{let e=function(){try{let e=(\"; \"+document.cookie).split(\"; \"+t+\"=\");if(2===e.length){let t=e.pop()?.split(\";\").shift();if(t\u0026\u0026[\"light\",\"dark\",\"system\"].includes(t))return t}}catch(e){}return r}();i(e),\"system\"===e\u0026\u0026window.matchMedia(\"(prefers-color-scheme: dark)\").addEventListener(\"change\",()=\u003e{i(\"system\")}),window.addEventListener(\"pinecone-theme-change\",function(t){t.detail\u0026\u0026t.detail.theme\u0026\u0026(e=t.detail.theme,i(t.detail.theme))}),document.addEventListener(\"visibilitychange\",()=\u003e{if(\"visible\"===document.visibilityState){let r=function(){try{let e=(\"; \"+document.cookie).split(\"; \"+t+\"=\");if(2===e.length){let t=e.pop()?."])</script><script>self.__next_f.push([1,"split(\";\").shift();if(t\u0026\u0026[\"light\",\"dark\",\"system\"].includes(t))return t}}catch(e){}return null}();if(r\u0026\u0026r!==e){e=r,i(r);try{let e=window.location.hostname,t=e.includes(\"pinecone.io\")\u0026\u0026!e.includes(\"localhost\"),r=Date.now().toString();document.cookie=`theme_change_timestamp=${r};path=/${t?\";domain=.pinecone.io\":\"\"};max-age=31536000`}catch(e){}}}}),n.classList.add(\"ssr-theme-applied\"),requestAnimationFrame(()=\u003e{n.classList.remove(\"theme-initializing\")});let s=setTimeout(()=\u003e{n.classList.contains(\"theme-initializing\")\u0026\u0026n.classList.remove(\"theme-initializing\")},300);window.addEventListener(\"beforeunload\",()=\u003e{clearTimeout(s)},{once:!0})}catch(e){n.classList.remove(\"theme-initializing\")}})(\"pinecone_theme_mode\",\"pinecone_theme_mode\",\"system\",null)18:Tda4,"])</script><script>self.__next_f.push([1,"window._vwo_code || (function() {\n            var account_id=758750,\n            version=2.1,\n            settings_tolerance=2000,\n            hide_element='body',\n            hide_element_style = 'opacity:0 !important;filter:alpha(opacity=0) !important;background:none !important',\n            /* DO NOT EDIT BELOW THIS LINE */\n            f=false,w=window,d=document,v=d.querySelector('#vwoCode'),cK='_vwo_'+account_id+'_settings',cc={};try{var c=JSON.parse(localStorage.getItem('_vwo_'+account_id+'_config'));cc=c\u0026\u0026typeof c==='object'?c:{}}catch(e){}var stT=cc.stT==='session'?w.sessionStorage:w.localStorage;code={use_existing_jquery:function(){return typeof use_existing_jquery!=='undefined'?use_existing_jquery:undefined},library_tolerance:function(){return typeof library_tolerance!=='undefined'?library_tolerance:undefined},settings_tolerance:function(){return cc.sT||settings_tolerance},hide_element_style:function(){return'{'+(cc.hES||hide_element_style)+'}'},hide_element:function(){if(performance.getEntriesByName('first-contentful-paint')[0]){return''}return typeof cc.hE==='string'?cc.hE:hide_element},getVersion:function(){return version},finish:function(e){if(!f){f=true;var t=d.getElementById('_vis_opt_path_hides');if(t)t.parentNode.removeChild(t);if(e)(new Image).src='https://dev.visualwebsiteoptimizer.com/ee.gif?a='+account_id+e}},finished:function(){return f},addScript:function(e){var t=d.createElement('script');t.type='text/javascript';if(e.src){t.src=e.src}else{t.text=e.text}d.getElementsByTagName('head')[0].appendChild(t)},load:function(e,t){var i=this.getSettings(),n=d.createElement('script'),r=this;t=t||{};if(i){n.textContent=i;d.getElementsByTagName('head')[0].appendChild(n);if(!w.VWO||VWO.caE){stT.removeItem(cK);r.load(e)}}else{var o=new XMLHttpRequest;o.open('GET',e,true);o.withCredentials=!t.dSC;o.responseType=t.responseType||'text';o.onload=function(){if(t.onloadCb){return t.onloadCb(o,e)}if(o.status===200){_vwo_code.addScript({text:o.responseText})}else{_vwo_code.finish('\u0026e=loading_failure:'+e)}};o.onerror=function(){if(t.onerrorCb){return t.onerrorCb(e)}_vwo_code.finish('\u0026e=loading_failure:'+e)};o.send()}},getSettings:function(){try{var e=stT.getItem(cK);if(!e){return}e=JSON.parse(e);if(Date.now()\u003ee.e){stT.removeItem(cK);return}return e.s}catch(e){return}},init:function(){if(d.URL.indexOf('__vwo_disable__')\u003e-1)return;var e=this.settings_tolerance();w._vwo_settings_timer=setTimeout(function(){_vwo_code.finish();stT.removeItem(cK)},e);var t;if(this.hide_element()!=='body'){t=d.createElement('style');var i=this.hide_element(),n=i?i+this.hide_element_style():'',r=d.getElementsByTagName('head')[0];t.setAttribute('id','_vis_opt_path_hides');v\u0026\u0026t.setAttribute('nonce',v.nonce);t.setAttribute('type','text/css');if(t.styleSheet)t.styleSheet.cssText=n;else t.appendChild(d.createTextNode(n));r.appendChild(t)}else{t=d.getElementsByTagName('head')[0];var n=d.createElement('div');n.style.cssText='z-index: 2147483647 !important;position: fixed !important;left: 0 !important;top: 0 !important;width: 100% !important;height: 100% !important;background: white !important;';n.setAttribute('id','_vis_opt_path_hides');n.classList.add('_vis_hide_layer');t.parentNode.insertBefore(n,t.nextSibling)}var o='https://dev.visualwebsiteoptimizer.com/j.php?a='+account_id+'\u0026u='+encodeURIComponent(d.URL)+'\u0026vn='+version;if(w.location.search.indexOf('_vwo_xhr')!==-1){this.addScript({src:o})}else{this.load(o+'\u0026x=true')}}};w._vwo_code=code;code.init();})();"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"$L16\",null,{\"id\":\"consent-data-layer\",\"src\":\"/scripts/consent-data-layer.js\",\"strategy\":\"beforeInteractive\"}],[\"$\",\"$L16\",null,{\"id\":\"osano-cmp-script\",\"src\":\"https://cmp.osano.com/AzZbUITh61cXz4HqL/fee5a4ca-0585-49b2-b317-c6a8cf7b66fa/osano.js\",\"strategy\":\"beforeInteractive\"}],[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n    html.theme-initializing {\\n      opacity: 0 !important;\\n      visibility: hidden !important;\\n      pointer-events: none !important;\\n      background-color: var(--background) !important;\\n    }\\n  \"}}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$17\"},\"nonce\":\"theme-script\"}]],[\"$\",\"$L16\",null,{\"id\":\"vwoCode\",\"strategy\":\"beforeInteractive\",\"dangerouslySetInnerHTML\":{\"__html\":\"$18\"}}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1a\",null,{}]}],[\"$\",\"$L1b\",null,{}],[\"$\",\"$L16\",null,{\"id\":\"koala-script\",\"strategy\":\"afterInteractive\",\"dangerouslySetInnerHTML\":{\"__html\":\"\\n          !function(t){if(window.ko)return;window.ko=[],[\\\"identify\\\",\\\"track\\\",\\\"removeListeners\\\",\\\"open\\\",\\\"on\\\",\\\"off\\\",\\\"qualify\\\",\\\"ready\\\"].forEach(function(t){ko[t]=function(){var n=[].slice.call(arguments);return n.unshift(t),ko.push(n),ko}});var n=document.createElement(\\\"script\\\");n.async=!0,n.setAttribute(\\\"src\\\",\\\"https://cdn.getkoala.com/v1/pk_dedf7f497457f685cec291ae4d5002e828c1/sdk.js\\\"),(document.body || document.head).appendChild(n)}();\\n        \"}}],[\"$\",\"$L16\",null,{\"id\":\"hs-script-loader\",\"src\":\"https://js.hs-scripts.com/8231564.js\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$L16\",null,{\"id\":\"clearbits-script\",\"src\":\"https://tag.clearbitscripts.com/v1/pk_5add668ab6de5cdc81322cba801cf429/tags.js\",\"strategy\":\"afterInteractive\",\"referrerPolicy\":\"strict-origin-when-cross-origin\"}],[\"$\",\"$L16\",null,{\"id\":\"partnerstack-script\",\"strategy\":\"afterInteractive\",\"dangerouslySetInnerHTML\":{\"__html\":\"\\n          (function() {var gs = document.createElement('script');gs.src = 'https://js.partnerstack.com/v1/';gs.type = 'text/javascript';gs.async = 'true';gs.onload = gs.onreadystatechange = function() {var rs = this.readyState;if (rs \u0026\u0026 rs != 'complete' \u0026\u0026 rs != 'loaded') return;try {growsumo._initialize('pk_jvoXqtGYtmYGQJlSZmilQ0OR1zIkTziF'); if (typeof(growsumoInit) === 'function') {growsumoInit();}} catch (e) {}};var s = document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gs, s);})();\\n        \"}}],[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}],[\"$\",\"$L1c\",null,{\"sampleRate\":0.4}]]\n"])</script><script>self.__next_f.push([1,"1d:I[68514,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"9948\",\"static/chunks/9948-7ca7d72282f7a143.js\",\"6012\",\"static/chunks/app/(frontend)/(shared-layout)/layout-bcb0bfbab41a9d1e.js\"],\"default\"]\n1e:I[19316,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"9948\",\"static/chunks/9948-7ca7d72282f7a143.js\",\"6012\",\"static/chunks/app/(frontend)/(shared-layout)/layout-bcb"])</script><script>self.__next_f.push([1,"0bfbab41a9d1e.js\"],\"default\"]\n1f:I[72972,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"\"]\n20:I[58098,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca."])</script><script>self.__next_f.push([1,"js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"9948\",\"static/chunks/9948-7ca7d72282f7a143.js\",\"6012\",\"static/chunks/app/(frontend)/(shared-layout)/layout-bcb0bfbab41a9d1e.js\"],\"default\"]\n21:I[73944,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"9948\",\"static/chunks/9948-7ca7d72282f7a143.js\",\"6012\",\"static/chunks/app/(frontend)/(shared-layout)/layout-bcb0bfbab41a9d1e.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"$L1d\",null,{\"announcement\":{\"category\":\"Announcement\",\"cta\":\"Discover More\",\"description\":\"Announcing the Pinecone Pioneers program: for developers who share and educate\\n\",\"enabled\":true,\"link\":\"https://www.pinecone.io/blog/announcing-pinecone-pioneers-program/\"}}],[\"$\",\"$L1e\",null,{}],null,[\"$\",\"main\",null,{\"children\":[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"(shared-layout)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"id\":\"$undefined\",\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative px-4 sm:px-8 undefined\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-background-light\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex flex-col items-center gap-25 py-50 text-center md:py-70 lg:py-100 xl:py-150 text-text-primary\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-h2 text-text-primary lg:text-h1\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-body font-semibold lg:text-h3\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-small lg:text-body\",\"children\":\"Sorry, looks like that page doesnt exist!\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-25 font-semibold [\u0026\u003ea]:w-32\",\"children\":[[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"flex items-center justify-center border-2 border-brand-blue text-center font-semibold  transition-colors  focus:outline-offset-2 focus:outline-alpha1 group-hover/boxy:bg-alpha1 px-5 py-2.5 text-base bg-brand-blue text-white hover:border-brand-blue hover:bg-brand-blue/80 }\",\"children\":\"Home\"}],[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://docs.pinecone.io/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"flex items-center justify-center border-2 border-brand-blue text-center font-semibold  transition-colors  focus:outline-offset-2 focus:outline-alpha1 group-hover/boxy:bg-alpha1 px-5 py-2.5 text-base bg-brand-blue text-white hover:border-brand-blue hover:bg-brand-blue/80 }\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://docs.pinecone.io/docs/examples\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"flex items-center justify-center border-2 border-brand-blue text-center font-semibold  transition-colors  focus:outline-offset-2 focus:outline-alpha1 group-hover/boxy:bg-alpha1 px-5 py-2.5 text-base bg-brand-blue text-white hover:border-brand-blue hover:bg-brand-blue/80 }\",\"children\":\"Examples\"}]]}]]}]}],\"$undefined\",[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute left-0 top-0 w-full\",\"children\":[[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"}]],[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"}]]]}],[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute bottom-0 left-0 w-full\",\"children\":[[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l bottom-0 left-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]\"}]],[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l bottom-0 right-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]\"}]]]}]]}]}],\"notFoundStyles\":[]}]}],[\"$\",\"$L20\",null,{}],[\"$\",\"$L21\",null,{}]]\n"])</script><script>self.__next_f.push([1,"12:[[\"$\",\"$L1d\",null,{\"announcement\":{\"category\":\"Announcement\",\"cta\":\"Discover More\",\"description\":\"Announcing the Pinecone Pioneers program: for developers who share and educate\\n\",\"enabled\":true,\"link\":\"https://www.pinecone.io/blog/announcing-pinecone-pioneers-program/\"}}],[\"$\",\"$L1e\",null,{}],[\"$\",\"div\",null,{\"id\":\"$undefined\",\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative px-4 sm:px-8 undefined\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-background-light\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex flex-col items-center gap-25 py-50 text-center md:py-70 lg:py-100 xl:py-150 text-text-primary\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-h2 text-text-primary lg:text-h1\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-body font-semibold lg:text-h3\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-small lg:text-body\",\"children\":\"Sorry, looks like that page doesnt exist!\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-25 font-semibold [\u0026\u003ea]:w-32\",\"children\":[[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"flex items-center justify-center border-2 border-brand-blue text-center font-semibold  transition-colors  focus:outline-offset-2 focus:outline-alpha1 group-hover/boxy:bg-alpha1 px-5 py-2.5 text-base bg-brand-blue text-white hover:border-brand-blue hover:bg-brand-blue/80 }\",\"children\":\"Home\"}],[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://docs.pinecone.io/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"flex items-center justify-center border-2 border-brand-blue text-center font-semibold  transition-colors  focus:outline-offset-2 focus:outline-alpha1 group-hover/boxy:bg-alpha1 px-5 py-2.5 text-base bg-brand-blue text-white hover:border-brand-blue hover:bg-brand-blue/80 }\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://docs.pinecone.io/docs/examples\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"flex items-center justify-center border-2 border-brand-blue text-center font-semibold  transition-colors  focus:outline-offset-2 focus:outline-alpha1 group-hover/boxy:bg-alpha1 px-5 py-2.5 text-base bg-brand-blue text-white hover:border-brand-blue hover:bg-brand-blue/80 }\",\"children\":\"Examples\"}]]}]]}]}],\"$undefined\",[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute left-0 top-0 w-full\",\"children\":[[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"}]],[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"}]]]}],[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute bottom-0 left-0 w-full\",\"children\":[[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l bottom-0 left-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]\"}]],[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l bottom-0 right-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]\"}]]]}]]}]}],[\"$\",\"$L20\",null,{}]]\n"])</script><script>self.__next_f.push([1,"22:I[61420,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"8667\",\"static/chunks/8667-3b0d6e3f612cdd6b.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"6266\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/page-9a6749d88c6c9981.js\"],\"default\"]\n23:I[81523,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks"])</script><script>self.__next_f.push([1,"/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"BailoutToCSR\"]\n24:I[80974,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/931"])</script><script>self.__next_f.push([1,"6-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"default\"]\n26:I[10229,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"default\"]\n27:I[70049,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.j"])</script><script>self.__next_f.push([1,"s\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"PreloadCss\"]\n28:I[21639,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339"])</script><script>self.__next_f.push([1,"\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"default\"]\n29:I[38680,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"default\"]\n2c:I[82374,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01"])</script><script>self.__next_f.push([1,"c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"8667\",\"static/chunks/8667-3b0d6e3f612cdd6b.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"6266\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/page-9a6749d88c6c9981.js\"],\"default\"]\n2a:[\"../sanity/sanity-utils.tsx -\u003e @/components/article/Notebook\"]\n2b:[\"../sanity/sanity-utils.tsx -\u003e @/components/article/Codeblock\"]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[\"$\",\"$L22\",null,{\"script\":\"window.heap.clearEventProperties(); window.heap.addEventProperties({\\\"Created At\\\": \\\"2023-10-27T11:36:56Z\\\"});\"}]}],[\"$\",\"article\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex w-full flex-col gap-75 lg:flex-row\",\"children\":[\"$\",\"div\",null,{\"className\":\"overflow-hidden lg:order-2 mb-12 [\u0026\u003e*:first-child]:mt-0\",\"children\":[\"$undefined\",[[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/retrieval-augmented-generation/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"Retrieval Augmented Generation (RAG)\"]}],\" is an overloaded term. It promises the world, but after developing a RAG pipeline, there are many of us left wondering why it doesn't work as well as we had expected.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"As with most tools, RAG is easy to use but hard to master. The truth is that there is more to RAG than putting documents into a vector DB and adding an LLM on top. That \",[\"$\",\"em\",null,{\"children\":[\"can work\"]}],\", but it won't always.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"This ebook aims to tell you what to do when out-of-the-box RAG \",[\"$\",\"em\",null,{\"children\":[\"doesn't\"]}],\" work. In this first chapter, we'll look at what is often the easiest and fastest to implement solution for suboptimal RAG pipelines  we'll be learning about \",[\"$\",\"em\",null,{\"children\":[\"rerankers\"]}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[\"$\",\"$L23\",null,{\"reason\":\"next/dynamic\",\"children\":[\"$\",\"$L24\",null,{\"url\":\"https://www.youtube.com/watch?v=Uh9bYiVrW_s\"}]}]}],[\"$\",\"div\",null,{\"className\":\"pt-3 text-center text-sm\",\"children\":\"Video companion for this chapter.\"}],\"$undefined\"]}],[\"$\",\"hr\",null,{\"className\":\"my-8 border-solid\"}],[\"$\",\"h2\",null,{\"className\":\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\",\"id\":\"Recall-vs.-Context-Windows\",\"children\":[\"Recall vs. Context Windows\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Before jumping into the solution, let's talk about the problem. With RAG, we are performing a \",[\"$\",\"em\",null,{\"children\":[\"semantic search\"]}],\" across many text documents  these could be tens of thousands up to tens of billions of documents.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"To ensure fast search times at scale, we typically use vector search  that is, we transform our text into vectors, place them all into a vector space, and compare their proximity to a query vector using a similarity metric like cosine similarity.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"For vector search to work, we need vectors. These vectors are essentially compressions of the \\\"meaning\\\" behind some text into (typically) 768 or 1536-dimensional vectors. There is some information loss because we're compressing this information into a single vector.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Because of this information loss, we often see that the top three (for example) vector search documents will miss relevant information. Unfortunately, the retrieval may return relevant information below our \",[\"$\",\"code\",null,{\"children\":[\"top_k\"]}],\" cutoff.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"What do we do if relevant information at a lower position would help our LLM formulate a better response? The easiest approach is to increase the number of documents we're returning (increase \",[\"$\",\"code\",null,{\"children\":[\"top_k\"]}],\") and pass them all to the LLM.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"The metric we would measure here is \",[\"$\",\"em\",null,{\"children\":[\"recall\"]}],\"  meaning \\\"how many of the relevant documents are we retrieving\\\". Recall does not consider the total number of retrieved documents  so we can hack the metric and get \",[\"$\",\"em\",null,{\"children\":[\"perfect\"]}],\" recall by returning \",[\"$\",\"em\",null,{\"children\":[\"everything\"]}],\".\"]}],[\"$\",\"div\",null,{\"className\":\"overflow-x-auto pt-2 md:pt-2\",\"children\":[\"$\",\"div\",null,{\"className\":\"latex-container text-sm sm:text-base md:text-lg\",\"children\":[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[\"$\",\"$L23\",null,{\"reason\":\"next/dynamic\",\"children\":\"$L25\"}]}]}]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Unfortunately, we cannot return everything. LLMs have limits on how much text we can pass to them  we call this limit the \",[\"$\",\"em\",null,{\"children\":[\"context window\"]}],\". Some LLMs have huge context windows, like Anthropic's Claude, with a context window of 100K tokens [1]. With that, we could fit many tens of pages of text  so could we return many documents (not quite all) and \\\"stuff\\\" the context window to improve recall?\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Again, no. We cannot use context stuffing because this reduces the LLM's \",[\"$\",\"em\",null,{\"children\":[\"recall\"]}],\" performance  note that this is the LLM recall, which is different from the retrieval recall we have been discussing so far.\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-full\",\"children\":[\"$\",\"$L26\",null,{\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/ca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png\",\"alt\":\"When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place\",\"width\":1212,\"height\":688}]}],[\"$\",\"div\",null,{\"className\":\"w-full text-center text-sm text-text-secondary mt-4\",\"children\":\"When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place [2].\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"LLM recall refers to the ability of an LLM to find information from the text placed within its context window. Research shows that LLM recall degrades as we put more tokens in the context window [2]. LLMs are also less likely to follow instructions as we stuff the context window  so context stuffing is a bad idea.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We can increase the number of documents returned by our vector DB to increase retrieval recall, but we cannot pass these to our LLM without damaging LLM recall.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"The solution to this issue is to maximize retrieval recall by retrieving plenty of documents and then maximize LLM recall by \",[\"$\",\"em\",null,{\"children\":[\"minimizing\"]}],\" the number of documents that make it to the LLM. To do that, we reorder retrieved documents and keep just the most relevant for our LLM  to do that, we use \",[\"$\",\"em\",null,{\"children\":[\"reranking\"]}],\".\"]}],[\"$\",\"h2\",null,{\"className\":\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\",\"id\":\"Power-of-Rerankers\",\"children\":[\"Power of Rerankers\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"A \",[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://docs.pinecone.io/models/bge-reranker-v2-m3\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"reranking model\"]}],\"  also known as a \",[\"$\",\"strong\",null,{\"children\":[\"cross-encoder\"]}],\"  is a type of model that, given a query and document pair, will output a similarity score. We use this score to reorder the documents by relevance to our query.\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-full\",\"children\":[\"$\",\"$L26\",null,{\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/906c3c0f8fe637840f134dbf966839ef89ac7242-3443x1641.png\",\"alt\":\"A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.\",\"width\":3443,\"height\":1641}]}],[\"$\",\"div\",null,{\"className\":\"w-full text-center text-sm text-text-secondary mt-4\",\"children\":\"A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Search engineers have used rerankers in two-stage retrieval systems for \",[\"$\",\"em\",null,{\"children\":[\"a long time\"]}],\". In these two-stage systems, a first-stage model (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. Then, a second-stage model (the reranker) is used to rerank those documents retrieved by the first-stage model.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We use two stages because retrieving a small set of documents from a large dataset is much faster than reranking a large set of documents  we'll discuss why this is the case soon  but TL;DR, rerankers are slow, and retrievers are \",[\"$\",\"em\",null,{\"children\":[\"fast\"]}],\".\"]}],[\"$\",\"h3\",null,{\"className\":\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\",\"id\":\"Why-Rerankers\",\"children\":[\"Why Rerankers?\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"If a reranker is so much slower, why bother using them? The answer is that rerankers are much more accurate than \",[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/series/rag/embedding-models-rundown/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"embedding models\"]}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"The intuition behind a bi-encoder's inferior accuracy is that bi-encoders must compress all of the possible meanings of a document into a single vector  meaning we lose information. Additionally, bi-encoders have no context on the query because we don't know the query until we receive it (we create embeddings before user query time).\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"On the other hand, a reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Because we are running the reranker at user query time, we have the added benefit of analyzing our document's meaning specific to the user query  rather than trying to produce a generic, averaged meaning.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Rerankers avoid the information loss of bi-encoders  but they come with a different penalty  \",[\"$\",\"em\",null,{\"children\":[\"time\"]}],\".\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-full\",\"children\":[\"$\",\"$L26\",null,{\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420.png\",\"alt\":\"A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.\",\"width\":2760,\"height\":1420}]}],[\"$\",\"div\",null,{\"className\":\"w-full text-center text-sm text-text-secondary mt-4\",\"children\":\"A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"When using bi-encoder models with vector search, we frontload all of the heavy transformer computation to when we are creating the initial vectors  that means that when a user queries our system, we have already created the vectors, so all we need to do is:\"]}],[\"$\",\"ol\",null,{\"className\":\"!marker:text-text-primary ml-8! list-decimal! [\u0026_ul]:list-circle! mt-4 [\u0026_ol]:mt-0 [\u0026_ul]:mt-0\",\"children\":[[\"$\",\"li\",null,{\"className\":\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\",\"children\":[\"$\",\"span\",null,{\"className\":\"mt-0 block leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Run a single transformer computation to create the query vector.\"]}]}],[\"$\",\"li\",null,{\"className\":\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\",\"children\":[\"$\",\"span\",null,{\"className\":\"mt-0 block leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Compare the query vector to document vectors with \",[\"$\",\"em\",null,{\"children\":[\"cosine similarity\"]}],\" (or another lightweight metric).\"]}]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"With rerankers, we are not pre-computing anything. Instead, we're feeding our query and a single other document into the transformer, running a whole transformer inference step, and outputting a single similarity score.\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-full\",\"children\":[\"$\",\"$L26\",null,{\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png\",\"alt\":\"A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.\",\"width\":2440,\"height\":1100}]}],[\"$\",\"div\",null,{\"className\":\"w-full text-center text-sm text-text-secondary mt-4\",\"children\":\"A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Given 40M records, if we use a \",[\"$\",\"strong\",null,{\"children\":[\"small\"]}],\" reranking model like BERT on a V100 GPU  we'd be waiting more than 50 hours to return a single query result [3]. We can do the same in \u003c100ms with encoder models and vector search.\"]}],[\"$\",\"h2\",null,{\"className\":\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\",\"id\":\"Implementing-Two-Stage-Retrieval-with-Reranking\",\"children\":[\"Implementing Two-Stage Retrieval with Reranking\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Now that we understand the idea and reason behind two-stage retrieval with rerankers, let's see how to implement it (you can follow along with \",[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"this notebook\"]}],\". To begin we will set up our prerequisite libraries:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":[\"../sanity/sanity-utils.tsx -\u003e @/components/article/Codeblock\"]}],[\"$\",\"$L28\",null,{\"code\":\"!pip install -qU \\\\\\n    datasets==2.14.5 \\\\\\n    \\\"pinecone[grpc]\\\"==5.1.0\",\"language\":\"$undefined\"}]]}],[\"$\",\"h3\",null,{\"className\":\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\",\"id\":\"Data-Preparation\",\"children\":[\"Data Preparation\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Before setting up the retrieval pipeline, we need data to retrieve! We will use the \",[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[[\"$\",\"code\",null,{\"children\":[\"jamescalam/ai-arxiv-chunked\"]}]]}],\" dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs  including the Llama 2, GPTQ, and GPT-4 papers.\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":[\"../sanity/sanity-utils.tsx -\u003e @/components/article/Notebook\"]}],[\"$\",\"$L29\",null,{\"file\":{\"_key\":\"fc2fd187d922\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-d858315997239ee9631257691107bed218131ef4-ipynb\",\"_type\":\"reference\"}}}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"The dataset contains 41.5K pre-chunked records. Each record is 1-2 paragraphs long and includes additional metadata about the paper from which it comes. Here is an example:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2a\"}],[\"$\",\"$L29\",null,{\"file\":{\"_key\":\"581ef5269e22\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-d4e771a8d321b15576b6b3f72644cdf141c48db2-ipynb\",\"_type\":\"reference\"}}}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We'll be feeding this data into Pinecone, so let's reformat the dataset to be more Pinecone-friendly when it does come to the later embed and index process. The format will contain \",[\"$\",\"code\",null,{\"children\":[\"id\"]}],\", \",[\"$\",\"code\",null,{\"children\":[\"text\"]}],\" (which we will embed), and \",[\"$\",\"code\",null,{\"children\":[\"metadata\"]}],\". For this example, we won't use metadata, but it can be helpful to include if we want to do \",[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/vector-search-filtering/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"metadata filtering\"]}],\" in the future.\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2a\"}],[\"$\",\"$L29\",null,{\"file\":{\"_key\":\"b2d7f4c0f965\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-65bb438d978ee37cc404d7cc5f646de0443053e6-ipynb\",\"_type\":\"reference\"}}}]]}],[\"$\",\"h3\",null,{\"className\":\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\",\"id\":\"Embed-and-Index\",\"children\":[\"Embed and Index\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"To store everything in the vector DB, we need to encode everything with an embedding / bi-encoder model. We will use the open source \",[\"$\",\"code\",null,{\"children\":[\"multilingial-e5-large\"]}],\" via Pinecone Inference. We need a [free Pinecone API key](https://app.pinecone.io) to authenticate ourselves via the client:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"from pinecone.grpc import PineconeGRPC\\n\\n# get API key from app.pinecone.io\\napi_key = \\\"PINECONE_API_KEY\\\"\\n\\nembed_model = \\\"multilingual-e5-large\\\"\\n\\n# configure client\\npc = PineconeGRPC(api_key=api_key)\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Now, we create our vector DB to store our vectors. We set \",[\"$\",\"code\",null,{\"children\":[\"dimension\"]}],\" equal to the dimensionality of E5 large (\",[\"$\",\"code\",null,{\"children\":[\"1024\"]}],\") and use a \",[\"$\",\"code\",null,{\"children\":[\"metric\"]}],\" compatible with E5  ie \",[\"$\",\"code\",null,{\"children\":[\"cosine\"]}],\".\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"import time\\n\\nindex_name = \\\"rerankers\\\"\\nexisting_indexes = [\\n    index_info[\\\"name\\\"] for index_info in pc.list_indexes()\\n]\\n\\n# check if index already exists (it shouldn't if this is first time)\\nif index_name not in existing_indexes:\\n    # if does not exist, create index\\n    pc.create_index(\\n        index_name,\\n        dimension=1024,  # dimensionality of e5-large\\n        metric='cosine',\\n        spec=spec\\n    )\\n    # wait for index to be initialized\\n    while not pc.describe_index(index_name).status['ready']:\\n        time.sleep(1)\\n\\n# connect to index\\nindex = pc.Index(index_name)\\ntime.sleep(1)\\n# view index stats\\nindex.describe_index_stats()\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We create a new function, `embed`, to handle embedding with our model. Within the function, we also include the handling of rate limit errors.\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"from pinecone_plugins.inference.core.client.exceptions import PineconeApiException\\n\\ndef embed(batch: list[str]) -\u003e list[float]:\\n    # create embeddings (exponential backoff to avoid RateLimitError)\\n    for j in range(5):  # max 5 retries\\n        try:\\n            res = pc.inference.embed(\\n                model=embed_model,\\n                inputs=batch,\\n                parameters={\\n                    \\\"input_type\\\": \\\"passage\\\",  # for docs/context/chunks\\n                    \\\"truncate\\\": \\\"END\\\",  # truncate to max length\\n                }\\n            )\\n            passed = True\\n        except PineconeApiException:\\n            time.sleep(2**j)  # wait 2^j seconds before retrying\\n            print(\\\"Retrying...\\\")\\n    if not passed:\\n        raise RuntimeError(\\\"Failed to create embeddings.\\\")\\n    # get embeddings\\n    embeds = [x[\\\"values\\\"] for x in res.data]\\n    return embeds\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We're now ready to begin populating the index using the E5 embedding model like so:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"from tqdm.auto import tqdm\\n\\nbatch_size = 96  # how many embeddings we create and insert at once\\n\\nfor i in tqdm(range(0, len(data), batch_size)):\\n    passed = False\\n    # find end of batch\\n    i_end = min(len(data), i+batch_size)\\n    # create batch\\n    batch = data[i:i_end]\\n    embeds = embed(batch[\\\"text\\\"])\\n    to_upsert = list(zip(batch[\\\"id\\\"], embeds, batch[\\\"metadata\\\"]))\\n    # upsert to Pinecone\\n    index.upsert(vectors=to_upsert)\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Our index is now populated and ready for us to query!\"]}],[\"$\",\"h3\",null,{\"className\":\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\",\"id\":\"Retrieval-Without-Reranking\",\"children\":[\"Retrieval Without Reranking\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Before reranking, let's see how our results look \",[\"$\",\"em\",null,{\"children\":[\"without\"]}],\" it. We will define a function called \",[\"$\",\"code\",null,{\"children\":[\"get_docs\"]}],\" to return documents using the first stage of retrieval only:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"def get_docs(query: str, top_k: int) -\u003e list[str]:\\n    # encode query\\n    res = pc.inference.embed(\\n        model=embed_model,\\n        inputs=[query],\\n        parameters={\\n            \\\"input_type\\\": \\\"query\\\",  # for queries\\n            \\\"truncate\\\": \\\"END\\\",  # truncate to max length\\n        }\\n    )\\n    xq = res.data[0][\\\"values\\\"]\\n    # search pinecone index\\n    res = index.query(vector=xq, top_k=top_k, include_metadata=True)\\n    # get doc text\\n    docs = [{\\n        \\\"id\\\": str(i),\\n        \\\"text\\\": x[\\\"metadata\\\"]['text']\\n    } for i, x in enumerate(res[\\\"matches\\\"])]\\n    return docs\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Let's ask about \",[\"$\",\"strong\",null,{\"children\":[\"R\"]}],\"einforcement \",[\"$\",\"strong\",null,{\"children\":[\"L\"]}],\"earning with \",[\"$\",\"strong\",null,{\"children\":[\"H\"]}],\"uman \",[\"$\",\"strong\",null,{\"children\":[\"F\"]}],\"eedback  a popular fine-tuning method behind the sudden performance gains demonstrated by ChatGPT when it was released.\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2a\"}],[\"$\",\"$L29\",null,{\"file\":{\"_key\":\"bbe9ae8537e5\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-0f23cad0b913da3b50f37636136f18e74fad417c-ipynb\",\"_type\":\"reference\"}}}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We get reasonable performance here  notably relevant chunks of text:\"]}],[\"$\",\"div\",null,{\"className\":\"w-full overflow-x-auto my-6\",\"children\":[\"$\",\"table\",null,{\"className\":\"w-full text-text-primary table-auto border-collapse text-sm md:text-base\\n        [\u0026_td]:border [\u0026_td]:border-border [\u0026_td]:p-3 md:[\u0026_td]:p-4 [\u0026_td]:align-top\\n        [\u0026\u003etbody\u003etr\u003etd:first-of-type]:text-left\\n        [\u0026_th]:border [\u0026_th]:border-border [\u0026_th]:p-3 md:[\u0026_th]:p-4 [\u0026_th]:align-top [\u0026_th]:font-medium\\n        [\u0026_th:first-of-type]:text-left undefined\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",\"0\",{\"className\":\"text-center\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"Document\"}]}],[\"$\",\"th\",\"1\",{\"className\":\"text-center\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"Chunk\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"39532801-57cc-431b-99fe-76050cb3a702\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"0\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"enabling significant improvements in their performance\\\"\"}]}]]}],[\"$\",\"tr\",\"3a6f6c54-397a-4e47-8862-6a0ab0b4eaa0\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"0\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"iteratively aligning the models' responses more closely with human expectations and preferences\\\"\"}]}]]}],[\"$\",\"tr\",\"a639e5bf-9657-4682-87a2-416e81da8028\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"0\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness\\\"\"}]}]]}],[\"$\",\"tr\",\"c9e11ffa-c89d-4c3e-b826-7a354d75a5ab\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"1\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"increasingly popular technique for reducing harmful behaviors in large language models\\\"\"}]}]]}]]}]]}]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"The remaining documents and text cover RLHF but don't answer our specific question of \",[\"$\",\"em\",null,{\"children\":[\"\\\"why we would want to do rlhf?\\\"\"]}],\".\"]}],[\"$\",\"h3\",null,{\"className\":\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\",\"id\":\"Reranking-Responses\",\"children\":[\"Reranking Responses\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We will use Pinecone's rerank endpoint for this. We use the same Pinecone client but now hit \",[\"$\",\"code\",null,{\"children\":[\"inference.rerank\"]}],\" like so:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"rerank_name = \\\"bge-reranker-v2-m3\\\"\\n\\nrerank_docs = pc.inference.rerank(\\n    model=rerank_name,\\n    query=query,\\n    documents=docs,\\n    top_n=25,\\n    return_documents=True\\n)\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"This returns a \",[\"$\",\"code\",null,{\"children\":[\"RerankResult\"]}],\" object:\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"RerankResult(\\n  model='bge-reranker-v2-m3',\\n  data=[\\n    { index=1, score=0.9071478,\\n      document={id=\\\"1\\\", text=\\\"RLHF Response ! I...\\\"} },\\n    { index=9, score=0.6954414,\\n      document={id=\\\"9\\\", text=\\\"team, instead of ...\\\"} },\\n    ... (21 more documents) ...,\\n    { index=17, score=0.13420755,\\n      document={id=\\\"17\\\", text=\\\"helpfulness and h...\\\"} },\\n    { index=23, score=0.11417085,\\n      document={id=\\\"23\\\", text=\\\"responses respons...\\\"} }\\n  ],\\n  usage={'rerank_units': 1}\\n)\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We access the text content of the docs via \",[\"$\",\"code\",null,{\"children\":[\"rerank_docs.data[0][\\\"document\\\"][\\\"text\\\"]\"]}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Let's create a function that will allow us to quickly compare original vs. reranked results.\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2b\"}],[\"$\",\"$L28\",null,{\"code\":\"def compare(query: str, top_k: int, top_n: int):\\n    # first get vec search results\\n    top_k_docs = get_docs(query, top_k=top_k)\\n    # rerank\\n    top_n_docs = pc.inference.rerank(\\n        model=rerank_name,\\n        query=query,\\n        documents=docs,\\n        top_n=top_n,\\n        return_documents=True\\n    )\\n    original_docs = []\\n    reranked_docs = []\\n    # compare order change\\n    print(\\\"[ORIGINAL] -\u003e [NEW]\\\")\\n    for i, doc in enumerate(top_n_docs.data):\\n        print(str(doc.index)+\\\"\\\\t-\u003e\\\\t\\\"+str(i))\\n        if i != doc.index:\\n            reranked_docs.append(f\\\"[{doc.index}]\\\\n\\\"+doc[\\\"document\\\"][\\\"text\\\"])\\n            original_docs.append(f\\\"[{i}]\\\\n\\\"+top_k_docs[i]['text'])\\n        else:\\n            reranked_docs.append(doc[\\\"document\\\"][\\\"text\\\"])\\n            original_docs.append(None)\\n    # print results\\n    for orig, rerank in zip(original_docs, reranked_docs):\\n        if not orig:\\n            print(f\\\"SAME:\\\\n{rerank}\\\\n\\\\n---\\\\n\\\")\\n        else:\\n            print(f\\\"ORIGINAL:\\\\n{orig}\\\\n\\\\nRERANKED:\\\\n{rerank}\\\\n\\\\n---\\\\n\\\")\",\"language\":\"$undefined\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We start with our RLHF query. This time, we do a more standard retrieval-rerank process of retrieving 25 documents (\",[\"$\",\"code\",null,{\"children\":[\"top_k=25\"]}],\") and reranking to the top three documents (\",[\"$\",\"code\",null,{\"children\":[\"top_n=3\"]}],\").\"]}],[\"$\",\"$19\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L27\",null,{\"moduleIds\":\"$2a\"}],[\"$\",\"$L29\",null,{\"file\":{\"_key\":\"d744b901d2d7\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-c3d071ccb9aadca3c841b14e5557b2b27ef8d1e1-ipynb\",\"_type\":\"reference\"}}}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Looking at these, we have dropped the one relevant chunk of text from document \",[\"$\",\"code\",null,{\"children\":[\"1\"]}],\" and \",[\"$\",\"em\",null,{\"children\":[\"no relevant\"]}],\" chunks of text from document \",[\"$\",\"code\",null,{\"children\":[\"2\"]}],\"  the following relevant pieces of information now replace these:\"]}],[\"$\",\"div\",null,{\"className\":\"w-full overflow-x-auto my-6\",\"children\":[\"$\",\"table\",null,{\"className\":\"w-full text-text-primary table-auto border-collapse text-sm md:text-base\\n        [\u0026_td]:border [\u0026_td]:border-border [\u0026_td]:p-3 md:[\u0026_td]:p-4 [\u0026_td]:align-top\\n        [\u0026\u003etbody\u003etr\u003etd:first-of-type]:text-left\\n        [\u0026_th]:border [\u0026_th]:border-border [\u0026_th]:p-3 md:[\u0026_th]:p-4 [\u0026_th]:align-top [\u0026_th]:font-medium\\n        [\u0026_th:first-of-type]:text-left undefined\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",\"0\",{\"className\":\"text-center\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"Original Position\"}]}],[\"$\",\"th\",\"1\",{\"className\":\"text-center\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"Rerank Position\"}]}],[\"$\",\"th\",\"2\",{\"className\":\"text-center\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"Chunk\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"3c2e4688-fe9f-41ed-a264-7584feef8b17\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"23\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"1\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"train language models that act as helpful and harmless assistants\\\"\"}]}]]}],[\"$\",\"tr\",\"320e3e8e-516f-46b6-825a-6821532eedff\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"23\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"1\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"RLHF training also improves honesty\\\"\"}]}]]}],[\"$\",\"tr\",\"24376f08-8bd5-405b-a81c-3e6dfc685c84\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"23\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"1\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"RLHF improves helpfulness and harmlessness by a huge margin\\\"\"}]}]]}],[\"$\",\"tr\",\"07a1ebbe-fda2-4cab-ac94-971cf5a798eb\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"23\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"1\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"enhance the capabilities of large models\\\"\"}]}]]}],[\"$\",\"tr\",\"2bdea449-bcb5-4384-96cb-f0b29e223e55\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"14\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"2\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"the model outputs safe responses\\\"\"}]}]]}],[\"$\",\"tr\",\"f6b91520-dedc-404c-b821-4701cf4d5984\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"14\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"2\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"often more detailed than what the average annotator writes\\\"\"}]}]]}],[\"$\",\"tr\",\"6f047bf3-6dc2-4d29-8b74-ccd50de8e53e\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"14\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"2\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"RLHF to reach the model how to write more nuanced responses\\\"\"}]}]]}],[\"$\",\"tr\",\"c435d4ac-8faf-4d1e-be6d-e0617bb7c42f\",{\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"14\"}]}],[\"$\",\"td\",\"1\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"2\"}]}],[\"$\",\"td\",\"2\",{\"className\":\"text-center [\u0026\u003espan]:space-y-2 [\u0026_li\u003espan]:mt-0 [\u0026_ul]:mt-0\",\"children\":[\"$\",\"span\",\"outer\",{\"className\":\"$undefined\",\"children\":\"\\\"make the model more robust to jailbreak attempts\\\"\"}]}]]}]]}]]}]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"After reranking, we have\",[\"$\",\"em\",null,{\"children\":[\"far more\"]}],\"relevant information. Naturally, this can result in significantly better performance for RAG. It means we maximize relevant information while minimizing noise input into our LLM.\"]}],[\"$\",\"hr\",null,{\"className\":\"my-8 border-solid\"}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"Reranking is one of the simplest methods for dramatically improving recall performance in \",[\"$\",\"strong\",null,{\"children\":[\"R\"]}],\"etrieval \",[\"$\",\"strong\",null,{\"children\":[\"A\"]}],\"ugmented \",[\"$\",\"strong\",null,{\"children\":[\"G\"]}],\"eneration (RAG) or any other retrieval-based pipeline.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"We've explored why \",[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/refine-with-rerank/\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"rerankers can provide so much better performance\"]}],\" than their embedding model counterparts  and how a two-stage retrieval system allows us to get the best of both, enabling search at scale while maintaining quality performance.\"]}],[\"$\",\"hr\",null,{\"className\":\"my-8 border-solid\"}],[\"$\",\"h2\",null,{\"className\":\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\",\"id\":\"References\",\"children\":[\"References\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"[1] \",[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://www.anthropic.com/index/100k-context-windows\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"Introducing 100K Context Windows\"]}],\" (2023), Anthropic\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"[2] N. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, P. Liang, \",[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://arxiv.org/abs/2307.03172\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"Lost in the Middle: How Language Models Use Long Contexts\"]}],\" (2023),\"]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-body leading-6 [\u0026_b]:font-semibold [\u0026_strong]:font-semibold text-text-primary\",\"children\":[\"[3] N. Reimers, I. Gurevych, \",[\"$\",\"a\",null,{\"id\":\"$undefined\",\"href\":\"https://arxiv.org/pdf/1908.10084.pdf\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\",\"children\":[\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"]}],\" (2019), UKP-TUDA\"]}]],[\"$\",\"div\",null,{\"className\":\"lg:hidden pt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\",\"children\":[\"Share:\",\" \",[\"$\",\"div\",null,{\"className\":\"flex items-start\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/series/rag/rerankers\",\"target\":\"_blank\",\"aria-label\":\"Share to Twitter\",\"className\":\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\",\"children\":[\"$\",\"svg\",null,{\"width\":\"14\",\"height\":\"12\",\"viewBox\":\"0 0 14 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\",\"className\":\"fill-text-secondary group-hover:fill-text-primary transition-colors\"}]}]}],[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/series/rag/rerankers\",\"target\":\"_blank\",\"aria-label\":\"Share to LinkedIn\",\"className\":\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\",\"children\":[\"$\",\"svg\",null,{\"width\":\"13\",\"height\":\"13\",\"viewBox\":\"0 0 13 13\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\",\"className\":\"fill-text-secondary group-hover:fill-text-primary transition-colors\"}]}]}],[\"$\",\"a\",null,{\"href\":\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/series/rag/rerankers\",\"target\":\"_blank\",\"aria-label\":\"Share to Hacker News\",\"className\":\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\",\"children\":[\"$\",\"svg\",null,{\"width\":\"13\",\"height\":\"13\",\"viewBox\":\"0 0 13 13\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\",\"className\":\"fill-text-secondary group-hover:fill-text-primary transition-colors\"}]}]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"mt-10\",\"children\":[\"$\",\"$L2c\",null,{\"pageTitle\":\"Rerankers and Two-Stage Retrieval\",\"url\":\"https://www.pinecone.io/learn/series/rag/rerankers\"}]}]]}]}]}]}]]\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Rerankers and Two-Stage Retrieval | Pinecone\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn how to build better retrieval augmented generation (RAG) pipelines for LLMs, search, and recommendation. In this chapter we explore two-stage retrieval and the incredible accuracy of reranker models.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"canonical\",\"href\":\"https://www.pinecone.io/learn/series/rag/rerankers/\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Rerankers and Two-Stage Retrieval | Pinecone\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"Learn how to build better retrieval augmented generation (RAG) pipelines for LLMs, search, and recommendation. In this chapter we explore two-stage retrieval and the incredible accuracy of reranker models.\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://www.pinecone.io/api/og/?title=Rerankers%20and%20Two-Stage%20Retrieval\u0026category=Learn\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:site\",\"content\":\"@pinecone\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:creator\",\"content\":\"@pinecone\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Rerankers and Two-Stage Retrieval\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"Learn how to build better retrieval augmented generation (RAG) pipelines for LLMs, search, and recommendation. In this chapter we explore two-stage retrieval and the incredible accuracy of reranker models.\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:image\",\"content\":\"https://www.pinecone.io/api/og/?title=Rerankers%20and%20Two-Stage%20Retrieval\u0026category=Learn\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"meta\",\"15\",{\"name\":\"next-size-adjust\"}]]\n9:null\n"])</script><script>self.__next_f.push([1,"2e:I[33689,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chunks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"default\"]\n30:I[65878,[\"6281\",\"static/chunks/08ffe114-ffac5ea08ea3b971.js\",\"6115\",\"static/chunks/8dc5345f-e183b54149bccc09.js\",\"3712\",\"static/chunks/3627521c-c9e8a1c79edd7561.js\",\"9573\",\"static/chunks/d8f92815-98c12721cfa98058.js\",\"6682\",\"static/chunks/07115393-01c29098453ff257.js\",\"2331\",\"static/chunks/3204862b-d5e6e14a59dedbca.js\",\"2972\",\"static/chunks/2972-1470746df2c539e6.js\",\"9291\",\"static/chunks/9291-ed64be53c7295d4f.js\",\"8076\",\"static/chunks/8076-55c63a69d26d910d.js\",\"466\",\"static/chunks/466-1359f016cbb61606.js\",\"1101\",\"static/chunks/1101-229bbbbd02dcbc67.js\",\"5757\",\"static/chunks/5757-36d92f2a990b34ca.js\",\"6951\",\"static/chun"])</script><script>self.__next_f.push([1,"ks/6951-4bc4103c43c657d3.js\",\"4715\",\"static/chunks/4715-00d7cff39ed87d8f.js\",\"6327\",\"static/chunks/6327-3a5eadbcfae2ffed.js\",\"9159\",\"static/chunks/9159-699bd0d299123ab9.js\",\"3617\",\"static/chunks/3617-d8a85a8d442778a6.js\",\"3335\",\"static/chunks/3335-40843fa911f88e9b.js\",\"9111\",\"static/chunks/9111-1485527d67ca5f72.js\",\"4089\",\"static/chunks/4089-02ac20c12970a615.js\",\"45\",\"static/chunks/45-4c9a51cfc08d6de3.js\",\"9339\",\"static/chunks/9339-8fe59c1ff0393aeb.js\",\"6591\",\"static/chunks/6591-75e707f3cacea3a0.js\",\"8098\",\"static/chunks/8098-38edba19209c5d5b.js\",\"9316\",\"static/chunks/9316-8148cc88da752ca5.js\",\"4853\",\"static/chunks/4853-48e50bc54d42e954.js\",\"3927\",\"static/chunks/app/(frontend)/(shared-layout)/learn/series/%5BseriesSlug%5D/%5BchapterSlug%5D/layout-30cf9885a60eecd4.js\"],\"Image\"]\n2d:T1249,"])</script><script>self.__next_f.push([1,"\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e@\u003c/mi\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e#\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ef\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eu\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e#\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ef\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmtext\u003e\u003c/mtext\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003erecall@K = \\frac{\\#\\;of\\;relevant\\;docs\\;returned}{\\#\\;of\\;relevant\\;docs\\;in\\;dataset}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003erec\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.01968em;\"\u003ell\u003c/span\u003e\u003cspan class=\"mord\"\u003e@\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eK\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:2.2519em;vertical-align:-0.8804em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mopen nulldelimiter\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.3714em;\"\u003e\u003cspan style=\"top:-2.314em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord\"\u003e#\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ere\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.01968em;\"\u003el\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ev\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ean\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eocs\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ein\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ese\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.23em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.677em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord\"\u003e#\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ere\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.01968em;\"\u003el\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ev\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ean\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eocs\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ere\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eu\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8804em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e"])</script><script>self.__next_f.push([1,"25:[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"$2d\"}}]\n2f:[\"chapterSlug\",\"rerankers\",\"d\"]\n35:[\"edb14340181b\"]\n34:{\"_key\":\"3824add52b350\",\"_type\":\"span\",\"marks\":\"$35\",\"text\":\"Retrieval Augmented Generation (RAG)\"}\n37:[]\n36:{\"_key\":\"331f00c8ea33\",\"_type\":\"span\",\"marks\":\"$37\",\"text\":\" is an overloaded term. It promises the world, but after developing a RAG pipeline, there are many of us left wondering why it doesn't work as well as we had expected.\"}\n33:[\"$34\",\"$36\"]\n39:{\"_key\":\"edb14340181b\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/retrieval-augmented-generation/\"}\n38:[\"$39\"]\n32:{\"_key\":\"b5c3d8ff292a\",\"_type\":\"block\",\"children\":\"$33\",\"markDefs\":\"$38\",\"style\":\"normal\"}\n3d:[]\n3c:{\"_key\":\"e1c9a65b7ba30\",\"_type\":\"span\",\"marks\":\"$3d\",\"text\":\"As with most tools, RAG is easy to use but hard to master. The truth is that there is more to RAG than putting documents into a vector DB and adding an LLM on top. That \"}\n3f:[\"em\"]\n3e:{\"_key\":\"e1c9a65b7ba31\",\"_type\":\"span\",\"marks\":\"$3f\",\"text\":\"can work\"}\n41:[]\n40:{\"_key\":\"e1c9a65b7ba32\",\"_type\":\"span\",\"marks\":\"$41\",\"text\":\", but it won't always.\"}\n3b:[\"$3c\",\"$3e\",\"$40\"]\n42:[]\n3a:{\"_key\":\"5cf640b84731\",\"_type\":\"block\",\"children\":\"$3b\",\"markDefs\":\"$42\",\"style\":\"normal\"}\n46:[]\n45:{\"_key\":\"f1f64c6528800\",\"_type\":\"span\",\"marks\":\"$46\",\"text\":\"This ebook aims to tell you what to do when out-of-the-box RAG \"}\n48:[\"em\"]\n47:{\"_key\":\"f1f64c6528801\",\"_type\":\"span\",\"marks\":\"$48\",\"text\":\"doesn't\"}\n4a:[]\n49:{\"_key\":\"f1f64c6528802\",\"_type\":\"span\",\"marks\":\"$4a\",\"text\":\" work. In this first chapter, we'll look at what is often the easiest and fastest to implement solution for suboptimal RAG pipelines  we'll be learning about \"}\n4c:[\"em\"]\n4b:{\"_key\":\"f1f64c6528803\",\"_type\":\"span\",\"marks\":\"$4c\",\"text\":\"rerankers\"}\n4e:[]\n4d:{\"_key\":\"f1f64c6528804\",\"_type\":\"span\",\"marks\":\"$4e\",\"text\":\".\"}\n44:[\"$45\",\"$47\",\"$49\",\"$4b\",\"$4d\"]\n4f:[]\n43:{\"_key\":\"30e134d596b0\",\"_type\":\"block\",\"children\":\"$44\",\"markDefs\":\"$4f\",\"style\":\"normal\"}\n53:[]\n52:{\"_key\":\"82f45bf4e9d"])</script><script>self.__next_f.push([1,"e\",\"_type\":\"span\",\"marks\":\"$53\",\"text\":\"\"}\n51:[\"$52\"]\n54:[]\n50:{\"_key\":\"067464a393a2\",\"_type\":\"block\",\"children\":\"$51\",\"markDefs\":\"$54\",\"style\":\"normal\"}\n55:{\"_key\":\"15466780da58\",\"_type\":\"video\",\"caption\":\"Video companion for this chapter.\",\"videoURL\":\"https://www.youtube.com/watch?v=Uh9bYiVrW_s\"}\n56:{\"_key\":\"ecf3fb10fbdc\",\"_type\":\"horizontalLine\",\"style\":\"Solid\"}\n5a:[]\n59:{\"_key\":\"36c051a681020\",\"_type\":\"span\",\"marks\":\"$5a\",\"text\":\"Recall vs. Context Windows\"}\n58:[\"$59\"]\n5b:[]\n57:{\"_key\":\"e5d649e8cc7e\",\"_type\":\"block\",\"children\":\"$58\",\"markDefs\":\"$5b\",\"style\":\"h2\"}\n5f:[]\n5e:{\"_key\":\"862f76e935f40\",\"_type\":\"span\",\"marks\":\"$5f\",\"text\":\"Before jumping into the solution, let's talk about the problem. With RAG, we are performing a \"}\n61:[\"em\"]\n60:{\"_key\":\"862f76e935f41\",\"_type\":\"span\",\"marks\":\"$61\",\"text\":\"semantic search\"}\n63:[]\n62:{\"_key\":\"862f76e935f42\",\"_type\":\"span\",\"marks\":\"$63\",\"text\":\" across many text documents  these could be tens of thousands up to tens of billions of documents.\"}\n5d:[\"$5e\",\"$60\",\"$62\"]\n64:[]\n5c:{\"_key\":\"6fdf675f90e3\",\"_type\":\"block\",\"children\":\"$5d\",\"markDefs\":\"$64\",\"style\":\"normal\"}\n68:[]\n67:{\"_key\":\"001d8b81e5a50\",\"_type\":\"span\",\"marks\":\"$68\",\"text\":\"To ensure fast search times at scale, we typically use vector search  that is, we transform our text into vectors, place them all into a vector space, and compare their proximity to a query vector using a similarity metric like cosine similarity.\"}\n66:[\"$67\"]\n69:[]\n65:{\"_key\":\"4ae1f1b40a07\",\"_type\":\"block\",\"children\":\"$66\",\"markDefs\":\"$69\",\"style\":\"normal\"}\n6d:[]\n6c:{\"_key\":\"7cb27ba8d0370\",\"_type\":\"span\",\"marks\":\"$6d\",\"text\":\"For vector search to work, we need vectors. These vectors are essentially compressions of the \\\"meaning\\\" behind some text into (typically) 768 or 1536-dimensional vectors. There is some information loss because we're compressing this information into a single vector.\"}\n6b:[\"$6c\"]\n6e:[]\n6a:{\"_key\":\"963eddee4ce0\",\"_type\":\"block\",\"children\":\"$6b\",\"markDefs\":\"$6e\",\"style\":\"normal\"}\n72:[]\n71:{\"_key\":\"67e2f9f2cd630\","])</script><script>self.__next_f.push([1,"\"_type\":\"span\",\"marks\":\"$72\",\"text\":\"Because of this information loss, we often see that the top three (for example) vector search documents will miss relevant information. Unfortunately, the retrieval may return relevant information below our \"}\n74:[\"code\"]\n73:{\"_key\":\"67e2f9f2cd631\",\"_type\":\"span\",\"marks\":\"$74\",\"text\":\"top_k\"}\n76:[]\n75:{\"_key\":\"67e2f9f2cd632\",\"_type\":\"span\",\"marks\":\"$76\",\"text\":\" cutoff.\"}\n70:[\"$71\",\"$73\",\"$75\"]\n77:[]\n6f:{\"_key\":\"0c90e515cdb1\",\"_type\":\"block\",\"children\":\"$70\",\"markDefs\":\"$77\",\"style\":\"normal\"}\n7b:[]\n7a:{\"_key\":\"dda24ee4145b0\",\"_type\":\"span\",\"marks\":\"$7b\",\"text\":\"What do we do if relevant information at a lower position would help our LLM formulate a better response? The easiest approach is to increase the number of documents we're returning (increase \"}\n7d:[\"code\"]\n7c:{\"_key\":\"dda24ee4145b1\",\"_type\":\"span\",\"marks\":\"$7d\",\"text\":\"top_k\"}\n7f:[]\n7e:{\"_key\":\"dda24ee4145b2\",\"_type\":\"span\",\"marks\":\"$7f\",\"text\":\") and pass them all to the LLM.\"}\n79:[\"$7a\",\"$7c\",\"$7e\"]\n80:[]\n78:{\"_key\":\"b044a09e107d\",\"_type\":\"block\",\"children\":\"$79\",\"markDefs\":\"$80\",\"style\":\"normal\"}\n84:[]\n83:{\"_key\":\"13a4530a39540\",\"_type\":\"span\",\"marks\":\"$84\",\"text\":\"The metric we would measure here is \"}\n86:[\"em\"]\n85:{\"_key\":\"13a4530a39541\",\"_type\":\"span\",\"marks\":\"$86\",\"text\":\"recall\"}\n88:[]\n87:{\"_key\":\"13a4530a39542\",\"_type\":\"span\",\"marks\":\"$88\",\"text\":\"  meaning \\\"how many of the relevant documents are we retrieving\\\". Recall does not consider the total number of retrieved documents  so we can hack the metric and get \"}\n8a:[\"em\"]\n89:{\"_key\":\"13a4530a39543\",\"_type\":\"span\",\"marks\":\"$8a\",\"text\":\"perfect\"}\n8c:[]\n8b:{\"_key\":\"13a4530a39544\",\"_type\":\"span\",\"marks\":\"$8c\",\"text\":\" recall by returning \"}\n8e:[\"em\"]\n8d:{\"_key\":\"13a4530a39545\",\"_type\":\"span\",\"marks\":\"$8e\",\"text\":\"everything\"}\n90:[]\n8f:{\"_key\":\"13a4530a39546\",\"_type\":\"span\",\"marks\":\"$90\",\"text\":\".\"}\n82:[\"$83\",\"$85\",\"$87\",\"$89\",\"$8b\",\"$8d\",\"$8f\"]\n91:[]\n81:{\"_key\":\"ff0b1cfb75c3\",\"_type\":\"block\",\"children\":\"$82\",\"markDefs\":\"$91\",\"style\":\"normal\"}\n92:{\"_key\":\""])</script><script>self.__next_f.push([1,"68936a1772c5\",\"_type\":\"latex\",\"body\":\"recall@K = \\\\frac{\\\\#\\\\;of\\\\;relevant\\\\;docs\\\\;returned}{\\\\#\\\\;of\\\\;relevant\\\\;docs\\\\;in\\\\;dataset}\"}\n96:[]\n95:{\"_key\":\"09d45f86a8ca0\",\"_type\":\"span\",\"marks\":\"$96\",\"text\":\"Unfortunately, we cannot return everything. LLMs have limits on how much text we can pass to them  we call this limit the \"}\n98:[\"em\"]\n97:{\"_key\":\"09d45f86a8ca1\",\"_type\":\"span\",\"marks\":\"$98\",\"text\":\"context window\"}\n9a:[]\n99:{\"_key\":\"09d45f86a8ca2\",\"_type\":\"span\",\"marks\":\"$9a\",\"text\":\". Some LLMs have huge context windows, like Anthropic's Claude, with a context window of 100K tokens [1]. With that, we could fit many tens of pages of text  so could we return many documents (not quite all) and \\\"stuff\\\" the context window to improve recall?\"}\n94:[\"$95\",\"$97\",\"$99\"]\n9b:[]\n93:{\"_key\":\"ce406c57c353\",\"_type\":\"block\",\"children\":\"$94\",\"markDefs\":\"$9b\",\"style\":\"normal\"}\n9f:[]\n9e:{\"_key\":\"5d54cdfe38040\",\"_type\":\"span\",\"marks\":\"$9f\",\"text\":\"Again, no. We cannot use context stuffing because this reduces the LLM's \"}\na1:[\"em\"]\na0:{\"_key\":\"5d54cdfe38041\",\"_type\":\"span\",\"marks\":\"$a1\",\"text\":\"recall\"}\na3:[]\na2:{\"_key\":\"5d54cdfe38042\",\"_type\":\"span\",\"marks\":\"$a3\",\"text\":\" performance  note that this is the LLM recall, which is different from the retrieval recall we have been discussing so far.\"}\n9d:[\"$9e\",\"$a0\",\"$a2\"]\na4:[]\n9c:{\"_key\":\"95822269433b\",\"_type\":\"block\",\"children\":\"$9d\",\"markDefs\":\"$a4\",\"style\":\"normal\"}\na6:{\"_ref\":\"image-ca206b6ada9163bffad313e0e18feee0b460c768-1212x688-png\",\"_type\":\"reference\"}\na5:{\"_key\":\"ec94139f3550\",\"_type\":\"image\",\"alt\":\"When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place\",\"asset\":\"$a6\",\"caption\":\"When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place [2].\"}\naa:[]\na9:{\"_key\":\"e33ef08068190\",\"_type\":\"span\",\"marks\":\"$aa\",\"text\":\"LLM recall refers to the abil"])</script><script>self.__next_f.push([1,"ity of an LLM to find information from the text placed within its context window. Research shows that LLM recall degrades as we put more tokens in the context window [2]. LLMs are also less likely to follow instructions as we stuff the context window  so context stuffing is a bad idea.\"}\na8:[\"$a9\"]\nab:[]\na7:{\"_key\":\"83365ba70768\",\"_type\":\"block\",\"children\":\"$a8\",\"markDefs\":\"$ab\",\"style\":\"normal\"}\naf:[]\nae:{\"_key\":\"07d5b0ba245f0\",\"_type\":\"span\",\"marks\":\"$af\",\"text\":\"We can increase the number of documents returned by our vector DB to increase retrieval recall, but we cannot pass these to our LLM without damaging LLM recall.\"}\nad:[\"$ae\"]\nb0:[]\nac:{\"_key\":\"8666bc6e5baf\",\"_type\":\"block\",\"children\":\"$ad\",\"markDefs\":\"$b0\",\"style\":\"normal\"}\nb4:[]\nb3:{\"_key\":\"cdb8caa82d0b0\",\"_type\":\"span\",\"marks\":\"$b4\",\"text\":\"The solution to this issue is to maximize retrieval recall by retrieving plenty of documents and then maximize LLM recall by \"}\nb6:[\"em\"]\nb5:{\"_key\":\"cdb8caa82d0b1\",\"_type\":\"span\",\"marks\":\"$b6\",\"text\":\"minimizing\"}\nb8:[]\nb7:{\"_key\":\"cdb8caa82d0b2\",\"_type\":\"span\",\"marks\":\"$b8\",\"text\":\" the number of documents that make it to the LLM. To do that, we reorder retrieved documents and keep just the most relevant for our LLM  to do that, we use \"}\nba:[\"em\"]\nb9:{\"_key\":\"cdb8caa82d0b3\",\"_type\":\"span\",\"marks\":\"$ba\",\"text\":\"reranking\"}\nbc:[]\nbb:{\"_key\":\"cdb8caa82d0b4\",\"_type\":\"span\",\"marks\":\"$bc\",\"text\":\".\"}\nb2:[\"$b3\",\"$b5\",\"$b7\",\"$b9\",\"$bb\"]\nbd:[]\nb1:{\"_key\":\"e5aab68739fe\",\"_type\":\"block\",\"children\":\"$b2\",\"markDefs\":\"$bd\",\"style\":\"normal\"}\nc1:[]\nc0:{\"_key\":\"efde0a3161790\",\"_type\":\"span\",\"marks\":\"$c1\",\"text\":\"Power of Rerankers\"}\nbf:[\"$c0\"]\nc2:[]\nbe:{\"_key\":\"9538d0e3ba5d\",\"_type\":\"block\",\"children\":\"$bf\",\"markDefs\":\"$c2\",\"style\":\"h2\"}\nc6:[]\nc5:{\"_key\":\"2718ae36ff140\",\"_type\":\"span\",\"marks\":\"$c6\",\"text\":\"A \"}\nc8:[\"48c3ce857808\"]\nc7:{\"_key\":\"6cd15bfec99e\",\"_type\":\"span\",\"marks\":\"$c8\",\"text\":\"reranking model\"}\nca:[]\nc9:{\"_key\":\"e3ff4bcfaad0\",\"_type\":\"span\",\"marks\":\"$ca\",\"text\":\"  also known as a \"}\ncc:[\"strong\"]\ncb:{\"_k"])</script><script>self.__next_f.push([1,"ey\":\"2718ae36ff141\",\"_type\":\"span\",\"marks\":\"$cc\",\"text\":\"cross-encoder\"}\nce:[]\ncd:{\"_key\":\"2718ae36ff142\",\"_type\":\"span\",\"marks\":\"$ce\",\"text\":\"  is a type of model that, given a query and document pair, will output a similarity score. We use this score to reorder the documents by relevance to our query.\"}\nc4:[\"$c5\",\"$c7\",\"$c9\",\"$cb\",\"$cd\"]\nd0:{\"_key\":\"48c3ce857808\",\"_type\":\"link\",\"href\":\"https://docs.pinecone.io/models/bge-reranker-v2-m3\"}\ncf:[\"$d0\"]\nc3:{\"_key\":\"40888980cf60\",\"_type\":\"block\",\"children\":\"$c4\",\"markDefs\":\"$cf\",\"style\":\"normal\"}\nd2:{\"_ref\":\"image-906c3c0f8fe637840f134dbf966839ef89ac7242-3443x1641-png\",\"_type\":\"reference\"}\nd1:{\"_key\":\"f25be28644c9\",\"_type\":\"image\",\"alt\":\"A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.\",\"asset\":\"$d2\",\"caption\":\"A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.\"}\nd6:[]\nd5:{\"_key\":\"cd2123e8178b0\",\"_type\":\"span\",\"marks\":\"$d6\",\"text\":\"Search engineers have used rerankers in two-stage retrieval systems for \"}\nd8:[\"em\"]\nd7:{\"_key\":\"cd2123e8178b1\",\"_type\":\"span\",\"marks\":\"$d8\",\"text\":\"a long time\"}\nda:[]\nd9:{\"_key\":\"cd2123e8178b2\",\"_type\":\"span\",\"marks\":\"$da\",\"text\":\". In these two-stage systems, a first-stage model (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. Then, a second-stage model (the reranker) is used to rerank those documents retrieved by the first-stage model.\"}\nd4:[\"$d5\",\"$d7\",\"$d9\"]\ndb:[]\nd3:{\"_key\":\"8e082a19958e\",\"_type\":\"block\",\"children\":\"$d4\",\"markDefs\":\"$db\",\"style\":\"normal\"}\ndf:[]\nde:{\"_key\":\"f33b4e7c013e0\",\"_type\":\"span\",\"marks\":\"$df\",\"text\":\"We use two stages because retrieving a small set of documents from a large dataset is much faster than reranking a large set of documents  we'll discuss why this is the case soon  but TL;DR, rerankers are slow, and retrievers are \"}\ne1:[\"em\"]\ne0:{\"_key\":\"f33b4e7c013e1\",\"_type\":\"span\",\"marks\":\"$e1\",\"text\":\"fast\"}\ne3:[]\ne2:{\"_key\":\"f33b4e7c01"])</script><script>self.__next_f.push([1,"3e2\",\"_type\":\"span\",\"marks\":\"$e3\",\"text\":\".\"}\ndd:[\"$de\",\"$e0\",\"$e2\"]\ne4:[]\ndc:{\"_key\":\"bb69cbece890\",\"_type\":\"block\",\"children\":\"$dd\",\"markDefs\":\"$e4\",\"style\":\"normal\"}\ne8:[]\ne7:{\"_key\":\"129f5c517f870\",\"_type\":\"span\",\"marks\":\"$e8\",\"text\":\"Why Rerankers?\"}\ne6:[\"$e7\"]\ne9:[]\ne5:{\"_key\":\"e112003628b4\",\"_type\":\"block\",\"children\":\"$e6\",\"markDefs\":\"$e9\",\"style\":\"h3\"}\ned:[]\nec:{\"_key\":\"a28c8222d5fc0\",\"_type\":\"span\",\"marks\":\"$ed\",\"text\":\"If a reranker is so much slower, why bother using them? The answer is that rerankers are much more accurate than \"}\nef:[\"3eb2eed86fee\"]\nee:{\"_key\":\"7d5b0306bfbf\",\"_type\":\"span\",\"marks\":\"$ef\",\"text\":\"embedding models\"}\nf1:[]\nf0:{\"_key\":\"17911c1992cb\",\"_type\":\"span\",\"marks\":\"$f1\",\"text\":\".\"}\neb:[\"$ec\",\"$ee\",\"$f0\"]\nf3:{\"_key\":\"3eb2eed86fee\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/series/rag/embedding-models-rundown/\"}\nf2:[\"$f3\"]\nea:{\"_key\":\"933484ae975d\",\"_type\":\"block\",\"children\":\"$eb\",\"markDefs\":\"$f2\",\"style\":\"normal\"}\nf7:[]\nf6:{\"_key\":\"bcfc6d01f0c00\",\"_type\":\"span\",\"marks\":\"$f7\",\"text\":\"The intuition behind a bi-encoder's inferior accuracy is that bi-encoders must compress all of the possible meanings of a document into a single vector  meaning we lose information. Additionally, bi-encoders have no context on the query because we don't know the query until we receive it (we create embeddings before user query time).\"}\nf5:[\"$f6\"]\nf8:[]\nf4:{\"_key\":\"cd9ef31d35b9\",\"_type\":\"block\",\"children\":\"$f5\",\"markDefs\":\"$f8\",\"style\":\"normal\"}\nfc:[]\nfb:{\"_key\":\"42a387a53bda0\",\"_type\":\"span\",\"marks\":\"$fc\",\"text\":\"On the other hand, a reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Because we are running the reranker at user query time, we have the added benefit of analyzing our document's meaning specific to the user query  rather than trying to produce a generic, averaged meaning.\"}\nfa:[\"$fb\"]\nfd:[]\nf9:{\"_key\":\"7659ceb3516a\",\"_type\":\"block\",\"children\":\"$fa\",\"markDefs\":\"$fd\",\"style\":\"normal\"}\n101:[]\n100:{\"_key\":\"718"])</script><script>self.__next_f.push([1,"eaa9a9d920\",\"_type\":\"span\",\"marks\":\"$101\",\"text\":\"Rerankers avoid the information loss of bi-encoders  but they come with a different penalty  \"}\n103:[\"em\"]\n102:{\"_key\":\"718eaa9a9d921\",\"_type\":\"span\",\"marks\":\"$103\",\"text\":\"time\"}\n105:[]\n104:{\"_key\":\"718eaa9a9d922\",\"_type\":\"span\",\"marks\":\"$105\",\"text\":\".\"}\nff:[\"$100\",\"$102\",\"$104\"]\n106:[]\nfe:{\"_key\":\"123dfd4a4669\",\"_type\":\"block\",\"children\":\"$ff\",\"markDefs\":\"$106\",\"style\":\"normal\"}\n108:{\"_ref\":\"image-4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420-png\",\"_type\":\"reference\"}\n107:{\"_key\":\"a7138853e8fc\",\"_type\":\"image\",\"alt\":\"A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.\",\"asset\":\"$108\",\"caption\":\"A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.\"}\n10c:[]\n10b:{\"_key\":\"1053f9dd85eb0\",\"_type\":\"span\",\"marks\":\"$10c\",\"text\":\"When using bi-encoder models with vector search, we frontload all of the heavy transformer computation to when we are creating the initial vectors  that means that when a user queries our system, we have already created the vectors, so all we need to do is:\"}\n10a:[\"$10b\"]\n10d:[]\n109:{\"_key\":\"bb6e0fb9f6c7\",\"_type\":\"block\",\"children\":\"$10a\",\"markDefs\":\"$10d\",\"style\":\"normal\"}\n111:[]\n110:{\"_key\":\"b06f72eaec5b0\",\"_type\":\"span\",\"marks\":\"$111\",\"text\":\"Run a single transformer computation to create the query vector.\"}\n10f:[\"$110\"]\n112:[]\n10e:{\"_key\":\"a113b87e52e5\",\"_type\":\"block\",\"children\":\"$10f\",\"level\":1,\"listItem\":\"number\",\"markDefs\":\"$112\",\"style\":\"normal\"}\n116:[]\n115:{\"_key\":\"cf79269df47b0\",\"_type\":\"span\",\"marks\":\"$116\",\"text\":\"Compare the query vector to document vectors with \"}\n118:[\"em\"]\n117:{\"_key\":\"cf79269df47b1\",\"_type\":\"span\",\"marks\":\"$118\",\"text\":\"cosine similarity\"}\n11a:[]\n119:{\"_key\":\"cf79269df47b2\",\"_type\":\"span\",\"marks\":\"$11a\",\"text\":\" (or another light"])</script><script>self.__next_f.push([1,"weight metric).\"}\n114:[\"$115\",\"$117\",\"$119\"]\n11b:[]\n113:{\"_key\":\"a73fcb2a0faa\",\"_type\":\"block\",\"children\":\"$114\",\"level\":1,\"listItem\":\"number\",\"markDefs\":\"$11b\",\"style\":\"normal\"}\n11f:[]\n11e:{\"_key\":\"aa4a4d92a8110\",\"_type\":\"span\",\"marks\":\"$11f\",\"text\":\"With rerankers, we are not pre-computing anything. Instead, we're feeding our query and a single other document into the transformer, running a whole transformer inference step, and outputting a single similarity score.\"}\n11d:[\"$11e\"]\n120:[]\n11c:{\"_key\":\"4a6a143cbbfa\",\"_type\":\"block\",\"children\":\"$11d\",\"markDefs\":\"$120\",\"style\":\"normal\"}\n122:{\"_ref\":\"image-9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100-png\",\"_type\":\"reference\"}\n121:{\"_key\":\"a7cc58dc92aa\",\"_type\":\"image\",\"alt\":\"A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.\",\"asset\":\"$122\",\"caption\":\"A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.\"}\n126:[]\n125:{\"_key\":\"56cd29eda72e0\",\"_type\":\"span\",\"marks\":\"$126\",\"text\":\"Given 40M records, if we use a \"}\n128:[\"strong\"]\n127:{\"_key\":\"56cd29eda72e1\",\"_type\":\"span\",\"marks\":\"$128\",\"text\":\"small\"}\n12a:[]\n129:{\"_key\":\"56cd29eda72e2\",\"_type\":\"span\",\"marks\":\"$12a\",\"text\":\" reranking model like BERT on a V100 GPU  we'd be waiting more than 50 hours to return a single query result [3]. We can do the same in \u003c100ms with encoder models and vector search.\"}\n124:[\"$125\",\"$127\",\"$129\"]\n12b:[]\n123:{\"_key\":\"7eb32a9d8196\",\"_type\":\"block\",\"children\":\"$124\",\"markDefs\":\"$12b\",\"style\":\"normal\"}\n12f:[]\n12e:{\"_key\":\"7ced0fd75c590\",\"_type\":\"span\",\"marks\":\"$12f\",\"text\":\"Implementing Two-Stage Retrieval with Reranking\"}\n12d:[\"$12e\"]\n130:[]\n12c:{\"_key\":\"859de630f213\",\"_type\":\"block\",\"children\":\"$12d\",\"markDefs\":\"$130\",\"style\":\"h2\"}\n134:[]\n133:{\"_key\":\"2f030b1659650\",\"_type\":\"span\",\"marks\":\"$134\",\"text\":\"Now that we understand the idea and reason b"])</script><script>self.__next_f.push([1,"ehind two-stage retrieval with rerankers, let's see how to implement it (you can follow along with \"}\n136:[\"fee4b4819b21\"]\n135:{\"_key\":\"2f030b1659651\",\"_type\":\"span\",\"marks\":\"$136\",\"text\":\"this notebook\"}\n138:[]\n137:{\"_key\":\"2f030b1659652\",\"_type\":\"span\",\"marks\":\"$138\",\"text\":\". To begin we will set up our prerequisite libraries:\"}\n132:[\"$133\",\"$135\",\"$137\"]\n13a:{\"_key\":\"fee4b4819b21\",\"_type\":\"link\",\"href\":\"https://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb\"}\n139:[\"$13a\"]\n131:{\"_key\":\"2d919503698b\",\"_type\":\"block\",\"children\":\"$132\",\"markDefs\":\"$139\",\"style\":\"normal\"}\n13b:{\"_key\":\"2d2be6ace2bc\",\"_type\":\"code\",\"code\":\"!pip install -qU \\\\\\n    datasets==2.14.5 \\\\\\n    \\\"pinecone[grpc]\\\"==5.1.0\"}\n13f:[]\n13e:{\"_key\":\"33d3a0a9329c0\",\"_type\":\"span\",\"marks\":\"$13f\",\"text\":\"Data Preparation\"}\n13d:[\"$13e\"]\n140:[]\n13c:{\"_key\":\"f949883a420e\",\"_type\":\"block\",\"children\":\"$13d\",\"markDefs\":\"$140\",\"style\":\"h3\"}\n144:[]\n143:{\"_key\":\"1da42e3de5390\",\"_type\":\"span\",\"marks\":\"$144\",\"text\":\"Before setting up the retrieval pipeline, we need data to retrieve! We will use the \"}\n146:[\"aa3fdb860f66\",\"code\"]\n145:{\"_key\":\"1da42e3de5391\",\"_type\":\"span\",\"marks\":\"$146\",\"text\":\"jamescalam/ai-arxiv-chunked\"}\n148:[]\n147:{\"_key\":\"1da42e3de5392\",\"_type\":\"span\",\"marks\":\"$148\",\"text\":\" dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs  including the Llama 2, GPTQ, and GPT-4 papers.\"}\n142:[\"$143\",\"$145\",\"$147\"]\n14a:{\"_key\":\"aa3fdb860f66\",\"_type\":\"link\",\"href\":\"https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked\"}\n149:[\"$14a\"]\n141:{\"_key\":\"ff77c5ba4a8a\",\"_type\":\"block\",\"children\":\"$142\",\"markDefs\":\"$149\",\"style\":\"normal\"}\n14c:{\"_ref\":\"file-d858315997239ee9631257691107bed218131ef4-ipynb\",\"_type\":\"reference\"}\n14b:{\"_key\":\"fc2fd187d922\",\"_type\":\"colabFile\",\"asset\":\"$14c\"}\n150:[]\n14f:{\"_key\":\"dec9dfbd9a820\",\"_type\":\"span\",\"marks\":\"$150\",\"text\":\"The dataset contains 41.5K pre-chunked records. Each record is 1-2 paragraphs long and includes addition"])</script><script>self.__next_f.push([1,"al metadata about the paper from which it comes. Here is an example:\"}\n14e:[\"$14f\"]\n151:[]\n14d:{\"_key\":\"bac9a0c8fc8b\",\"_type\":\"block\",\"children\":\"$14e\",\"markDefs\":\"$151\",\"style\":\"normal\"}\n153:{\"_ref\":\"file-d4e771a8d321b15576b6b3f72644cdf141c48db2-ipynb\",\"_type\":\"reference\"}\n152:{\"_key\":\"581ef5269e22\",\"_type\":\"colabFile\",\"asset\":\"$153\"}\n157:[]\n156:{\"_key\":\"794e53bcba950\",\"_type\":\"span\",\"marks\":\"$157\",\"text\":\"We'll be feeding this data into Pinecone, so let's reformat the dataset to be more Pinecone-friendly when it does come to the later embed and index process. The format will contain \"}\n159:[\"code\"]\n158:{\"_key\":\"794e53bcba951\",\"_type\":\"span\",\"marks\":\"$159\",\"text\":\"id\"}\n15b:[]\n15a:{\"_key\":\"794e53bcba952\",\"_type\":\"span\",\"marks\":\"$15b\",\"text\":\", \"}\n15d:[\"code\"]\n15c:{\"_key\":\"794e53bcba953\",\"_type\":\"span\",\"marks\":\"$15d\",\"text\":\"text\"}\n15f:[]\n15e:{\"_key\":\"794e53bcba954\",\"_type\":\"span\",\"marks\":\"$15f\",\"text\":\" (which we will embed), and \"}\n161:[\"code\"]\n160:{\"_key\":\"794e53bcba955\",\"_type\":\"span\",\"marks\":\"$161\",\"text\":\"metadata\"}\n163:[]\n162:{\"_key\":\"794e53bcba956\",\"_type\":\"span\",\"marks\":\"$163\",\"text\":\". For this example, we won't use metadata, but it can be helpful to include if we want to do \"}\n165:[\"f4f8615ff21a\"]\n164:{\"_key\":\"794e53bcba957\",\"_type\":\"span\",\"marks\":\"$165\",\"text\":\"metadata filtering\"}\n167:[]\n166:{\"_key\":\"794e53bcba958\",\"_type\":\"span\",\"marks\":\"$167\",\"text\":\" in the future.\"}\n155:[\"$156\",\"$158\",\"$15a\",\"$15c\",\"$15e\",\"$160\",\"$162\",\"$164\",\"$166\"]\n169:{\"_key\":\"f4f8615ff21a\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/vector-search-filtering/\"}\n168:[\"$169\"]\n154:{\"_key\":\"998d5f945055\",\"_type\":\"block\",\"children\":\"$155\",\"markDefs\":\"$168\",\"style\":\"normal\"}\n16b:{\"_ref\":\"file-65bb438d978ee37cc404d7cc5f646de0443053e6-ipynb\",\"_type\":\"reference\"}\n16a:{\"_key\":\"b2d7f4c0f965\",\"_type\":\"colabFile\",\"asset\":\"$16b\"}\n16f:[]\n16e:{\"_key\":\"4f813c0a20e20\",\"_type\":\"span\",\"marks\":\"$16f\",\"text\":\"Embed and Index\"}\n16d:[\"$16e\"]\n170:[]\n16c:{\"_key\":\"43fcccff3909\",\"_type\":\"block\",\"children\":\"$16d\",\"markDefs\":\"$170\",\"style\":\"h3\"}\n174"])</script><script>self.__next_f.push([1,":[]\n173:{\"_key\":\"ceaee4de0b920\",\"_type\":\"span\",\"marks\":\"$174\",\"text\":\"To store everything in the vector DB, we need to encode everything with an embedding / bi-encoder model. We will use the open source \"}\n176:[\"code\"]\n175:{\"_key\":\"ceaee4de0b921\",\"_type\":\"span\",\"marks\":\"$176\",\"text\":\"multilingial-e5-large\"}\n178:[]\n177:{\"_key\":\"ceaee4de0b922\",\"_type\":\"span\",\"marks\":\"$178\",\"text\":\" via Pinecone Inference. We need a [free Pinecone API key](https://app.pinecone.io) to authenticate ourselves via the client:\"}\n172:[\"$173\",\"$175\",\"$177\"]\n179:[]\n171:{\"_key\":\"7b2bee46847d\",\"_type\":\"block\",\"children\":\"$172\",\"markDefs\":\"$179\",\"style\":\"normal\"}\n17a:{\"_key\":\"5d09c9702ff6\",\"_type\":\"code\",\"code\":\"from pinecone.grpc import PineconeGRPC\\n\\n# get API key from app.pinecone.io\\napi_key = \\\"PINECONE_API_KEY\\\"\\n\\nembed_model = \\\"multilingual-e5-large\\\"\\n\\n# configure client\\npc = PineconeGRPC(api_key=api_key)\"}\n17e:[]\n17d:{\"_key\":\"f948d5514ded0\",\"_type\":\"span\",\"marks\":\"$17e\",\"text\":\"Now, we create our vector DB to store our vectors. We set \"}\n180:[\"code\"]\n17f:{\"_key\":\"cc2a19866b781\",\"_type\":\"span\",\"marks\":\"$180\",\"text\":\"dimension\"}\n182:[]\n181:{\"_key\":\"cc2a19866b782\",\"_type\":\"span\",\"marks\":\"$182\",\"text\":\" equal to the dimensionality of E5 large (\"}\n184:[\"code\"]\n183:{\"_key\":\"cc2a19866b783\",\"_type\":\"span\",\"marks\":\"$184\",\"text\":\"1024\"}\n186:[]\n185:{\"_key\":\"cc2a19866b784\",\"_type\":\"span\",\"marks\":\"$186\",\"text\":\") and use a \"}\n188:[\"code\"]\n187:{\"_key\":\"cc2a19866b785\",\"_type\":\"span\",\"marks\":\"$188\",\"text\":\"metric\"}\n18a:[]\n189:{\"_key\":\"cc2a19866b786\",\"_type\":\"span\",\"marks\":\"$18a\",\"text\":\" compatible with E5  ie \"}\n18c:[\"code\"]\n18b:{\"_key\":\"cc2a19866b787\",\"_type\":\"span\",\"marks\":\"$18c\",\"text\":\"cosine\"}\n18e:[]\n18d:{\"_key\":\"cc2a19866b788\",\"_type\":\"span\",\"marks\":\"$18e\",\"text\":\".\"}\n17c:[\"$17d\",\"$17f\",\"$181\",\"$183\",\"$185\",\"$187\",\"$189\",\"$18b\",\"$18d\"]\n18f:[]\n17b:{\"_key\":\"029d4033c1f2\",\"_type\":\"block\",\"children\":\"$17c\",\"markDefs\":\"$18f\",\"style\":\"normal\"}\n190:{\"_key\":\"ed905184f11e\",\"_type\":\"code\",\"code\":\"import time\\n\\nindex_name = \\\"rerankers\\\"\\nexisti"])</script><script>self.__next_f.push([1,"ng_indexes = [\\n    index_info[\\\"name\\\"] for index_info in pc.list_indexes()\\n]\\n\\n# check if index already exists (it shouldn't if this is first time)\\nif index_name not in existing_indexes:\\n    # if does not exist, create index\\n    pc.create_index(\\n        index_name,\\n        dimension=1024,  # dimensionality of e5-large\\n        metric='cosine',\\n        spec=spec\\n    )\\n    # wait for index to be initialized\\n    while not pc.describe_index(index_name).status['ready']:\\n        time.sleep(1)\\n\\n# connect to index\\nindex = pc.Index(index_name)\\ntime.sleep(1)\\n# view index stats\\nindex.describe_index_stats()\"}\n194:[]\n193:{\"_key\":\"283923e9dc23\",\"_type\":\"span\",\"marks\":\"$194\",\"text\":\"We create a new function, `embed`, to handle embedding with our model. Within the function, we also include the handling of rate limit errors.\"}\n192:[\"$193\"]\n195:[]\n191:{\"_key\":\"fd48b611d9a1\",\"_type\":\"block\",\"children\":\"$192\",\"markDefs\":\"$195\",\"style\":\"normal\"}\n196:{\"_key\":\"684b7df32dcb\",\"_type\":\"code\",\"code\":\"from pinecone_plugins.inference.core.client.exceptions import PineconeApiException\\n\\ndef embed(batch: list[str]) -\u003e list[float]:\\n    # create embeddings (exponential backoff to avoid RateLimitError)\\n    for j in range(5):  # max 5 retries\\n        try:\\n            res = pc.inference.embed(\\n                model=embed_model,\\n                inputs=batch,\\n                parameters={\\n                    \\\"input_type\\\": \\\"passage\\\",  # for docs/context/chunks\\n                    \\\"truncate\\\": \\\"END\\\",  # truncate to max length\\n                }\\n            )\\n            passed = True\\n        except PineconeApiException:\\n            time.sleep(2**j)  # wait 2^j seconds before retrying\\n            print(\\\"Retrying...\\\")\\n    if not passed:\\n        raise RuntimeError(\\\"Failed to create embeddings.\\\")\\n    # get embeddings\\n    embeds = [x[\\\"values\\\"] for x in res.data]\\n    return embeds\"}\n19a:[]\n199:{\"_key\":\"4d4448a2ef4e0\",\"_type\":\"span\",\"marks\":\"$19a\",\"text\":\"We're now ready to begin populating the index using t"])</script><script>self.__next_f.push([1,"he E5 embedding model like so:\"}\n198:[\"$199\"]\n19b:[]\n197:{\"_key\":\"c2af4093a270\",\"_type\":\"block\",\"children\":\"$198\",\"markDefs\":\"$19b\",\"style\":\"normal\"}\n19c:{\"_key\":\"6f1fbba90a52\",\"_type\":\"code\",\"code\":\"from tqdm.auto import tqdm\\n\\nbatch_size = 96  # how many embeddings we create and insert at once\\n\\nfor i in tqdm(range(0, len(data), batch_size)):\\n    passed = False\\n    # find end of batch\\n    i_end = min(len(data), i+batch_size)\\n    # create batch\\n    batch = data[i:i_end]\\n    embeds = embed(batch[\\\"text\\\"])\\n    to_upsert = list(zip(batch[\\\"id\\\"], embeds, batch[\\\"metadata\\\"]))\\n    # upsert to Pinecone\\n    index.upsert(vectors=to_upsert)\"}\n1a0:[]\n19f:{\"_key\":\"3c7df7023ab20\",\"_type\":\"span\",\"marks\":\"$1a0\",\"text\":\"Our index is now populated and ready for us to query!\"}\n19e:[\"$19f\"]\n1a1:[]\n19d:{\"_key\":\"563c20077d9f\",\"_type\":\"block\",\"children\":\"$19e\",\"markDefs\":\"$1a1\",\"style\":\"normal\"}\n1a5:[]\n1a4:{\"_key\":\"3252041f68c10\",\"_type\":\"span\",\"marks\":\"$1a5\",\"text\":\"Retrieval Without Reranking\"}\n1a3:[\"$1a4\"]\n1a6:[]\n1a2:{\"_key\":\"afc9e37d8730\",\"_type\":\"block\",\"children\":\"$1a3\",\"markDefs\":\"$1a6\",\"style\":\"h3\"}\n1aa:[]\n1a9:{\"_key\":\"8bb90e420ae40\",\"_type\":\"span\",\"marks\":\"$1aa\",\"text\":\"Before reranking, let's see how our results look \"}\n1ac:[\"em\"]\n1ab:{\"_key\":\"8bb90e420ae41\",\"_type\":\"span\",\"marks\":\"$1ac\",\"text\":\"without\"}\n1ae:[]\n1ad:{\"_key\":\"8bb90e420ae42\",\"_type\":\"span\",\"marks\":\"$1ae\",\"text\":\" it. We will define a function called \"}\n1b0:[\"code\"]\n1af:{\"_key\":\"8bb90e420ae43\",\"_type\":\"span\",\"marks\":\"$1b0\",\"text\":\"get_docs\"}\n1b2:[]\n1b1:{\"_key\":\"8bb90e420ae44\",\"_type\":\"span\",\"marks\":\"$1b2\",\"text\":\" to return documents using the first stage of retrieval only:\"}\n1a8:[\"$1a9\",\"$1ab\",\"$1ad\",\"$1af\",\"$1b1\"]\n1b3:[]\n1a7:{\"_key\":\"807cb6771c0b\",\"_type\":\"block\",\"children\":\"$1a8\",\"markDefs\":\"$1b3\",\"style\":\"normal\"}\n1b4:{\"_key\":\"48a06d3951e1\",\"_type\":\"code\",\"code\":\"def get_docs(query: str, top_k: int) -\u003e list[str]:\\n    # encode query\\n    res = pc.inference.embed(\\n        model=embed_model,\\n        inputs=[query],\\n        parameters={\\n    "])</script><script>self.__next_f.push([1,"        \\\"input_type\\\": \\\"query\\\",  # for queries\\n            \\\"truncate\\\": \\\"END\\\",  # truncate to max length\\n        }\\n    )\\n    xq = res.data[0][\\\"values\\\"]\\n    # search pinecone index\\n    res = index.query(vector=xq, top_k=top_k, include_metadata=True)\\n    # get doc text\\n    docs = [{\\n        \\\"id\\\": str(i),\\n        \\\"text\\\": x[\\\"metadata\\\"]['text']\\n    } for i, x in enumerate(res[\\\"matches\\\"])]\\n    return docs\"}\n1b8:[]\n1b7:{\"_key\":\"589290d112930\",\"_type\":\"span\",\"marks\":\"$1b8\",\"text\":\"Let's ask about \"}\n1ba:[\"strong\"]\n1b9:{\"_key\":\"589290d112931\",\"_type\":\"span\",\"marks\":\"$1ba\",\"text\":\"R\"}\n1bc:[]\n1bb:{\"_key\":\"589290d112932\",\"_type\":\"span\",\"marks\":\"$1bc\",\"text\":\"einforcement \"}\n1be:[\"strong\"]\n1bd:{\"_key\":\"589290d112933\",\"_type\":\"span\",\"marks\":\"$1be\",\"text\":\"L\"}\n1c0:[]\n1bf:{\"_key\":\"589290d112934\",\"_type\":\"span\",\"marks\":\"$1c0\",\"text\":\"earning with \"}\n1c2:[\"strong\"]\n1c1:{\"_key\":\"589290d112935\",\"_type\":\"span\",\"marks\":\"$1c2\",\"text\":\"H\"}\n1c4:[]\n1c3:{\"_key\":\"589290d112936\",\"_type\":\"span\",\"marks\":\"$1c4\",\"text\":\"uman \"}\n1c6:[\"strong\"]\n1c5:{\"_key\":\"589290d112937\",\"_type\":\"span\",\"marks\":\"$1c6\",\"text\":\"F\"}\n1c8:[]\n1c7:{\"_key\":\"589290d112938\",\"_type\":\"span\",\"marks\":\"$1c8\",\"text\":\"eedback  a popular fine-tuning method behind the sudden performance gains demonstrated by ChatGPT when it was released.\"}\n1b6:[\"$1b7\",\"$1b9\",\"$1bb\",\"$1bd\",\"$1bf\",\"$1c1\",\"$1c3\",\"$1c5\",\"$1c7\"]\n1c9:[]\n1b5:{\"_key\":\"157a3394ae28\",\"_type\":\"block\",\"children\":\"$1b6\",\"markDefs\":\"$1c9\",\"style\":\"normal\"}\n1cb:{\"_ref\":\"file-0f23cad0b913da3b50f37636136f18e74fad417c-ipynb\",\"_type\":\"reference\"}\n1ca:{\"_key\":\"bbe9ae8537e5\",\"_type\":\"colabFile\",\"asset\":\"$1cb\"}\n1cf:[]\n1ce:{\"_key\":\"bf7a38e53fa50\",\"_type\":\"span\",\"marks\":\"$1cf\",\"text\":\"We get reasonable performance here  notably relevant chunks of text:\"}\n1cd:[\"$1ce\"]\n1d0:[]\n1cc:{\"_key\":\"968074d1a926\",\"_type\":\"block\",\"children\":\"$1cd\",\"markDefs\":\"$1d0\",\"style\":\"normal\"}\n1d5:[\"Document\",\"Chunk\"]\n1d4:{\"_key\":\"a7a956fb-2ef9-42d8-bad2-75a0965ce593\",\"_type\":\"tableRow\",\"cells\":\"$1d5\"}\n1d7:[\"0\",\"\\\"enabling signifi"])</script><script>self.__next_f.push([1,"cant improvements in their performance\\\"\"]\n1d6:{\"_key\":\"39532801-57cc-431b-99fe-76050cb3a702\",\"_type\":\"tableRow\",\"cells\":\"$1d7\"}\n1d9:[\"0\",\"\\\"iteratively aligning the models' responses more closely with human expectations and preferences\\\"\"]\n1d8:{\"_key\":\"3a6f6c54-397a-4e47-8862-6a0ab0b4eaa0\",\"_type\":\"tableRow\",\"cells\":\"$1d9\"}\n1db:[\"0\",\"\\\"instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness\\\"\"]\n1da:{\"_key\":\"a639e5bf-9657-4682-87a2-416e81da8028\",\"_type\":\"tableRow\",\"cells\":\"$1db\"}\n1dd:[\"1\",\"\\\"increasingly popular technique for reducing harmful behaviors in large language models\\\"\"]\n1dc:{\"_key\":\"c9e11ffa-c89d-4c3e-b826-7a354d75a5ab\",\"_type\":\"tableRow\",\"cells\":\"$1dd\"}\n1d3:[\"$1d4\",\"$1d6\",\"$1d8\",\"$1da\",\"$1dc\"]\n1d2:{\"rows\":\"$1d3\"}\n1d1:{\"_key\":\"eaf26a826737\",\"_type\":\"tableField\",\"tableInput\":\"$1d2\"}\n1e1:[]\n1e0:{\"_key\":\"6ace92f658ba0\",\"_type\":\"span\",\"marks\":\"$1e1\",\"text\":\"The remaining documents and text cover RLHF but don't answer our specific question of \"}\n1e3:[\"em\"]\n1e2:{\"_key\":\"6ace92f658ba1\",\"_type\":\"span\",\"marks\":\"$1e3\",\"text\":\"\\\"why we would want to do rlhf?\\\"\"}\n1e5:[]\n1e4:{\"_key\":\"6ace92f658ba2\",\"_type\":\"span\",\"marks\":\"$1e5\",\"text\":\".\"}\n1df:[\"$1e0\",\"$1e2\",\"$1e4\"]\n1e6:[]\n1de:{\"_key\":\"26fad405566d\",\"_type\":\"block\",\"children\":\"$1df\",\"markDefs\":\"$1e6\",\"style\":\"normal\"}\n1ea:[]\n1e9:{\"_key\":\"8af9b6e51f980\",\"_type\":\"span\",\"marks\":\"$1ea\",\"text\":\"Reranking Responses\"}\n1e8:[\"$1e9\"]\n1eb:[]\n1e7:{\"_key\":\"56791e4222dc\",\"_type\":\"block\",\"children\":\"$1e8\",\"markDefs\":\"$1eb\",\"style\":\"h3\"}\n1ef:[]\n1ee:{\"_key\":\"215e2263c48d0\",\"_type\":\"span\",\"marks\":\"$1ef\",\"text\":\"We will use Pinecone's rerank endpoint for this. We use the same Pinecone client but now hit \"}\n1f1:[\"code\"]\n1f0:{\"_key\":\"706e5bbf5539\",\"_type\":\"span\",\"marks\":\"$1f1\",\"text\":\"inference.rerank\"}\n1f3:[]\n1f2:{\"_key\":\"15df71eba5d3\",\"_type\":\"span\",\"marks\":\"$1f3\",\"text\":\" like so:\"}\n1ed:[\"$1ee\",\"$1f0\",\"$1f2\"]\n1f4:[]\n1ec:{\"_key\":\"5e1a917e0d6b\",\"_type\":\"block\",\"children\":\"$1ed\",\"markDefs\":\"$1f4\",\"style\":\"normal\"}\n1f5:{\"_key\":\"db5ed2c7a2b8\",\"_typ"])</script><script>self.__next_f.push([1,"e\":\"code\",\"code\":\"rerank_name = \\\"bge-reranker-v2-m3\\\"\\n\\nrerank_docs = pc.inference.rerank(\\n    model=rerank_name,\\n    query=query,\\n    documents=docs,\\n    top_n=25,\\n    return_documents=True\\n)\"}\n1f9:[]\n1f8:{\"_key\":\"818c40fd7c3c0\",\"_type\":\"span\",\"marks\":\"$1f9\",\"text\":\"This returns a \"}\n1fb:[\"code\"]\n1fa:{\"_key\":\"818c40fd7c3c1\",\"_type\":\"span\",\"marks\":\"$1fb\",\"text\":\"RerankResult\"}\n1fd:[]\n1fc:{\"_key\":\"818c40fd7c3c2\",\"_type\":\"span\",\"marks\":\"$1fd\",\"text\":\" object:\"}\n1f7:[\"$1f8\",\"$1fa\",\"$1fc\"]\n1fe:[]\n1f6:{\"_key\":\"b7fb60d534b3\",\"_type\":\"block\",\"children\":\"$1f7\",\"markDefs\":\"$1fe\",\"style\":\"normal\"}\n1ff:{\"_key\":\"36db9967c859\",\"_type\":\"code\",\"code\":\"RerankResult(\\n  model='bge-reranker-v2-m3',\\n  data=[\\n    { index=1, score=0.9071478,\\n      document={id=\\\"1\\\", text=\\\"RLHF Response ! I...\\\"} },\\n    { index=9, score=0.6954414,\\n      document={id=\\\"9\\\", text=\\\"team, instead of ...\\\"} },\\n    ... (21 more documents) ...,\\n    { index=17, score=0.13420755,\\n      document={id=\\\"17\\\", text=\\\"helpfulness and h...\\\"} },\\n    { index=23, score=0.11417085,\\n      document={id=\\\"23\\\", text=\\\"responses respons...\\\"} }\\n  ],\\n  usage={'rerank_units': 1}\\n)\"}\n203:[]\n202:{\"_key\":\"1048cbe96ca50\",\"_type\":\"span\",\"marks\":\"$203\",\"text\":\"We access the text content of the docs via \"}\n205:[\"code\"]\n204:{\"_key\":\"b8ab5c188819\",\"_type\":\"span\",\"marks\":\"$205\",\"text\":\"rerank_docs.data[0][\\\"document\\\"][\\\"text\\\"]\"}\n207:[]\n206:{\"_key\":\"20250d3b70d1\",\"_type\":\"span\",\"marks\":\"$207\",\"text\":\".\"}\n201:[\"$202\",\"$204\",\"$206\"]\n208:[]\n200:{\"_key\":\"3f08f11a916d\",\"_type\":\"block\",\"children\":\"$201\",\"markDefs\":\"$208\",\"style\":\"normal\"}\n20c:[]\n20b:{\"_key\":\"384dabeef50a0\",\"_type\":\"span\",\"marks\":\"$20c\",\"text\":\"Let's create a function that will allow us to quickly compare original vs. reranked results.\"}\n20a:[\"$20b\"]\n20d:[]\n209:{\"_key\":\"3ed8e7b486ac\",\"_type\":\"block\",\"children\":\"$20a\",\"markDefs\":\"$20d\",\"style\":\"normal\"}\n20e:{\"_key\":\"a7639a94b804\",\"_type\":\"code\",\"code\":\"def compare(query: str, top_k: int, top_n: int):\\n    # first get vec search results\\n    top_k_docs"])</script><script>self.__next_f.push([1," = get_docs(query, top_k=top_k)\\n    # rerank\\n    top_n_docs = pc.inference.rerank(\\n        model=rerank_name,\\n        query=query,\\n        documents=docs,\\n        top_n=top_n,\\n        return_documents=True\\n    )\\n    original_docs = []\\n    reranked_docs = []\\n    # compare order change\\n    print(\\\"[ORIGINAL] -\u003e [NEW]\\\")\\n    for i, doc in enumerate(top_n_docs.data):\\n        print(str(doc.index)+\\\"\\\\t-\u003e\\\\t\\\"+str(i))\\n        if i != doc.index:\\n            reranked_docs.append(f\\\"[{doc.index}]\\\\n\\\"+doc[\\\"document\\\"][\\\"text\\\"])\\n            original_docs.append(f\\\"[{i}]\\\\n\\\"+top_k_docs[i]['text'])\\n        else:\\n            reranked_docs.append(doc[\\\"document\\\"][\\\"text\\\"])\\n            original_docs.append(None)\\n    # print results\\n    for orig, rerank in zip(original_docs, reranked_docs):\\n        if not orig:\\n            print(f\\\"SAME:\\\\n{rerank}\\\\n\\\\n---\\\\n\\\")\\n        else:\\n            print(f\\\"ORIGINAL:\\\\n{orig}\\\\n\\\\nRERANKED:\\\\n{rerank}\\\\n\\\\n---\\\\n\\\")\"}\n212:[]\n211:{\"_key\":\"2febc37f87ff0\",\"_type\":\"span\",\"marks\":\"$212\",\"text\":\"We start with our RLHF query. This time, we do a more standard retrieval-rerank process of retrieving 25 documents (\"}\n214:[\"code\"]\n213:{\"_key\":\"2febc37f87ff1\",\"_type\":\"span\",\"marks\":\"$214\",\"text\":\"top_k=25\"}\n216:[]\n215:{\"_key\":\"2febc37f87ff2\",\"_type\":\"span\",\"marks\":\"$216\",\"text\":\") and reranking to the top three documents (\"}\n218:[\"code\"]\n217:{\"_key\":\"2febc37f87ff3\",\"_type\":\"span\",\"marks\":\"$218\",\"text\":\"top_n=3\"}\n21a:[]\n219:{\"_key\":\"2febc37f87ff4\",\"_type\":\"span\",\"marks\":\"$21a\",\"text\":\").\"}\n210:[\"$211\",\"$213\",\"$215\",\"$217\",\"$219\"]\n21b:[]\n20f:{\"_key\":\"f4994bb06be3\",\"_type\":\"block\",\"children\":\"$210\",\"markDefs\":\"$21b\",\"style\":\"normal\"}\n21d:{\"_ref\":\"file-c3d071ccb9aadca3c841b14e5557b2b27ef8d1e1-ipynb\",\"_type\":\"reference\"}\n21c:{\"_key\":\"d744b901d2d7\",\"_type\":\"colabFile\",\"asset\":\"$21d\"}\n221:[]\n220:{\"_key\":\"4b38576facad0\",\"_type\":\"span\",\"marks\":\"$221\",\"text\":\"Looking at these, we have dropped the one relevant chunk of text from document \"}\n223:[\"code\"]\n222:{\"_key\":\"4b38576facad1\","])</script><script>self.__next_f.push([1,"\"_type\":\"span\",\"marks\":\"$223\",\"text\":\"1\"}\n225:[]\n224:{\"_key\":\"4b38576facad2\",\"_type\":\"span\",\"marks\":\"$225\",\"text\":\" and \"}\n227:[\"em\"]\n226:{\"_key\":\"4b38576facad3\",\"_type\":\"span\",\"marks\":\"$227\",\"text\":\"no relevant\"}\n229:[]\n228:{\"_key\":\"4b38576facad4\",\"_type\":\"span\",\"marks\":\"$229\",\"text\":\" chunks of text from document \"}\n22b:[\"code\"]\n22a:{\"_key\":\"4b38576facad5\",\"_type\":\"span\",\"marks\":\"$22b\",\"text\":\"2\"}\n22d:[]\n22c:{\"_key\":\"4b38576facad6\",\"_type\":\"span\",\"marks\":\"$22d\",\"text\":\"  the following relevant pieces of information now replace these:\"}\n21f:[\"$220\",\"$222\",\"$224\",\"$226\",\"$228\",\"$22a\",\"$22c\"]\n22e:[]\n21e:{\"_key\":\"ab235ca27612\",\"_type\":\"block\",\"children\":\"$21f\",\"markDefs\":\"$22e\",\"style\":\"normal\"}\n233:[\"Original Position\",\"Rerank Position\",\"Chunk\"]\n232:{\"_key\":\"53a8d273-e210-4ce0-9e2b-b00dc60d33bd\",\"_type\":\"tableRow\",\"cells\":\"$233\"}\n235:[\"23\",\"1\",\"\\\"train language models that act as helpful and harmless assistants\\\"\"]\n234:{\"_key\":\"3c2e4688-fe9f-41ed-a264-7584feef8b17\",\"_type\":\"tableRow\",\"cells\":\"$235\"}\n237:[\"23\",\"1\",\"\\\"RLHF training also improves honesty\\\"\"]\n236:{\"_key\":\"320e3e8e-516f-46b6-825a-6821532eedff\",\"_type\":\"tableRow\",\"cells\":\"$237\"}\n239:[\"23\",\"1\",\"\\\"RLHF improves helpfulness and harmlessness by a huge margin\\\"\"]\n238:{\"_key\":\"24376f08-8bd5-405b-a81c-3e6dfc685c84\",\"_type\":\"tableRow\",\"cells\":\"$239\"}\n23b:[\"23\",\"1\",\"\\\"enhance the capabilities of large models\\\"\"]\n23a:{\"_key\":\"07a1ebbe-fda2-4cab-ac94-971cf5a798eb\",\"_type\":\"tableRow\",\"cells\":\"$23b\"}\n23d:[\"14\",\"2\",\"\\\"the model outputs safe responses\\\"\"]\n23c:{\"_key\":\"2bdea449-bcb5-4384-96cb-f0b29e223e55\",\"_type\":\"tableRow\",\"cells\":\"$23d\"}\n23f:[\"14\",\"2\",\"\\\"often more detailed than what the average annotator writes\\\"\"]\n23e:{\"_key\":\"f6b91520-dedc-404c-b821-4701cf4d5984\",\"_type\":\"tableRow\",\"cells\":\"$23f\"}\n241:[\"14\",\"2\",\"\\\"RLHF to reach the model how to write more nuanced responses\\\"\"]\n240:{\"_key\":\"6f047bf3-6dc2-4d29-8b74-ccd50de8e53e\",\"_type\":\"tableRow\",\"cells\":\"$241\"}\n243:[\"14\",\"2\",\"\\\"make the model more robust to jailbreak attempts\\\"\"]\n242:{\"_key\":\"c435d4ac-8faf-4d1"])</script><script>self.__next_f.push([1,"e-be6d-e0617bb7c42f\",\"_type\":\"tableRow\",\"cells\":\"$243\"}\n231:[\"$232\",\"$234\",\"$236\",\"$238\",\"$23a\",\"$23c\",\"$23e\",\"$240\",\"$242\"]\n230:{\"rows\":\"$231\"}\n22f:{\"_key\":\"b422a2a2d698\",\"_type\":\"tableField\",\"tableInput\":\"$230\"}\n247:[]\n246:{\"_key\":\"34e37fb4fb7e0\",\"_type\":\"span\",\"marks\":\"$247\",\"text\":\"After reranking, we have\"}\n249:[\"em\"]\n248:{\"_key\":\"ae198355e831\",\"_type\":\"span\",\"marks\":\"$249\",\"text\":\"far more\"}\n24b:[]\n24a:{\"_key\":\"cc249079b163\",\"_type\":\"span\",\"marks\":\"$24b\",\"text\":\"relevant information. Naturally, this can result in significantly better performance for RAG. It means we maximize relevant information while minimizing noise input into our LLM.\"}\n245:[\"$246\",\"$248\",\"$24a\"]\n24c:[]\n244:{\"_key\":\"9703122318a8\",\"_type\":\"block\",\"children\":\"$245\",\"markDefs\":\"$24c\",\"style\":\"normal\"}\n24d:{\"_key\":\"2d35f4398409\",\"_type\":\"horizontalLine\",\"style\":\"Solid\"}\n251:[]\n250:{\"_key\":\"ae4f83257fb90\",\"_type\":\"span\",\"marks\":\"$251\",\"text\":\"Reranking is one of the simplest methods for dramatically improving recall performance in \"}\n253:[\"strong\"]\n252:{\"_key\":\"ae4f83257fb91\",\"_type\":\"span\",\"marks\":\"$253\",\"text\":\"R\"}\n255:[]\n254:{\"_key\":\"ae4f83257fb92\",\"_type\":\"span\",\"marks\":\"$255\",\"text\":\"etrieval \"}\n257:[\"strong\"]\n256:{\"_key\":\"ae4f83257fb93\",\"_type\":\"span\",\"marks\":\"$257\",\"text\":\"A\"}\n259:[]\n258:{\"_key\":\"ae4f83257fb94\",\"_type\":\"span\",\"marks\":\"$259\",\"text\":\"ugmented \"}\n25b:[\"strong\"]\n25a:{\"_key\":\"ae4f83257fb95\",\"_type\":\"span\",\"marks\":\"$25b\",\"text\":\"G\"}\n25d:[]\n25c:{\"_key\":\"ae4f83257fb96\",\"_type\":\"span\",\"marks\":\"$25d\",\"text\":\"eneration (RAG) or any other retrieval-based pipeline.\"}\n24f:[\"$250\",\"$252\",\"$254\",\"$256\",\"$258\",\"$25a\",\"$25c\"]\n25e:[]\n24e:{\"_key\":\"22f56f0fd0d5\",\"_type\":\"block\",\"children\":\"$24f\",\"markDefs\":\"$25e\",\"style\":\"normal\"}\n262:[]\n261:{\"_key\":\"21d37abf98de0\",\"_type\":\"span\",\"marks\":\"$262\",\"text\":\"We've explored why \"}\n264:[\"295c9ea8d44f\"]\n263:{\"_key\":\"4e7e9f9aaa1f\",\"_type\":\"span\",\"marks\":\"$264\",\"text\":\"rerankers can provide so much better performance\"}\n266:[]\n265:{\"_key\":\"7e7f9def19cf\",\"_type\":\"span\",\"marks\":\"$266\",\"text\":\" than "])</script><script>self.__next_f.push([1,"their embedding model counterparts  and how a two-stage retrieval system allows us to get the best of both, enabling search at scale while maintaining quality performance.\"}\n260:[\"$261\",\"$263\",\"$265\"]\n268:{\"_key\":\"295c9ea8d44f\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/refine-with-rerank/\"}\n267:[\"$268\"]\n25f:{\"_key\":\"4f0510a67140\",\"_type\":\"block\",\"children\":\"$260\",\"markDefs\":\"$267\",\"style\":\"normal\"}\n269:{\"_key\":\"115bfb9d1a8e\",\"_type\":\"horizontalLine\",\"style\":\"Solid\"}\n26d:[]\n26c:{\"_key\":\"f872f5f1d2280\",\"_type\":\"span\",\"marks\":\"$26d\",\"text\":\"References\"}\n26b:[\"$26c\"]\n26e:[]\n26a:{\"_key\":\"2f98a96bd2db\",\"_type\":\"block\",\"children\":\"$26b\",\"markDefs\":\"$26e\",\"style\":\"h2\"}\n272:[]\n271:{\"_key\":\"48bc26f52b7e0\",\"_type\":\"span\",\"marks\":\"$272\",\"text\":\"[1] \"}\n274:[\"a016c39699d1\"]\n273:{\"_key\":\"48bc26f52b7e1\",\"_type\":\"span\",\"marks\":\"$274\",\"text\":\"Introducing 100K Context Windows\"}\n276:[]\n275:{\"_key\":\"48bc26f52b7e2\",\"_type\":\"span\",\"marks\":\"$276\",\"text\":\" (2023), Anthropic\"}\n270:[\"$271\",\"$273\",\"$275\"]\n278:{\"_key\":\"a016c39699d1\",\"_type\":\"link\",\"href\":\"https://www.anthropic.com/index/100k-context-windows\"}\n277:[\"$278\"]\n26f:{\"_key\":\"371a091d378a\",\"_type\":\"block\",\"children\":\"$270\",\"markDefs\":\"$277\",\"style\":\"normal\"}\n27c:[]\n27b:{\"_key\":\"eb1db7457b8f0\",\"_type\":\"span\",\"marks\":\"$27c\",\"text\":\"[2] N. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, P. Liang, \"}\n27e:[\"82dfbe390d90\"]\n27d:{\"_key\":\"eb1db7457b8f1\",\"_type\":\"span\",\"marks\":\"$27e\",\"text\":\"Lost in the Middle: How Language Models Use Long Contexts\"}\n280:[]\n27f:{\"_key\":\"eb1db7457b8f2\",\"_type\":\"span\",\"marks\":\"$280\",\"text\":\" (2023),\"}\n27a:[\"$27b\",\"$27d\",\"$27f\"]\n282:{\"_key\":\"82dfbe390d90\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2307.03172\"}\n281:[\"$282\"]\n279:{\"_key\":\"b8a0bb0008f5\",\"_type\":\"block\",\"children\":\"$27a\",\"markDefs\":\"$281\",\"style\":\"normal\"}\n286:[]\n285:{\"_key\":\"be88ef614d280\",\"_type\":\"span\",\"marks\":\"$286\",\"text\":\"[3] N. Reimers, I. Gurevych, \"}\n288:[\"4c29fe15e918\"]\n287:{\"_key\":\"be88ef614d281\",\"_type\":\"span\",\"marks\":\"$288\",\"text\":\"Sentence-BERT: Sentence"])</script><script>self.__next_f.push([1," Embeddings using Siamese BERT-Networks\"}\n28a:[]\n289:{\"_key\":\"be88ef614d282\",\"_type\":\"span\",\"marks\":\"$28a\",\"text\":\" (2019), UKP-TUDA\"}\n284:[\"$285\",\"$287\",\"$289\"]\n28c:{\"_key\":\"4c29fe15e918\",\"_type\":\"link\",\"href\":\"https://arxiv.org/pdf/1908.10084.pdf\"}\n28b:[\"$28c\"]\n283:{\"_key\":\"a7051a9f4e79\",\"_type\":\"block\",\"children\":\"$284\",\"markDefs\":\"$28b\",\"style\":\"normal\"}\n31:[\"$32\",\"$3a\",\"$43\",\"$50\",\"$55\",\"$56\",\"$57\",\"$5c\",\"$65\",\"$6a\",\"$6f\",\"$78\",\"$81\",\"$92\",\"$93\",\"$9c\",\"$a5\",\"$a7\",\"$ac\",\"$b1\",\"$be\",\"$c3\",\"$d1\",\"$d3\",\"$dc\",\"$e5\",\"$ea\",\"$f4\",\"$f9\",\"$fe\",\"$107\",\"$109\",\"$10e\",\"$113\",\"$11c\",\"$121\",\"$123\",\"$12c\",\"$131\",\"$13b\",\"$13c\",\"$141\",\"$14b\",\"$14d\",\"$152\",\"$154\",\"$16a\",\"$16c\",\"$171\",\"$17a\",\"$17b\",\"$190\",\"$191\",\"$196\",\"$197\",\"$19c\",\"$19d\",\"$1a2\",\"$1a7\",\"$1b4\",\"$1b5\",\"$1ca\",\"$1cc\",\"$1d1\",\"$1de\",\"$1e7\",\"$1ec\",\"$1f5\",\"$1f6\",\"$1ff\",\"$200\",\"$209\",\"$20e\",\"$20f\",\"$21c\",\"$21e\",\"$22f\",\"$244\",\"$24d\",\"$24e\",\"$25f\",\"$269\",\"$26a\",\"$26f\",\"$279\",\"$283\"]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"article\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"border-b\",\"children\":[\"$\",\"div\",null,{\"id\":\"$undefined\",\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative px-4 sm:px-8 flex flex-col gap-8 pb-4 pt-16 md:gap-16 md:pb-8 md:pt-24 xl:flex-row\",\"children\":[[[\"$\",\"div\",null,{\"className\":\"w-full xl:w-[13.5rem]\",\"children\":\"$undefined\"}],[\"$\",\"div\",null,{\"className\":\"flex-1 space-y-8\",\"children\":[[\"$\",\"div\",null,{\"children\":[false,[\"$\",\"h1\",null,{\"className\":\"xxs:text-[38px] sm:text-h1/[1.1] text-text-primary text-[30px]\",\"children\":\"Rerankers and Two-Stage Retrieval\"}]]}],\"$undefined\",\"$undefined\",false]}],[\"$\",\"div\",null,{\"className\":\"flex w-full flex-wrap justify-between gap-4 xl:w-[13.5rem] xl:flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[\"$undefined\",false,\"$undefined\",\"$undefined\"]}],\"$undefined\"]}]],[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute inset-0\",\"children\":[[\"$\",\"span\",null,{\"className\":\"from-background absolute bottom-0 left-0 top-0 h-full w-[1px] border-l\"}],[\"$\",\"span\",null,{\"className\":\"from-background absolute bottom-0 right-0 top-0 h-full w-[1px] border-r\"}]]}],\"$undefined\",\"$undefined\"]}]}]}],[\"$\",\"div\",null,{\"id\":\"$undefined\",\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative px-4 sm:px-8 !px-0 !mx-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"container pb-75 pt-25 lg:pb-150 lg:pt-25\",\"children\":[[\"$\",\"div\",null,{\"className\":\"block lg:hidden\",\"children\":[[\"$\",\"$L2e\",null,{\"title\":\"Jump to section\",\"articleContent\":[{\"_key\":\"b5c3d8ff292a\",\"_type\":\"block\",\"children\":[{\"_key\":\"3824add52b350\",\"_type\":\"span\",\"marks\":[\"edb14340181b\"],\"text\":\"Retrieval Augmented Generation (RAG)\"},{\"_key\":\"331f00c8ea33\",\"_type\":\"span\",\"marks\":[],\"text\":\" is an overloaded term. It promises the world, but after developing a RAG pipeline, there are many of us left wondering why it doesn't work as well as we had expected.\"}],\"markDefs\":[{\"_key\":\"edb14340181b\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/retrieval-augmented-generation/\"}],\"style\":\"normal\"},{\"_key\":\"5cf640b84731\",\"_type\":\"block\",\"children\":[{\"_key\":\"e1c9a65b7ba30\",\"_type\":\"span\",\"marks\":[],\"text\":\"As with most tools, RAG is easy to use but hard to master. The truth is that there is more to RAG than putting documents into a vector DB and adding an LLM on top. That \"},{\"_key\":\"e1c9a65b7ba31\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"can work\"},{\"_key\":\"e1c9a65b7ba32\",\"_type\":\"span\",\"marks\":[],\"text\":\", but it won't always.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"30e134d596b0\",\"_type\":\"block\",\"children\":[{\"_key\":\"f1f64c6528800\",\"_type\":\"span\",\"marks\":[],\"text\":\"This ebook aims to tell you what to do when out-of-the-box RAG \"},{\"_key\":\"f1f64c6528801\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"doesn't\"},{\"_key\":\"f1f64c6528802\",\"_type\":\"span\",\"marks\":[],\"text\":\" work. In this first chapter, we'll look at what is often the easiest and fastest to implement solution for suboptimal RAG pipelines  we'll be learning about \"},{\"_key\":\"f1f64c6528803\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"rerankers\"},{\"_key\":\"f1f64c6528804\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"067464a393a2\",\"_type\":\"block\",\"children\":[{\"_key\":\"82f45bf4e9de\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"15466780da58\",\"_type\":\"video\",\"caption\":\"Video companion for this chapter.\",\"videoURL\":\"https://www.youtube.com/watch?v=Uh9bYiVrW_s\"},{\"_key\":\"ecf3fb10fbdc\",\"_type\":\"horizontalLine\",\"style\":\"Solid\"},{\"_key\":\"e5d649e8cc7e\",\"_type\":\"block\",\"children\":[{\"_key\":\"36c051a681020\",\"_type\":\"span\",\"marks\":[],\"text\":\"Recall vs. Context Windows\"}],\"markDefs\":[],\"style\":\"h2\"},{\"_key\":\"6fdf675f90e3\",\"_type\":\"block\",\"children\":[{\"_key\":\"862f76e935f40\",\"_type\":\"span\",\"marks\":[],\"text\":\"Before jumping into the solution, let's talk about the problem. With RAG, we are performing a \"},{\"_key\":\"862f76e935f41\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"semantic search\"},{\"_key\":\"862f76e935f42\",\"_type\":\"span\",\"marks\":[],\"text\":\" across many text documents  these could be tens of thousands up to tens of billions of documents.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4ae1f1b40a07\",\"_type\":\"block\",\"children\":[{\"_key\":\"001d8b81e5a50\",\"_type\":\"span\",\"marks\":[],\"text\":\"To ensure fast search times at scale, we typically use vector search  that is, we transform our text into vectors, place them all into a vector space, and compare their proximity to a query vector using a similarity metric like cosine similarity.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"963eddee4ce0\",\"_type\":\"block\",\"children\":[{\"_key\":\"7cb27ba8d0370\",\"_type\":\"span\",\"marks\":[],\"text\":\"For vector search to work, we need vectors. These vectors are essentially compressions of the \\\"meaning\\\" behind some text into (typically) 768 or 1536-dimensional vectors. There is some information loss because we're compressing this information into a single vector.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0c90e515cdb1\",\"_type\":\"block\",\"children\":[{\"_key\":\"67e2f9f2cd630\",\"_type\":\"span\",\"marks\":[],\"text\":\"Because of this information loss, we often see that the top three (for example) vector search documents will miss relevant information. Unfortunately, the retrieval may return relevant information below our \"},{\"_key\":\"67e2f9f2cd631\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"top_k\"},{\"_key\":\"67e2f9f2cd632\",\"_type\":\"span\",\"marks\":[],\"text\":\" cutoff.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b044a09e107d\",\"_type\":\"block\",\"children\":[{\"_key\":\"dda24ee4145b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"What do we do if relevant information at a lower position would help our LLM formulate a better response? The easiest approach is to increase the number of documents we're returning (increase \"},{\"_key\":\"dda24ee4145b1\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"top_k\"},{\"_key\":\"dda24ee4145b2\",\"_type\":\"span\",\"marks\":[],\"text\":\") and pass them all to the LLM.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ff0b1cfb75c3\",\"_type\":\"block\",\"children\":[{\"_key\":\"13a4530a39540\",\"_type\":\"span\",\"marks\":[],\"text\":\"The metric we would measure here is \"},{\"_key\":\"13a4530a39541\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"recall\"},{\"_key\":\"13a4530a39542\",\"_type\":\"span\",\"marks\":[],\"text\":\"  meaning \\\"how many of the relevant documents are we retrieving\\\". Recall does not consider the total number of retrieved documents  so we can hack the metric and get \"},{\"_key\":\"13a4530a39543\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"perfect\"},{\"_key\":\"13a4530a39544\",\"_type\":\"span\",\"marks\":[],\"text\":\" recall by returning \"},{\"_key\":\"13a4530a39545\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"everything\"},{\"_key\":\"13a4530a39546\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"68936a1772c5\",\"_type\":\"latex\",\"body\":\"recall@K = \\\\frac{\\\\#\\\\;of\\\\;relevant\\\\;docs\\\\;returned}{\\\\#\\\\;of\\\\;relevant\\\\;docs\\\\;in\\\\;dataset}\"},{\"_key\":\"ce406c57c353\",\"_type\":\"block\",\"children\":[{\"_key\":\"09d45f86a8ca0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Unfortunately, we cannot return everything. LLMs have limits on how much text we can pass to them  we call this limit the \"},{\"_key\":\"09d45f86a8ca1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"context window\"},{\"_key\":\"09d45f86a8ca2\",\"_type\":\"span\",\"marks\":[],\"text\":\". Some LLMs have huge context windows, like Anthropic's Claude, with a context window of 100K tokens [1]. With that, we could fit many tens of pages of text  so could we return many documents (not quite all) and \\\"stuff\\\" the context window to improve recall?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"95822269433b\",\"_type\":\"block\",\"children\":[{\"_key\":\"5d54cdfe38040\",\"_type\":\"span\",\"marks\":[],\"text\":\"Again, no. We cannot use context stuffing because this reduces the LLM's \"},{\"_key\":\"5d54cdfe38041\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"recall\"},{\"_key\":\"5d54cdfe38042\",\"_type\":\"span\",\"marks\":[],\"text\":\" performance  note that this is the LLM recall, which is different from the retrieval recall we have been discussing so far.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ec94139f3550\",\"_type\":\"image\",\"alt\":\"When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place\",\"asset\":{\"_ref\":\"image-ca206b6ada9163bffad313e0e18feee0b460c768-1212x688-png\",\"_type\":\"reference\"},\"caption\":\"When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place [2].\"},{\"_key\":\"83365ba70768\",\"_type\":\"block\",\"children\":[{\"_key\":\"e33ef08068190\",\"_type\":\"span\",\"marks\":[],\"text\":\"LLM recall refers to the ability of an LLM to find information from the text placed within its context window. Research shows that LLM recall degrades as we put more tokens in the context window [2]. LLMs are also less likely to follow instructions as we stuff the context window  so context stuffing is a bad idea.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8666bc6e5baf\",\"_type\":\"block\",\"children\":[{\"_key\":\"07d5b0ba245f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We can increase the number of documents returned by our vector DB to increase retrieval recall, but we cannot pass these to our LLM without damaging LLM recall.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e5aab68739fe\",\"_type\":\"block\",\"children\":[{\"_key\":\"cdb8caa82d0b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"The solution to this issue is to maximize retrieval recall by retrieving plenty of documents and then maximize LLM recall by \"},{\"_key\":\"cdb8caa82d0b1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"minimizing\"},{\"_key\":\"cdb8caa82d0b2\",\"_type\":\"span\",\"marks\":[],\"text\":\" the number of documents that make it to the LLM. To do that, we reorder retrieved documents and keep just the most relevant for our LLM  to do that, we use \"},{\"_key\":\"cdb8caa82d0b3\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"reranking\"},{\"_key\":\"cdb8caa82d0b4\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9538d0e3ba5d\",\"_type\":\"block\",\"children\":[{\"_key\":\"efde0a3161790\",\"_type\":\"span\",\"marks\":[],\"text\":\"Power of Rerankers\"}],\"markDefs\":[],\"style\":\"h2\"},{\"_key\":\"40888980cf60\",\"_type\":\"block\",\"children\":[{\"_key\":\"2718ae36ff140\",\"_type\":\"span\",\"marks\":[],\"text\":\"A \"},{\"_key\":\"6cd15bfec99e\",\"_type\":\"span\",\"marks\":[\"48c3ce857808\"],\"text\":\"reranking model\"},{\"_key\":\"e3ff4bcfaad0\",\"_type\":\"span\",\"marks\":[],\"text\":\"  also known as a \"},{\"_key\":\"2718ae36ff141\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"cross-encoder\"},{\"_key\":\"2718ae36ff142\",\"_type\":\"span\",\"marks\":[],\"text\":\"  is a type of model that, given a query and document pair, will output a similarity score. We use this score to reorder the documents by relevance to our query.\"}],\"markDefs\":[{\"_key\":\"48c3ce857808\",\"_type\":\"link\",\"href\":\"https://docs.pinecone.io/models/bge-reranker-v2-m3\"}],\"style\":\"normal\"},{\"_key\":\"f25be28644c9\",\"_type\":\"image\",\"alt\":\"A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.\",\"asset\":{\"_ref\":\"image-906c3c0f8fe637840f134dbf966839ef89ac7242-3443x1641-png\",\"_type\":\"reference\"},\"caption\":\"A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.\"},{\"_key\":\"8e082a19958e\",\"_type\":\"block\",\"children\":[{\"_key\":\"cd2123e8178b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Search engineers have used rerankers in two-stage retrieval systems for \"},{\"_key\":\"cd2123e8178b1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"a long time\"},{\"_key\":\"cd2123e8178b2\",\"_type\":\"span\",\"marks\":[],\"text\":\". In these two-stage systems, a first-stage model (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. Then, a second-stage model (the reranker) is used to rerank those documents retrieved by the first-stage model.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bb69cbece890\",\"_type\":\"block\",\"children\":[{\"_key\":\"f33b4e7c013e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We use two stages because retrieving a small set of documents from a large dataset is much faster than reranking a large set of documents  we'll discuss why this is the case soon  but TL;DR, rerankers are slow, and retrievers are \"},{\"_key\":\"f33b4e7c013e1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"fast\"},{\"_key\":\"f33b4e7c013e2\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e112003628b4\",\"_type\":\"block\",\"children\":[{\"_key\":\"129f5c517f870\",\"_type\":\"span\",\"marks\":[],\"text\":\"Why Rerankers?\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"933484ae975d\",\"_type\":\"block\",\"children\":[{\"_key\":\"a28c8222d5fc0\",\"_type\":\"span\",\"marks\":[],\"text\":\"If a reranker is so much slower, why bother using them? The answer is that rerankers are much more accurate than \"},{\"_key\":\"7d5b0306bfbf\",\"_type\":\"span\",\"marks\":[\"3eb2eed86fee\"],\"text\":\"embedding models\"},{\"_key\":\"17911c1992cb\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[{\"_key\":\"3eb2eed86fee\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/series/rag/embedding-models-rundown/\"}],\"style\":\"normal\"},{\"_key\":\"cd9ef31d35b9\",\"_type\":\"block\",\"children\":[{\"_key\":\"bcfc6d01f0c00\",\"_type\":\"span\",\"marks\":[],\"text\":\"The intuition behind a bi-encoder's inferior accuracy is that bi-encoders must compress all of the possible meanings of a document into a single vector  meaning we lose information. Additionally, bi-encoders have no context on the query because we don't know the query until we receive it (we create embeddings before user query time).\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7659ceb3516a\",\"_type\":\"block\",\"children\":[{\"_key\":\"42a387a53bda0\",\"_type\":\"span\",\"marks\":[],\"text\":\"On the other hand, a reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Because we are running the reranker at user query time, we have the added benefit of analyzing our document's meaning specific to the user query  rather than trying to produce a generic, averaged meaning.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"123dfd4a4669\",\"_type\":\"block\",\"children\":[{\"_key\":\"718eaa9a9d920\",\"_type\":\"span\",\"marks\":[],\"text\":\"Rerankers avoid the information loss of bi-encoders  but they come with a different penalty  \"},{\"_key\":\"718eaa9a9d921\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"time\"},{\"_key\":\"718eaa9a9d922\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a7138853e8fc\",\"_type\":\"image\",\"alt\":\"A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.\",\"asset\":{\"_ref\":\"image-4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420-png\",\"_type\":\"reference\"},\"caption\":\"A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.\"},{\"_key\":\"bb6e0fb9f6c7\",\"_type\":\"block\",\"children\":[{\"_key\":\"1053f9dd85eb0\",\"_type\":\"span\",\"marks\":[],\"text\":\"When using bi-encoder models with vector search, we frontload all of the heavy transformer computation to when we are creating the initial vectors  that means that when a user queries our system, we have already created the vectors, so all we need to do is:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a113b87e52e5\",\"_type\":\"block\",\"children\":[{\"_key\":\"b06f72eaec5b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Run a single transformer computation to create the query vector.\"}],\"level\":1,\"listItem\":\"number\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a73fcb2a0faa\",\"_type\":\"block\",\"children\":[{\"_key\":\"cf79269df47b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Compare the query vector to document vectors with \"},{\"_key\":\"cf79269df47b1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"cosine similarity\"},{\"_key\":\"cf79269df47b2\",\"_type\":\"span\",\"marks\":[],\"text\":\" (or another lightweight metric).\"}],\"level\":1,\"listItem\":\"number\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4a6a143cbbfa\",\"_type\":\"block\",\"children\":[{\"_key\":\"aa4a4d92a8110\",\"_type\":\"span\",\"marks\":[],\"text\":\"With rerankers, we are not pre-computing anything. Instead, we're feeding our query and a single other document into the transformer, running a whole transformer inference step, and outputting a single similarity score.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a7cc58dc92aa\",\"_type\":\"image\",\"alt\":\"A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.\",\"asset\":{\"_ref\":\"image-9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100-png\",\"_type\":\"reference\"},\"caption\":\"A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.\"},{\"_key\":\"7eb32a9d8196\",\"_type\":\"block\",\"children\":[{\"_key\":\"56cd29eda72e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Given 40M records, if we use a \"},{\"_key\":\"56cd29eda72e1\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"small\"},{\"_key\":\"56cd29eda72e2\",\"_type\":\"span\",\"marks\":[],\"text\":\" reranking model like BERT on a V100 GPU  we'd be waiting more than 50 hours to return a single query result [3]. We can do the same in \u003c100ms with encoder models and vector search.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"859de630f213\",\"_type\":\"block\",\"children\":[{\"_key\":\"7ced0fd75c590\",\"_type\":\"span\",\"marks\":[],\"text\":\"Implementing Two-Stage Retrieval with Reranking\"}],\"markDefs\":[],\"style\":\"h2\"},{\"_key\":\"2d919503698b\",\"_type\":\"block\",\"children\":[{\"_key\":\"2f030b1659650\",\"_type\":\"span\",\"marks\":[],\"text\":\"Now that we understand the idea and reason behind two-stage retrieval with rerankers, let's see how to implement it (you can follow along with \"},{\"_key\":\"2f030b1659651\",\"_type\":\"span\",\"marks\":[\"fee4b4819b21\"],\"text\":\"this notebook\"},{\"_key\":\"2f030b1659652\",\"_type\":\"span\",\"marks\":[],\"text\":\". To begin we will set up our prerequisite libraries:\"}],\"markDefs\":[{\"_key\":\"fee4b4819b21\",\"_type\":\"link\",\"href\":\"https://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb\"}],\"style\":\"normal\"},{\"_key\":\"2d2be6ace2bc\",\"_type\":\"code\",\"code\":\"!pip install -qU \\\\\\n    datasets==2.14.5 \\\\\\n    \\\"pinecone[grpc]\\\"==5.1.0\"},{\"_key\":\"f949883a420e\",\"_type\":\"block\",\"children\":[{\"_key\":\"33d3a0a9329c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Data Preparation\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"ff77c5ba4a8a\",\"_type\":\"block\",\"children\":[{\"_key\":\"1da42e3de5390\",\"_type\":\"span\",\"marks\":[],\"text\":\"Before setting up the retrieval pipeline, we need data to retrieve! We will use the \"},{\"_key\":\"1da42e3de5391\",\"_type\":\"span\",\"marks\":[\"aa3fdb860f66\",\"code\"],\"text\":\"jamescalam/ai-arxiv-chunked\"},{\"_key\":\"1da42e3de5392\",\"_type\":\"span\",\"marks\":[],\"text\":\" dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs  including the Llama 2, GPTQ, and GPT-4 papers.\"}],\"markDefs\":[{\"_key\":\"aa3fdb860f66\",\"_type\":\"link\",\"href\":\"https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked\"}],\"style\":\"normal\"},{\"_key\":\"fc2fd187d922\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-d858315997239ee9631257691107bed218131ef4-ipynb\",\"_type\":\"reference\"}},{\"_key\":\"bac9a0c8fc8b\",\"_type\":\"block\",\"children\":[{\"_key\":\"dec9dfbd9a820\",\"_type\":\"span\",\"marks\":[],\"text\":\"The dataset contains 41.5K pre-chunked records. Each record is 1-2 paragraphs long and includes additional metadata about the paper from which it comes. Here is an example:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"581ef5269e22\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-d4e771a8d321b15576b6b3f72644cdf141c48db2-ipynb\",\"_type\":\"reference\"}},{\"_key\":\"998d5f945055\",\"_type\":\"block\",\"children\":[{\"_key\":\"794e53bcba950\",\"_type\":\"span\",\"marks\":[],\"text\":\"We'll be feeding this data into Pinecone, so let's reformat the dataset to be more Pinecone-friendly when it does come to the later embed and index process. The format will contain \"},{\"_key\":\"794e53bcba951\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"id\"},{\"_key\":\"794e53bcba952\",\"_type\":\"span\",\"marks\":[],\"text\":\", \"},{\"_key\":\"794e53bcba953\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"text\"},{\"_key\":\"794e53bcba954\",\"_type\":\"span\",\"marks\":[],\"text\":\" (which we will embed), and \"},{\"_key\":\"794e53bcba955\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"metadata\"},{\"_key\":\"794e53bcba956\",\"_type\":\"span\",\"marks\":[],\"text\":\". For this example, we won't use metadata, but it can be helpful to include if we want to do \"},{\"_key\":\"794e53bcba957\",\"_type\":\"span\",\"marks\":[\"f4f8615ff21a\"],\"text\":\"metadata filtering\"},{\"_key\":\"794e53bcba958\",\"_type\":\"span\",\"marks\":[],\"text\":\" in the future.\"}],\"markDefs\":[{\"_key\":\"f4f8615ff21a\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/vector-search-filtering/\"}],\"style\":\"normal\"},{\"_key\":\"b2d7f4c0f965\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-65bb438d978ee37cc404d7cc5f646de0443053e6-ipynb\",\"_type\":\"reference\"}},{\"_key\":\"43fcccff3909\",\"_type\":\"block\",\"children\":[{\"_key\":\"4f813c0a20e20\",\"_type\":\"span\",\"marks\":[],\"text\":\"Embed and Index\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"7b2bee46847d\",\"_type\":\"block\",\"children\":[{\"_key\":\"ceaee4de0b920\",\"_type\":\"span\",\"marks\":[],\"text\":\"To store everything in the vector DB, we need to encode everything with an embedding / bi-encoder model. We will use the open source \"},{\"_key\":\"ceaee4de0b921\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"multilingial-e5-large\"},{\"_key\":\"ceaee4de0b922\",\"_type\":\"span\",\"marks\":[],\"text\":\" via Pinecone Inference. We need a [free Pinecone API key](https://app.pinecone.io) to authenticate ourselves via the client:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"5d09c9702ff6\",\"_type\":\"code\",\"code\":\"from pinecone.grpc import PineconeGRPC\\n\\n# get API key from app.pinecone.io\\napi_key = \\\"PINECONE_API_KEY\\\"\\n\\nembed_model = \\\"multilingual-e5-large\\\"\\n\\n# configure client\\npc = PineconeGRPC(api_key=api_key)\"},{\"_key\":\"029d4033c1f2\",\"_type\":\"block\",\"children\":[{\"_key\":\"f948d5514ded0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Now, we create our vector DB to store our vectors. We set \"},{\"_key\":\"cc2a19866b781\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"dimension\"},{\"_key\":\"cc2a19866b782\",\"_type\":\"span\",\"marks\":[],\"text\":\" equal to the dimensionality of E5 large (\"},{\"_key\":\"cc2a19866b783\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"1024\"},{\"_key\":\"cc2a19866b784\",\"_type\":\"span\",\"marks\":[],\"text\":\") and use a \"},{\"_key\":\"cc2a19866b785\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"metric\"},{\"_key\":\"cc2a19866b786\",\"_type\":\"span\",\"marks\":[],\"text\":\" compatible with E5  ie \"},{\"_key\":\"cc2a19866b787\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"cosine\"},{\"_key\":\"cc2a19866b788\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ed905184f11e\",\"_type\":\"code\",\"code\":\"import time\\n\\nindex_name = \\\"rerankers\\\"\\nexisting_indexes = [\\n    index_info[\\\"name\\\"] for index_info in pc.list_indexes()\\n]\\n\\n# check if index already exists (it shouldn't if this is first time)\\nif index_name not in existing_indexes:\\n    # if does not exist, create index\\n    pc.create_index(\\n        index_name,\\n        dimension=1024,  # dimensionality of e5-large\\n        metric='cosine',\\n        spec=spec\\n    )\\n    # wait for index to be initialized\\n    while not pc.describe_index(index_name).status['ready']:\\n        time.sleep(1)\\n\\n# connect to index\\nindex = pc.Index(index_name)\\ntime.sleep(1)\\n# view index stats\\nindex.describe_index_stats()\"},{\"_key\":\"fd48b611d9a1\",\"_type\":\"block\",\"children\":[{\"_key\":\"283923e9dc23\",\"_type\":\"span\",\"marks\":[],\"text\":\"We create a new function, `embed`, to handle embedding with our model. Within the function, we also include the handling of rate limit errors.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"684b7df32dcb\",\"_type\":\"code\",\"code\":\"from pinecone_plugins.inference.core.client.exceptions import PineconeApiException\\n\\ndef embed(batch: list[str]) -\u003e list[float]:\\n    # create embeddings (exponential backoff to avoid RateLimitError)\\n    for j in range(5):  # max 5 retries\\n        try:\\n            res = pc.inference.embed(\\n                model=embed_model,\\n                inputs=batch,\\n                parameters={\\n                    \\\"input_type\\\": \\\"passage\\\",  # for docs/context/chunks\\n                    \\\"truncate\\\": \\\"END\\\",  # truncate to max length\\n                }\\n            )\\n            passed = True\\n        except PineconeApiException:\\n            time.sleep(2**j)  # wait 2^j seconds before retrying\\n            print(\\\"Retrying...\\\")\\n    if not passed:\\n        raise RuntimeError(\\\"Failed to create embeddings.\\\")\\n    # get embeddings\\n    embeds = [x[\\\"values\\\"] for x in res.data]\\n    return embeds\"},{\"_key\":\"c2af4093a270\",\"_type\":\"block\",\"children\":[{\"_key\":\"4d4448a2ef4e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We're now ready to begin populating the index using the E5 embedding model like so:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6f1fbba90a52\",\"_type\":\"code\",\"code\":\"from tqdm.auto import tqdm\\n\\nbatch_size = 96  # how many embeddings we create and insert at once\\n\\nfor i in tqdm(range(0, len(data), batch_size)):\\n    passed = False\\n    # find end of batch\\n    i_end = min(len(data), i+batch_size)\\n    # create batch\\n    batch = data[i:i_end]\\n    embeds = embed(batch[\\\"text\\\"])\\n    to_upsert = list(zip(batch[\\\"id\\\"], embeds, batch[\\\"metadata\\\"]))\\n    # upsert to Pinecone\\n    index.upsert(vectors=to_upsert)\"},{\"_key\":\"563c20077d9f\",\"_type\":\"block\",\"children\":[{\"_key\":\"3c7df7023ab20\",\"_type\":\"span\",\"marks\":[],\"text\":\"Our index is now populated and ready for us to query!\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"afc9e37d8730\",\"_type\":\"block\",\"children\":[{\"_key\":\"3252041f68c10\",\"_type\":\"span\",\"marks\":[],\"text\":\"Retrieval Without Reranking\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"807cb6771c0b\",\"_type\":\"block\",\"children\":[{\"_key\":\"8bb90e420ae40\",\"_type\":\"span\",\"marks\":[],\"text\":\"Before reranking, let's see how our results look \"},{\"_key\":\"8bb90e420ae41\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"without\"},{\"_key\":\"8bb90e420ae42\",\"_type\":\"span\",\"marks\":[],\"text\":\" it. We will define a function called \"},{\"_key\":\"8bb90e420ae43\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"get_docs\"},{\"_key\":\"8bb90e420ae44\",\"_type\":\"span\",\"marks\":[],\"text\":\" to return documents using the first stage of retrieval only:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"48a06d3951e1\",\"_type\":\"code\",\"code\":\"def get_docs(query: str, top_k: int) -\u003e list[str]:\\n    # encode query\\n    res = pc.inference.embed(\\n        model=embed_model,\\n        inputs=[query],\\n        parameters={\\n            \\\"input_type\\\": \\\"query\\\",  # for queries\\n            \\\"truncate\\\": \\\"END\\\",  # truncate to max length\\n        }\\n    )\\n    xq = res.data[0][\\\"values\\\"]\\n    # search pinecone index\\n    res = index.query(vector=xq, top_k=top_k, include_metadata=True)\\n    # get doc text\\n    docs = [{\\n        \\\"id\\\": str(i),\\n        \\\"text\\\": x[\\\"metadata\\\"]['text']\\n    } for i, x in enumerate(res[\\\"matches\\\"])]\\n    return docs\"},{\"_key\":\"157a3394ae28\",\"_type\":\"block\",\"children\":[{\"_key\":\"589290d112930\",\"_type\":\"span\",\"marks\":[],\"text\":\"Let's ask about \"},{\"_key\":\"589290d112931\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"R\"},{\"_key\":\"589290d112932\",\"_type\":\"span\",\"marks\":[],\"text\":\"einforcement \"},{\"_key\":\"589290d112933\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"L\"},{\"_key\":\"589290d112934\",\"_type\":\"span\",\"marks\":[],\"text\":\"earning with \"},{\"_key\":\"589290d112935\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"H\"},{\"_key\":\"589290d112936\",\"_type\":\"span\",\"marks\":[],\"text\":\"uman \"},{\"_key\":\"589290d112937\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"F\"},{\"_key\":\"589290d112938\",\"_type\":\"span\",\"marks\":[],\"text\":\"eedback  a popular fine-tuning method behind the sudden performance gains demonstrated by ChatGPT when it was released.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bbe9ae8537e5\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-0f23cad0b913da3b50f37636136f18e74fad417c-ipynb\",\"_type\":\"reference\"}},{\"_key\":\"968074d1a926\",\"_type\":\"block\",\"children\":[{\"_key\":\"bf7a38e53fa50\",\"_type\":\"span\",\"marks\":[],\"text\":\"We get reasonable performance here  notably relevant chunks of text:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"eaf26a826737\",\"_type\":\"tableField\",\"tableInput\":{\"rows\":[{\"_key\":\"a7a956fb-2ef9-42d8-bad2-75a0965ce593\",\"_type\":\"tableRow\",\"cells\":[\"Document\",\"Chunk\"]},{\"_key\":\"39532801-57cc-431b-99fe-76050cb3a702\",\"_type\":\"tableRow\",\"cells\":[\"0\",\"\\\"enabling significant improvements in their performance\\\"\"]},{\"_key\":\"3a6f6c54-397a-4e47-8862-6a0ab0b4eaa0\",\"_type\":\"tableRow\",\"cells\":[\"0\",\"\\\"iteratively aligning the models' responses more closely with human expectations and preferences\\\"\"]},{\"_key\":\"a639e5bf-9657-4682-87a2-416e81da8028\",\"_type\":\"tableRow\",\"cells\":[\"0\",\"\\\"instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness\\\"\"]},{\"_key\":\"c9e11ffa-c89d-4c3e-b826-7a354d75a5ab\",\"_type\":\"tableRow\",\"cells\":[\"1\",\"\\\"increasingly popular technique for reducing harmful behaviors in large language models\\\"\"]}]}},{\"_key\":\"26fad405566d\",\"_type\":\"block\",\"children\":[{\"_key\":\"6ace92f658ba0\",\"_type\":\"span\",\"marks\":[],\"text\":\"The remaining documents and text cover RLHF but don't answer our specific question of \"},{\"_key\":\"6ace92f658ba1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"\\\"why we would want to do rlhf?\\\"\"},{\"_key\":\"6ace92f658ba2\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"56791e4222dc\",\"_type\":\"block\",\"children\":[{\"_key\":\"8af9b6e51f980\",\"_type\":\"span\",\"marks\":[],\"text\":\"Reranking Responses\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"5e1a917e0d6b\",\"_type\":\"block\",\"children\":[{\"_key\":\"215e2263c48d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We will use Pinecone's rerank endpoint for this. We use the same Pinecone client but now hit \"},{\"_key\":\"706e5bbf5539\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"inference.rerank\"},{\"_key\":\"15df71eba5d3\",\"_type\":\"span\",\"marks\":[],\"text\":\" like so:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"db5ed2c7a2b8\",\"_type\":\"code\",\"code\":\"rerank_name = \\\"bge-reranker-v2-m3\\\"\\n\\nrerank_docs = pc.inference.rerank(\\n    model=rerank_name,\\n    query=query,\\n    documents=docs,\\n    top_n=25,\\n    return_documents=True\\n)\"},{\"_key\":\"b7fb60d534b3\",\"_type\":\"block\",\"children\":[{\"_key\":\"818c40fd7c3c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"This returns a \"},{\"_key\":\"818c40fd7c3c1\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"RerankResult\"},{\"_key\":\"818c40fd7c3c2\",\"_type\":\"span\",\"marks\":[],\"text\":\" object:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"36db9967c859\",\"_type\":\"code\",\"code\":\"RerankResult(\\n  model='bge-reranker-v2-m3',\\n  data=[\\n    { index=1, score=0.9071478,\\n      document={id=\\\"1\\\", text=\\\"RLHF Response ! I...\\\"} },\\n    { index=9, score=0.6954414,\\n      document={id=\\\"9\\\", text=\\\"team, instead of ...\\\"} },\\n    ... (21 more documents) ...,\\n    { index=17, score=0.13420755,\\n      document={id=\\\"17\\\", text=\\\"helpfulness and h...\\\"} },\\n    { index=23, score=0.11417085,\\n      document={id=\\\"23\\\", text=\\\"responses respons...\\\"} }\\n  ],\\n  usage={'rerank_units': 1}\\n)\"},{\"_key\":\"3f08f11a916d\",\"_type\":\"block\",\"children\":[{\"_key\":\"1048cbe96ca50\",\"_type\":\"span\",\"marks\":[],\"text\":\"We access the text content of the docs via \"},{\"_key\":\"b8ab5c188819\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"rerank_docs.data[0][\\\"document\\\"][\\\"text\\\"]\"},{\"_key\":\"20250d3b70d1\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"3ed8e7b486ac\",\"_type\":\"block\",\"children\":[{\"_key\":\"384dabeef50a0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Let's create a function that will allow us to quickly compare original vs. reranked results.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a7639a94b804\",\"_type\":\"code\",\"code\":\"def compare(query: str, top_k: int, top_n: int):\\n    # first get vec search results\\n    top_k_docs = get_docs(query, top_k=top_k)\\n    # rerank\\n    top_n_docs = pc.inference.rerank(\\n        model=rerank_name,\\n        query=query,\\n        documents=docs,\\n        top_n=top_n,\\n        return_documents=True\\n    )\\n    original_docs = []\\n    reranked_docs = []\\n    # compare order change\\n    print(\\\"[ORIGINAL] -\u003e [NEW]\\\")\\n    for i, doc in enumerate(top_n_docs.data):\\n        print(str(doc.index)+\\\"\\\\t-\u003e\\\\t\\\"+str(i))\\n        if i != doc.index:\\n            reranked_docs.append(f\\\"[{doc.index}]\\\\n\\\"+doc[\\\"document\\\"][\\\"text\\\"])\\n            original_docs.append(f\\\"[{i}]\\\\n\\\"+top_k_docs[i]['text'])\\n        else:\\n            reranked_docs.append(doc[\\\"document\\\"][\\\"text\\\"])\\n            original_docs.append(None)\\n    # print results\\n    for orig, rerank in zip(original_docs, reranked_docs):\\n        if not orig:\\n            print(f\\\"SAME:\\\\n{rerank}\\\\n\\\\n---\\\\n\\\")\\n        else:\\n            print(f\\\"ORIGINAL:\\\\n{orig}\\\\n\\\\nRERANKED:\\\\n{rerank}\\\\n\\\\n---\\\\n\\\")\"},{\"_key\":\"f4994bb06be3\",\"_type\":\"block\",\"children\":[{\"_key\":\"2febc37f87ff0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We start with our RLHF query. This time, we do a more standard retrieval-rerank process of retrieving 25 documents (\"},{\"_key\":\"2febc37f87ff1\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"top_k=25\"},{\"_key\":\"2febc37f87ff2\",\"_type\":\"span\",\"marks\":[],\"text\":\") and reranking to the top three documents (\"},{\"_key\":\"2febc37f87ff3\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"top_n=3\"},{\"_key\":\"2febc37f87ff4\",\"_type\":\"span\",\"marks\":[],\"text\":\").\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d744b901d2d7\",\"_type\":\"colabFile\",\"asset\":{\"_ref\":\"file-c3d071ccb9aadca3c841b14e5557b2b27ef8d1e1-ipynb\",\"_type\":\"reference\"}},{\"_key\":\"ab235ca27612\",\"_type\":\"block\",\"children\":[{\"_key\":\"4b38576facad0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Looking at these, we have dropped the one relevant chunk of text from document \"},{\"_key\":\"4b38576facad1\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"1\"},{\"_key\":\"4b38576facad2\",\"_type\":\"span\",\"marks\":[],\"text\":\" and \"},{\"_key\":\"4b38576facad3\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"no relevant\"},{\"_key\":\"4b38576facad4\",\"_type\":\"span\",\"marks\":[],\"text\":\" chunks of text from document \"},{\"_key\":\"4b38576facad5\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"2\"},{\"_key\":\"4b38576facad6\",\"_type\":\"span\",\"marks\":[],\"text\":\"  the following relevant pieces of information now replace these:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b422a2a2d698\",\"_type\":\"tableField\",\"tableInput\":{\"rows\":[{\"_key\":\"53a8d273-e210-4ce0-9e2b-b00dc60d33bd\",\"_type\":\"tableRow\",\"cells\":[\"Original Position\",\"Rerank Position\",\"Chunk\"]},{\"_key\":\"3c2e4688-fe9f-41ed-a264-7584feef8b17\",\"_type\":\"tableRow\",\"cells\":[\"23\",\"1\",\"\\\"train language models that act as helpful and harmless assistants\\\"\"]},{\"_key\":\"320e3e8e-516f-46b6-825a-6821532eedff\",\"_type\":\"tableRow\",\"cells\":[\"23\",\"1\",\"\\\"RLHF training also improves honesty\\\"\"]},{\"_key\":\"24376f08-8bd5-405b-a81c-3e6dfc685c84\",\"_type\":\"tableRow\",\"cells\":[\"23\",\"1\",\"\\\"RLHF improves helpfulness and harmlessness by a huge margin\\\"\"]},{\"_key\":\"07a1ebbe-fda2-4cab-ac94-971cf5a798eb\",\"_type\":\"tableRow\",\"cells\":[\"23\",\"1\",\"\\\"enhance the capabilities of large models\\\"\"]},{\"_key\":\"2bdea449-bcb5-4384-96cb-f0b29e223e55\",\"_type\":\"tableRow\",\"cells\":[\"14\",\"2\",\"\\\"the model outputs safe responses\\\"\"]},{\"_key\":\"f6b91520-dedc-404c-b821-4701cf4d5984\",\"_type\":\"tableRow\",\"cells\":[\"14\",\"2\",\"\\\"often more detailed than what the average annotator writes\\\"\"]},{\"_key\":\"6f047bf3-6dc2-4d29-8b74-ccd50de8e53e\",\"_type\":\"tableRow\",\"cells\":[\"14\",\"2\",\"\\\"RLHF to reach the model how to write more nuanced responses\\\"\"]},{\"_key\":\"c435d4ac-8faf-4d1e-be6d-e0617bb7c42f\",\"_type\":\"tableRow\",\"cells\":[\"14\",\"2\",\"\\\"make the model more robust to jailbreak attempts\\\"\"]}]}},{\"_key\":\"9703122318a8\",\"_type\":\"block\",\"children\":[{\"_key\":\"34e37fb4fb7e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"After reranking, we have\"},{\"_key\":\"ae198355e831\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"far more\"},{\"_key\":\"cc249079b163\",\"_type\":\"span\",\"marks\":[],\"text\":\"relevant information. Naturally, this can result in significantly better performance for RAG. It means we maximize relevant information while minimizing noise input into our LLM.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2d35f4398409\",\"_type\":\"horizontalLine\",\"style\":\"Solid\"},{\"_key\":\"22f56f0fd0d5\",\"_type\":\"block\",\"children\":[{\"_key\":\"ae4f83257fb90\",\"_type\":\"span\",\"marks\":[],\"text\":\"Reranking is one of the simplest methods for dramatically improving recall performance in \"},{\"_key\":\"ae4f83257fb91\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"R\"},{\"_key\":\"ae4f83257fb92\",\"_type\":\"span\",\"marks\":[],\"text\":\"etrieval \"},{\"_key\":\"ae4f83257fb93\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"A\"},{\"_key\":\"ae4f83257fb94\",\"_type\":\"span\",\"marks\":[],\"text\":\"ugmented \"},{\"_key\":\"ae4f83257fb95\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"G\"},{\"_key\":\"ae4f83257fb96\",\"_type\":\"span\",\"marks\":[],\"text\":\"eneration (RAG) or any other retrieval-based pipeline.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4f0510a67140\",\"_type\":\"block\",\"children\":[{\"_key\":\"21d37abf98de0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We've explored why \"},{\"_key\":\"4e7e9f9aaa1f\",\"_type\":\"span\",\"marks\":[\"295c9ea8d44f\"],\"text\":\"rerankers can provide so much better performance\"},{\"_key\":\"7e7f9def19cf\",\"_type\":\"span\",\"marks\":[],\"text\":\" than their embedding model counterparts  and how a two-stage retrieval system allows us to get the best of both, enabling search at scale while maintaining quality performance.\"}],\"markDefs\":[{\"_key\":\"295c9ea8d44f\",\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/refine-with-rerank/\"}],\"style\":\"normal\"},{\"_key\":\"115bfb9d1a8e\",\"_type\":\"horizontalLine\",\"style\":\"Solid\"},{\"_key\":\"2f98a96bd2db\",\"_type\":\"block\",\"children\":[{\"_key\":\"f872f5f1d2280\",\"_type\":\"span\",\"marks\":[],\"text\":\"References\"}],\"markDefs\":[],\"style\":\"h2\"},{\"_key\":\"371a091d378a\",\"_type\":\"block\",\"children\":[{\"_key\":\"48bc26f52b7e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"[1] \"},{\"_key\":\"48bc26f52b7e1\",\"_type\":\"span\",\"marks\":[\"a016c39699d1\"],\"text\":\"Introducing 100K Context Windows\"},{\"_key\":\"48bc26f52b7e2\",\"_type\":\"span\",\"marks\":[],\"text\":\" (2023), Anthropic\"}],\"markDefs\":[{\"_key\":\"a016c39699d1\",\"_type\":\"link\",\"href\":\"https://www.anthropic.com/index/100k-context-windows\"}],\"style\":\"normal\"},{\"_key\":\"b8a0bb0008f5\",\"_type\":\"block\",\"children\":[{\"_key\":\"eb1db7457b8f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"[2] N. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, P. Liang, \"},{\"_key\":\"eb1db7457b8f1\",\"_type\":\"span\",\"marks\":[\"82dfbe390d90\"],\"text\":\"Lost in the Middle: How Language Models Use Long Contexts\"},{\"_key\":\"eb1db7457b8f2\",\"_type\":\"span\",\"marks\":[],\"text\":\" (2023),\"}],\"markDefs\":[{\"_key\":\"82dfbe390d90\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2307.03172\"}],\"style\":\"normal\"},{\"_key\":\"a7051a9f4e79\",\"_type\":\"block\",\"children\":[{\"_key\":\"be88ef614d280\",\"_type\":\"span\",\"marks\":[],\"text\":\"[3] N. Reimers, I. Gurevych, \"},{\"_key\":\"be88ef614d281\",\"_type\":\"span\",\"marks\":[\"4c29fe15e918\"],\"text\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"},{\"_key\":\"be88ef614d282\",\"_type\":\"span\",\"marks\":[],\"text\":\" (2019), UKP-TUDA\"}],\"markDefs\":[{\"_key\":\"4c29fe15e918\",\"_type\":\"link\",\"href\":\"https://arxiv.org/pdf/1908.10084.pdf\"}],\"style\":\"normal\"}],\"collapsible\":true}],[\"$\",\"hr\",null,{\"className\":\"mt-25\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex w-full flex-col gap-75 lg:flex-row\",\"children\":[[\"$\",\"div\",null,{\"className\":\"overflow-hidden pt-25 lg:order-2 [\u0026\u003e*:first-child]:mt-0\",\"children\":[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"(shared-layout)\",\"children\",\"learn\",\"children\",\"series\",\"children\",\"$d\",\"children\",\"$2f\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]}],[\"$\",\"aside\",null,{\"className\":\"relative w-full lg:order-1 lg:w-[12.625rem] lg:shrink-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"lg:sticky lg:top-32\",\"children\":[\"$\",\"div\",null,{\"className\":\"scrollbar-hidden relative lg:max-h-screen lg:overflow-y-auto lg:py-25 lg:pb-36\",\"children\":[[\"$\",\"$L30\",null,{\"alt\":\"Learn how to build advanced retrieval augmented generation (RAG) pipelines. (series cover image)\",\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png\",\"quality\":100,\"width\":248,\"height\":320,\"priority\":true,\"sizes\":\"202px\",\"className\":\"w-full max-w-[12.625rem]\"}],[\"$\",\"$L1f\",null,{\"href\":\"/learn/series/rag\",\"className\":\"mt-3 block text-body font-semibold text-text-primary hover:underline\",\"children\":\"Retrieval Augmented Generation\"}],[\"$\",\"div\",null,{\"className\":\"mt-25 text-small\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-semibold text-text-secondary\",\"children\":\"Chapters\"}],[\"$\",\"ol\",null,{\"className\":\"mt-3 flex list-outside list-decimal flex-col gap-3 pl-7 !text-text-secondary\",\"children\":[[\"$\",\"li\",\"431fc97e-30f7-4964-ae00-a3cae9d8185d\",{\"className\":\"group\",\"children\":[[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/series/rag/rerankers\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"text-text-secondary hover:text-text-primary decoration-none hover:underline transition-all duration-200\",\"children\":\"Rerankers for RAG\"}],[\"$\",\"div\",null,{\"className\":\"hidden pl-4 lg:block\",\"children\":[\"$\",\"$L2e\",null,{\"articleContent\":\"$31\"}]}]]}],[\"$\",\"li\",\"aa0332c3-c3e6-43f5-b618-444779c29dee\",{\"className\":\"group\",\"children\":[[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/series/rag/embedding-models-rundown\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"text-text-secondary hover:text-text-primary decoration-none hover:underline transition-all duration-200\",\"children\":\"Embedding Models\"}],false]}],[\"$\",\"li\",\"225c80ca-2d9c-4289-9899-38c94ac02c30\",{\"className\":\"group\",\"children\":[[\"$\",\"$L1f\",null,{\"id\":\"$undefined\",\"href\":\"/learn/series/rag/ragas\",\"target\":\"$undefined\",\"onClick\":\"$undefined\",\"className\":\"text-text-secondary hover:text-text-primary decoration-none hover:underline transition-all duration-200\",\"children\":\"Agent Evaluation\"}],false]}]]}]]}]]}]}]}]]}]]}],\"$undefined\",[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute left-0 top-0 w-full\",\"children\":[[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"}]],[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"}]]]}],[\"$\",\"div\",null,{\"role\":\"presentation\",\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute bottom-0 left-0 w-full\",\"children\":[[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l bottom-0 left-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]\"}]],[[\"$\",\"span\",null,{\"className\":\"z-1 absolute border-l bottom-0 right-0 h-[7.125rem]\"}],[\"$\",\"span\",null,{\"className\":\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]\"}]]]}]]}]}],[\"$\",\"div\",null,{\"id\":\"$undefined\",\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative px-4 sm:px-8 !px-0 !mx-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center font-semibold md:flex-row md:items-start w-full border-border border divide-x divide-border\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full\"}],[\"$\",\"div\",null,{\"className\":\"w-full\"}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]}]}]]}]\n"])</script><iframe name="__uspapiLocator" style="display: none;"></iframe><script id="koala-script" data-nscript="afterInteractive">
          !function(t){if(window.ko)return;window.ko=[],["identify","track","removeListeners","open","on","off","qualify","ready"].forEach(function(t){ko[t]=function(){var n=[].slice.call(arguments);return n.unshift(t),ko.push(n),ko}});var n=document.createElement("script");n.async=!0,n.setAttribute("src","https://cdn.getkoala.com/v1/pk_dedf7f497457f685cec291ae4d5002e828c1/sdk.js"),(document.body || document.head).appendChild(n)}();
        </script><script async="" src="https://cdn.getkoala.com/v1/pk_dedf7f497457f685cec291ae4d5002e828c1/sdk.js"></script><script src="https://js.hs-scripts.com/8231564.js" id="hs-script-loader" data-nscript="afterInteractive"></script><script type="osano/blocked" id="clearbits-script" referrerpolicy="strict-origin-when-cross-origin" data-nscript="afterInteractive"></script><script id="partnerstack-script" data-nscript="afterInteractive">
          (function() {var gs = document.createElement('script');gs.src = 'https://js.partnerstack.com/v1/';gs.type = 'text/javascript';gs.async = 'true';gs.onload = gs.onreadystatechange = function() {var rs = this.readyState;if (rs && rs != 'complete' && rs != 'loaded') return;try {growsumo._initialize('pk_jvoXqtGYtmYGQJlSZmilQ0OR1zIkTziF'); if (typeof(growsumoInit) === 'function') {growsumoInit();}} catch (e) {}};var s = document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gs, s);})();
        </script><next-route-announcer style="position: absolute;"></next-route-announcer><script>(self.__next_s=self.__next_s||[]).push(["/scripts/consent-data-layer.js",{"id":"consent-data-layer"}])</script><script>(self.__next_s=self.__next_s||[]).push(["https://cmp.osano.com/AzZbUITh61cXz4HqL/fee5a4ca-0585-49b2-b317-c6a8cf7b66fa/osano.js",{"id":"osano-cmp-script"}])</script><style>
    html.theme-initializing {
      opacity: 0 !important;
      visibility: hidden !important;
      pointer-events: none !important;
      background-color: var(--background) !important;
    }
  </style><script nonce="theme-script">document.documentElement.classList.add('theme-initializing'); ((e,t,r,s)=>{document.documentElement.classList.add("theme-initializing");let n=document.documentElement;function i(e){let t="system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;n.setAttribute("data-initial-theme",e),n.dataset.theme=t,n.classList.remove("light","dark"),n.classList.add(t),n.style.colorScheme=t,n.style.setProperty("--theme",`"${t}"`)}try{let e=function(){try{let e=("; "+document.cookie).split("; "+t+"=");if(2===e.length){let t=e.pop()?.split(";").shift();if(t&&["light","dark","system"].includes(t))return t}}catch(e){}return r}();i(e),"system"===e&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",()=>{i("system")}),window.addEventListener("pinecone-theme-change",function(t){t.detail&&t.detail.theme&&(e=t.detail.theme,i(t.detail.theme))}),document.addEventListener("visibilitychange",()=>{if("visible"===document.visibilityState){let r=function(){try{let e=("; "+document.cookie).split("; "+t+"=");if(2===e.length){let t=e.pop()?.split(";").shift();if(t&&["light","dark","system"].includes(t))return t}}catch(e){}return null}();if(r&&r!==e){e=r,i(r);try{let e=window.location.hostname,t=e.includes("pinecone.io")&&!e.includes("localhost"),r=Date.now().toString();document.cookie=`theme_change_timestamp=${r};path=/${t?";domain=.pinecone.io":""};max-age=31536000`}catch(e){}}}}),n.classList.add("ssr-theme-applied"),requestAnimationFrame(()=>{n.classList.remove("theme-initializing")});let s=setTimeout(()=>{n.classList.contains("theme-initializing")&&n.classList.remove("theme-initializing")},300);window.addEventListener("beforeunload",()=>{clearTimeout(s)},{once:!0})}catch(e){n.classList.remove("theme-initializing")}})("pinecone_theme_mode","pinecone_theme_mode","system",null)</script><script>(self.__next_s=self.__next_s||[]).push([0,{"children":"window._vwo_code || (function() {\n            var account_id=758750,\n            version=2.1,\n            settings_tolerance=2000,\n            hide_element='body',\n            hide_element_style = 'opacity:0 !important;filter:alpha(opacity=0) !important;background:none !important',\n            /* DO NOT EDIT BELOW THIS LINE */\n            f=false,w=window,d=document,v=d.querySelector('#vwoCode'),cK='_vwo_'+account_id+'_settings',cc={};try{var c=JSON.parse(localStorage.getItem('_vwo_'+account_id+'_config'));cc=c&&typeof c==='object'?c:{}}catch(e){}var stT=cc.stT==='session'?w.sessionStorage:w.localStorage;code={use_existing_jquery:function(){return typeof use_existing_jquery!=='undefined'?use_existing_jquery:undefined},library_tolerance:function(){return typeof library_tolerance!=='undefined'?library_tolerance:undefined},settings_tolerance:function(){return cc.sT||settings_tolerance},hide_element_style:function(){return'{'+(cc.hES||hide_element_style)+'}'},hide_element:function(){if(performance.getEntriesByName('first-contentful-paint')[0]){return''}return typeof cc.hE==='string'?cc.hE:hide_element},getVersion:function(){return version},finish:function(e){if(!f){f=true;var t=d.getElementById('_vis_opt_path_hides');if(t)t.parentNode.removeChild(t);if(e)(new Image).src='https://dev.visualwebsiteoptimizer.com/ee.gif?a='+account_id+e}},finished:function(){return f},addScript:function(e){var t=d.createElement('script');t.type='text/javascript';if(e.src){t.src=e.src}else{t.text=e.text}d.getElementsByTagName('head')[0].appendChild(t)},load:function(e,t){var i=this.getSettings(),n=d.createElement('script'),r=this;t=t||{};if(i){n.textContent=i;d.getElementsByTagName('head')[0].appendChild(n);if(!w.VWO||VWO.caE){stT.removeItem(cK);r.load(e)}}else{var o=new XMLHttpRequest;o.open('GET',e,true);o.withCredentials=!t.dSC;o.responseType=t.responseType||'text';o.onload=function(){if(t.onloadCb){return t.onloadCb(o,e)}if(o.status===200){_vwo_code.addScript({text:o.responseText})}else{_vwo_code.finish('&e=loading_failure:'+e)}};o.onerror=function(){if(t.onerrorCb){return t.onerrorCb(e)}_vwo_code.finish('&e=loading_failure:'+e)};o.send()}},getSettings:function(){try{var e=stT.getItem(cK);if(!e){return}e=JSON.parse(e);if(Date.now()>e.e){stT.removeItem(cK);return}return e.s}catch(e){return}},init:function(){if(d.URL.indexOf('__vwo_disable__')>-1)return;var e=this.settings_tolerance();w._vwo_settings_timer=setTimeout(function(){_vwo_code.finish();stT.removeItem(cK)},e);var t;if(this.hide_element()!=='body'){t=d.createElement('style');var i=this.hide_element(),n=i?i+this.hide_element_style():'',r=d.getElementsByTagName('head')[0];t.setAttribute('id','_vis_opt_path_hides');v&&t.setAttribute('nonce',v.nonce);t.setAttribute('type','text/css');if(t.styleSheet)t.styleSheet.cssText=n;else t.appendChild(d.createTextNode(n));r.appendChild(t)}else{t=d.getElementsByTagName('head')[0];var n=d.createElement('div');n.style.cssText='z-index: 2147483647 !important;position: fixed !important;left: 0 !important;top: 0 !important;width: 100% !important;height: 100% !important;background: white !important;';n.setAttribute('id','_vis_opt_path_hides');n.classList.add('_vis_hide_layer');t.parentNode.insertBefore(n,t.nextSibling)}var o='https://dev.visualwebsiteoptimizer.com/j.php?a='+account_id+'&u='+encodeURIComponent(d.URL)+'&vn='+version;if(w.location.search.indexOf('_vwo_xhr')!==-1){this.addScript({src:o})}else{this.load(o+'&x=true')}}};w._vwo_code=code;code.init();})();","id":"vwoCode"}])</script><div class="container"><div class="bg-background flex border-x"><a class="text-text-primary block grow py-2 pl-4 text-sm/[1.2] sm:pl-8  pr-4 md:pr-0" href="/blog/announcing-pinecone-pioneers-program/">Announcing the Pinecone Pioneers program: for developers who share and educate
 - <span class="underline">Discover More</span></a><button class="text-text-secondary hover:text-text-primary group flex cursor-pointer items-center gap-1 pr-4 text-xs/[1.2] transition-colors sm:pr-8">Dismiss <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x text-text-secondary group-hover:text-text-primary w-3.5 transition-colors"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button></div></div><header class="z-50 w-full border-y sticky left-0 right-0 top-0 z-10 bg-background "><div class="container"><div class="relative px-4 sm:px-8 bg-paper flex h-[4.5625rem] items-center justify-between w-full border-x"><div class="block"><a class="w-32 shrink-0 focus:outline-offset-4 focus:outline-alpha2 block" aria-label="Pinecone" href="/"><svg enable-background="new 0 0 1077 220" class="w-full -translate-y-[4px]" viewBox="0 0 1077 220" xmlns="http://www.w3.org/2000/svg"><g fill="var(--text-primary)"><path d="m246.4 51.4h55.2c39.9 0 50.1 23.5 50.1 42.6s-10.3 42.6-50.1 42.6h-34.1v67.2h-21.1zm21.2 67.2h27.9c16.8 0 33.5-3.8 33.5-24.5s-16.8-24.5-33.5-24.5h-27.9z"></path><path d="m379.4 50.7c8 0 14.5 6.3 14.6 14 .1 7.8-6.2 14.2-14.2 14.4s-14.6-5.9-14.9-13.7c-.1-3.9 1.4-7.6 4.1-10.4 2.6-2.7 6.4-4.3 10.4-4.3zm-9.8 51.1h19.6v102.1h-19.6z"></path><path d="m412 101.8h19.9v15.8h.5c6.9-12 20.3-19.2 34.4-18.3 20.3 0 37.8 11.9 37.8 39v65.7h-19.6v-60.2c0-19.2-11.3-26.3-23.9-26.3-16.5 0-29.1 10.3-29.1 34v52.5h-20z"></path><path d="m540.9 160c0 17.8 17 29.5 35.3 29.5 11.5-.3 22.2-6 28.7-15.2l15.1 11.2c-11 14.1-28.4 22-46.5 21.1-33.2 0-53.8-23.2-53.8-53.6-.6-14.2 4.9-28 15.1-38.1s24.2-15.7 38.8-15.5c36.9 0 51 27.5 51 53.8v6.9h-83.7zm62.8-15.5c-.5-17-10.2-29.5-30.2-29.5-17.1-.1-31.4 12.8-32.5 29.5z"></path><path d="m714.6 129c-6.5-7.5-16-11.7-26.1-11.4-21.6 0-32.7 17-32.7 36.2-.5 9.1 2.8 17.9 9.2 24.5s15.3 10.3 24.6 10.2c9.8.3 19.2-3.9 25.4-11.4l14.1 13.6c-10.3 10.6-24.8 16.3-39.7 15.7-14.7.7-29-4.6-39.4-14.8-10.4-10.1-16-24.1-15.3-38.4-.7-14.4 4.9-28.4 15.3-38.6s24.7-15.7 39.5-15.2c15.1-.4 29.7 5.5 40.2 16.1z"></path><path d="m787.9 99.2c30.2.5 54.4 24.7 54.1 54.2s-25 53.2-55.3 53c-30.2-.2-54.7-24.1-54.7-53.6 0-14.4 5.9-28.1 16.4-38.2 10.6-10.1 24.8-15.6 39.5-15.4zm0 89.2c21.1 0 34.4-14.7 34.4-35.6 0-20.8-13.3-35.5-34.4-35.5s-34.5 14.7-34.5 35.5 13.3 35.6 34.5 35.6z"></path><path d="m859.7 101.8h19.9v15.8c6.9-12.1 20.4-19.2 34.5-18.3 20.3 0 37.8 11.9 37.8 39v65.7h-19.9v-60.2c0-19.2-11.3-26.3-23.8-26.3-16.6 0-29.1 10.3-29.1 34v52.5h-19.3v-102.2z"></path><path d="m988.8 160c0 17.8 17 29.5 35.3 29.5 11.5-.4 22.2-6 28.7-15.2l15.1 11.2c-11 14-28.3 21.8-46.4 20.8-33.1 0-53.8-23.2-53.8-53.6-.6-14.2 4.9-28.1 15.1-38.2 10.2-10.2 24.3-15.7 38.9-15.4 36.9 0 51 27.5 51 53.8v6.9zm62.7-15.6c-.5-17-10.1-29.5-30.2-29.5-17.1-.1-31.4 12.8-32.5 29.5z"></path><path clip-rule="evenodd" d="m127 6.4c-2.1-2.5-5.6-3.1-8.4-1.5l-2.6 1.4-28.3 16.1 6.6 11.6 18.4-10.5-4.5 24.6 13.1 2.4 4.6-24.7 13.6 16.2 10.2-8.6-20.6-24.6h-.1zm-39.7 207.5c6.8 0 12.3-5.4 12.3-12s-5.5-12-12.3-12-12.3 5.4-12.3 12c-.1 6.6 5.5 12 12.3 12zm16.5-65.9-4.4 24.7-13.2-2.4 4.4-24.6-18.4 10.6-6.7-11.6 28.1-16.1 2.6-1.5c2.8-1.6 6.3-1 8.4 1.5l2 2.4 20.9 24.5-10.2 8.7zm10.7-59-4.4 24.7-13.2-2.4 4.4-24.5-18.3 10.5-6.6-11.6 28-16v-.2h.2l2.6-1.5c2.8-1.6 6.3-1 8.4 1.5l2 2.3 20.8 24.6-10.2 8.7zm-86.3 97.6h-.1l-2.7-.8c-2.9-.8-4.8-3.6-4.6-6.6l2.4-33.4 12.7.9-1.5 20.3 19.7-13.4 7.1 10.5-19.3 13.1 19.7 5.7-3.5 12.2zm130.7 13.8-.9 2.9c-.9 2.8-3.5 4.7-6.5 4.5l-2.8-.2-.2.1-.1-.1-31-2.1.8-12.7 20.6 1.4-13.5-18.9 10.3-7.4 13.8 19.4 6-19.6 12.1 3.7zm36.4-68.8 1.5 2.7c1.5 2.7.9 6.1-1.5 8.1l-2.2 1.9v.1h-.1l-24.1 20.4-8.4-9.9 15.8-13.4-23.7-4.2 2.3-12.8 23.9 4.2-10-18 11.3-6.3zm-24.5-55.8-21.4 11.5-6.2-11.4 21.1-11.3-19.3-7.9 4.9-12 29.4 11.9.1-.1.1.2 2.7 1.1c2.9 1.2 4.5 4.2 4 7.2l-.5 3-5.5 30.5-12.8-2.3zm-143.6 26.8 23.8 4-2.2 12.8-24-4.1 10.2 18-11.3 6.4-15.4-27.1-1.5-2.6c-1.5-2.7-.9-6.1 1.4-8.1l2.2-1.9v-.1h.1l23.8-20.5 8.5 9.9zm35.9-55.4 15.8 17.6-9.7 8.7-16.2-18-3.7 20.5-12.8-2.3 5.6-30.4.6-3.1c.5-3 3.1-5.2 6.1-5.3l2.8-.1.1-.1.1.1 31.8-1.3.5 13z" fill-rule="evenodd"></path></g></svg></a></div><nav class="flex h-full w-full select-none flex-wrap items-center leading-none md:ml-8 lg:items-stretch"><div class="hidden gap-4 lg:flex"><div class="group relative flex cursor-pointer after:absolute after:-left-[25%] after:h-[calc(100%+1px)] after:w-[150%]"><button aria-label="Product"><span class="text-text-primary text-15 group-hover:border-border border border-transparent px-[0.9375rem] py-3 leading-none">Product</span></button></div><a href="https://docs.pinecone.io" class="focus:outline-hidden group flex items-center"><span class="text-text-primary text-15 group-hover:border-border border border-transparent px-[0.9375rem] py-3 leading-none">Docs</span></a><a class="focus:outline-hidden group flex items-center" href="/customers/"><span class="text-text-primary text-15 group-hover:border-border border border-transparent px-[0.9375rem] py-3 leading-none">Customers</span></a><div class="group relative flex cursor-pointer after:absolute after:-left-[25%] after:h-[calc(100%+1px)] after:w-[150%]"><button aria-label="Resources"><span class="text-text-primary text-15 group-hover:border-border border border-transparent px-[0.9375rem] py-3 leading-none">Resources</span></button></div><a class="focus:outline-hidden group flex items-center" href="/pricing/"><span class="text-text-primary text-15 group-hover:border-border border border-transparent px-[0.9375rem] py-3 leading-none">Pricing</span></a></div><div class="xs:flex ml-auto items-center whitespace-nowrap"><div><button aria-label="AI Chat" class="hover:border-text-primary hover:border flex h-[39px] w-[39px] cursor-pointer items-center justify-center border transition-colors mr-0"><span class="transition-all duration-300"><div class="group relative flex items-center gap-3"><svg width="16" height="16" viewBox="0 0 14 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0625 6.4375C10.0625 4.87891 9.21484 3.45703 7.875 2.66406C6.50781 1.87109 4.83984 1.87109 3.5 2.66406C2.13281 3.45703 1.3125 4.87891 1.3125 6.4375C1.3125 8.02344 2.13281 9.44531 3.5 10.2383C4.83984 11.0312 6.50781 11.0312 7.875 10.2383C9.21484 9.44531 10.0625 8.02344 10.0625 6.4375ZM9.21484 10.9219C8.23047 11.6875 7 12.125 5.6875 12.125C2.54297 12.125 0 9.58203 0 6.4375C0 3.32031 2.54297 0.75 5.6875 0.75C8.80469 0.75 11.375 3.32031 11.375 6.4375C11.375 7.77734 10.9102 9.00781 10.1445 9.99219L13.5352 13.3828L14 13.8477L13.0703 14.75L12.6055 14.2852L9.21484 10.8945V10.9219Z" class="fill-text-primary"></path></svg><div id="splash" class="absolute left-0 top-0 z-0 hidden h-full w-full scale-[1.75] rounded-full bg-text-primary opacity-0 transition-opacity duration-300 lg:block"></div></div></span></button></div><div class="lg-1:block hidden"><div class="relative"><button class="hover:border-text-primary hover:border flex h-[39px] w-[39px] cursor-pointer items-center justify-center border transition-colors border-l-0 border-r-0" aria-label="Current theme: light. Click to change." title="Current theme: light. Click to change."><span class="transition-all duration-300"><svg width="16" height="16" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg"><path class="fill-current undefined" d="M6.5625 1.0625V2.5625V3.125H5.4375V2.5625V1.0625V0.5H6.5625V1.0625ZM10.2422 3.05469L9.16406 4.13281L8.76562 4.53125L7.96875 3.73438L8.36719 3.33594L9.44531 2.25781L9.84375 1.85938L10.6406 2.65625L10.2422 3.05469ZM2.53125 2.25781L3.60938 3.33594L4.00781 3.73438L3.21094 4.53125L2.8125 4.13281L1.75781 3.05469L1.35938 2.65625L2.15625 1.85938L2.55469 2.25781H2.53125ZM0.5625 5.9375H2.0625H2.625V7.0625H2.0625H0.5625H0V5.9375H0.5625ZM9.9375 5.9375H11.4375H12V7.0625H11.4375H9.9375H9.375V5.9375H9.9375ZM3.60938 9.6875L2.53125 10.7422L2.15625 11.1406L1.35938 10.3438L1.75781 9.94531L2.8125 8.89062L3.21094 8.49219L4.00781 9.28906L3.60938 9.6875ZM9.16406 8.89062L10.2422 9.96875L10.6406 10.3672L9.84375 11.1406L9.44531 10.7422L8.36719 9.6875L7.96875 9.28906L8.76562 8.49219L9.16406 8.89062ZM6.5625 10.4375V11.9375V12.5H5.4375V11.9375V10.4375V9.875H6.5625V10.4375ZM7.5 6.5C7.5 5.98438 7.19531 5.49219 6.75 5.21094C6.28125 4.95312 5.69531 4.95312 5.25 5.21094C4.78125 5.49219 4.5 5.98438 4.5 6.5C4.5 7.03906 4.78125 7.53125 5.25 7.8125C5.69531 8.07031 6.28125 8.07031 6.75 7.8125C7.19531 7.53125 7.5 7.03906 7.5 6.5ZM3.375 6.5C3.375 5.5625 3.86719 4.71875 4.6875 4.25C5.48438 3.75781 6.49219 3.75781 7.3125 4.25C8.10938 4.71875 8.625 5.5625 8.625 6.5C8.625 7.46094 8.10938 8.30469 7.3125 8.77344C6.49219 9.26562 5.48438 9.26562 4.6875 8.77344C3.86719 8.30469 3.375 7.46094 3.375 6.5Z"></path></svg></span></button></div></div><div class="flex items-center gap-0"><a class="text-15/[140%] text-text-primary hover:border-brand-blue lg-1:block hidden border border-r-transparent px-4 py-2 transition-colors" href="/contact/">Contact</a><a href="https://app.pinecone.io/?sessionType=login" class="text-15/[140%] text-text-primary hover:border-brand-blue lg-2:block hidden border border-r-transparent px-4 py-2 transition-colors">Log in</a><a data-state="scrolled" href="https://app.pinecone.io/?sessionType=signup" class="border-brand-blue text-15/[140%] text-text-contrast bg-brand-blue hover:bg-primary-dark hover:text-background hidden border-y border-r px-4 py-2 transition-colors lg:block">Sign up</a></div></div></nav><button class="ml-2 flex h-[2.3125rem] w-[2.3125rem] shrink-0 cursor-pointer items-center  justify-center border lg:hidden" aria-label="Show navigation"><svg width="14" height="12" viewBox="0 0 14 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0H14V1.5H0V0ZM0 5H14V6.5H0V5ZM14 10V11.5H0V10H14Z" class="fill-text-primary"></path></svg></button></div></div></header><main><article><div class="border-b"><div class="container"><div class="relative px-4 sm:px-8 flex flex-col gap-8 pb-4 pt-16 md:gap-16 md:pb-8 md:pt-24 xl:flex-row"><div class="w-full xl:w-[13.5rem]"></div><div class="flex-1 space-y-8"><div><h1 class="xxs:text-[38px] sm:text-h1/[1.1] text-text-primary text-[30px]">Rerankers and Two-Stage Retrieval</h1></div></div><div class="flex w-full flex-wrap justify-between gap-4 xl:w-[13.5rem] xl:flex-col"><div class="space-y-4"></div></div><div role="presentation" aria-hidden="true" class="pointer-events-none absolute inset-0"><span class="from-background absolute bottom-0 left-0 top-0 h-full w-[1px] border-l"></span><span class="from-background absolute bottom-0 right-0 top-0 h-full w-[1px] border-r"></span></div></div></div></div><div class="container"><div class="relative px-4 sm:px-8 !px-0 !mx-0"><div class="container pb-75 pt-25 lg:pb-150 lg:pt-25"><div class="block lg:hidden"><div class="text-small text-[#71717A]"><button class="flex items-center gap-1.5 disabled:cursor-auto w-full">Jump to section <svg width="11" height="12" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg" class="mt-0.5"><rect x="12" y="5.5" width="2" height="12" transform="rotate(90 12 5.5)" fill="#B7B7DD"></rect></svg></button><ul class="mt-4 flex flex-col gap-3 underline !decoration-none text-text-secondary group-hover:text-text-primary transition-all duration-200 text-sm"><li><a href="#Recall-vs.-Context-Windows" class="hover:underline hover:opacity-50 cursor-pointer">Recall vs. Context Windows</a></li><li><a href="#Power-of-Rerankers" class="hover:underline hover:opacity-50 cursor-pointer">Power of Rerankers</a></li><li><a href="#Implementing-Two-Stage-Retrieval-with-Reranking" class="hover:underline hover:opacity-50 cursor-pointer">Implementing Two-Stage Retrieval with Reranking</a></li><li><a href="#References" class="hover:underline hover:opacity-50 cursor-pointer">References</a></li></ul></div><hr class="mt-25"></div><div class="flex w-full flex-col gap-75 lg:flex-row"><div class="overflow-hidden pt-25 lg:order-2 [&amp;>*:first-child]:mt-0"><article><div class=""><div class="flex w-full flex-col gap-75 lg:flex-row"><div class="overflow-hidden lg:order-2 mb-12 [&amp;>*:first-child]:mt-0"><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary"><a class="cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline" href="/learn/retrieval-augmented-generation/">Retrieval Augmented Generation (RAG)</a> is an overloaded term. It promises the world, but after developing a RAG pipeline, there are many of us left wondering why it doesn't work as well as we had expected.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">As with most tools, RAG is easy to use but hard to master. The truth is that there is more to RAG than putting documents into a vector DB and adding an LLM on top. That <em>can work</em>, but it won't always.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">This ebook aims to tell you what to do when out-of-the-box RAG <em>doesn't</em> work. In this first chapter, we'll look at what is often the easiest and fastest to implement solution for suboptimal RAG pipelines  we'll be learning about <em>rerankers</em>.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary"></p><div class="flex flex-col items-center"><div class="relative mt-50 w-full"><article class="yt-lite " data-title="Youtube Video Player" style="background-image: url(&quot;https://i.ytimg.com/vi/Uh9bYiVrW_s/hqdefault.jpg&quot;); --aspect-ratio: 56.25%;"><button type="button" class="lty-playbtn" aria-label="Watch Youtube Video Player"></button></article></div><div class="pt-3 text-center text-sm">Video companion for this chapter.</div></div><hr class="my-8 border-solid"><h2 class="text-h2-mobile lg:text-h2 text-text-primary my-[40px]" id="Recall-vs.-Context-Windows">Recall vs. Context Windows</h2><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Before jumping into the solution, let's talk about the problem. With RAG, we are performing a <em>semantic search</em> across many text documents  these could be tens of thousands up to tens of billions of documents.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">To ensure fast search times at scale, we typically use vector search  that is, we transform our text into vectors, place them all into a vector space, and compare their proximity to a query vector using a similarity metric like cosine similarity.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">For vector search to work, we need vectors. These vectors are essentially compressions of the "meaning" behind some text into (typically) 768 or 1536-dimensional vectors. There is some information loss because we're compressing this information into a single vector.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Because of this information loss, we often see that the top three (for example) vector search documents will miss relevant information. Unfortunately, the retrieval may return relevant information below our <code>top_k</code> cutoff.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">What do we do if relevant information at a lower position would help our LLM formulate a better response? The easiest approach is to increase the number of documents we're returning (increase <code>top_k</code>) and pass them all to the LLM.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">The metric we would measure here is <em>recall</em>  meaning "how many of the relevant documents are we retrieving". Recall does not consider the total number of retrieved documents  so we can hack the metric and get <em>perfect</em> recall by returning <em>everything</em>.</p><div class="overflow-x-auto pt-2 md:pt-2"><div class="latex-container text-sm sm:text-base md:text-lg"><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi mathvariant="normal">@</mi><mi>K</mi><mo>=</mo><mfrac><mrow><mi mathvariant="normal">#</mi><mtext></mtext><mi>o</mi><mi>f</mi><mtext></mtext><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mtext></mtext><mi>d</mi><mi>o</mi><mi>c</mi><mi>s</mi><mtext></mtext><mi>r</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>n</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi mathvariant="normal">#</mi><mtext></mtext><mi>o</mi><mi>f</mi><mtext></mtext><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mtext></mtext><mi>d</mi><mi>o</mi><mi>c</mi><mi>s</mi><mtext></mtext><mi>i</mi><mi>n</mi><mtext></mtext><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">recall@K = \frac{\#\;of\;relevant\;docs\;returned}{\#\;of\;relevant\;docs\;in\;dataset}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">rec</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span><span class="mord">@</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">#</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">an</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">ocs</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">in</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">se</span><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">#</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">an</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">ocs</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">re</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Unfortunately, we cannot return everything. LLMs have limits on how much text we can pass to them  we call this limit the <em>context window</em>. Some LLMs have huge context windows, like Anthropic's Claude, with a context window of 100K tokens [1]. With that, we could fit many tens of pages of text  so could we return many documents (not quite all) and "stuff" the context window to improve recall?</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Again, no. We cannot use context stuffing because this reduces the LLM's <em>recall</em> performance  note that this is the LLM recall, which is different from the retrieval recall we have been discussing so far.</p><div class="flex flex-col items-center align-middle md:px-0 mt-7 pb-3"><div class="w-full max-w-full"><div class="flex flex-col items-center justify-center"><div class="flex hover:cursor-zoom-in max-w-full items-center justify-center" style="height: auto; cursor: zoom-in;"><img alt="When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place" loading="lazy" width="1212" height="688" decoding="async" data-nimg="1" class="max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png&amp;w=1920&amp;q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png&amp;w=3840&amp;q=75 2x" src="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png&amp;w=3840&amp;q=75" style=""></div></div></div><div class="w-full text-center text-sm text-text-secondary mt-4">When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place [2].</div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">LLM recall refers to the ability of an LLM to find information from the text placed within its context window. Research shows that LLM recall degrades as we put more tokens in the context window [2]. LLMs are also less likely to follow instructions as we stuff the context window  so context stuffing is a bad idea.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We can increase the number of documents returned by our vector DB to increase retrieval recall, but we cannot pass these to our LLM without damaging LLM recall.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">The solution to this issue is to maximize retrieval recall by retrieving plenty of documents and then maximize LLM recall by <em>minimizing</em> the number of documents that make it to the LLM. To do that, we reorder retrieved documents and keep just the most relevant for our LLM  to do that, we use <em>reranking</em>.</p><h2 class="text-h2-mobile lg:text-h2 text-text-primary my-[40px]" id="Power-of-Rerankers">Power of Rerankers</h2><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">A <a href="https://docs.pinecone.io/models/bge-reranker-v2-m3" class="cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline">reranking model</a>  also known as a <strong>cross-encoder</strong>  is a type of model that, given a query and document pair, will output a similarity score. We use this score to reorder the documents by relevance to our query.</p><div class="flex flex-col items-center align-middle md:px-0 mt-7 pb-3"><div class="w-full max-w-full"><div class="flex flex-col items-center justify-center"><div class="flex hover:cursor-zoom-in max-w-full items-center justify-center" style="height: auto; cursor: zoom-in;"><img alt="A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model." loading="lazy" width="3443" height="1641" decoding="async" data-nimg="1" class="max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F906c3c0f8fe637840f134dbf966839ef89ac7242-3443x1641.png&amp;w=3840&amp;q=75 1x" src="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F906c3c0f8fe637840f134dbf966839ef89ac7242-3443x1641.png&amp;w=3840&amp;q=75" style=""></div></div></div><div class="w-full text-center text-sm text-text-secondary mt-4">A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.</div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Search engineers have used rerankers in two-stage retrieval systems for <em>a long time</em>. In these two-stage systems, a first-stage model (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. Then, a second-stage model (the reranker) is used to rerank those documents retrieved by the first-stage model.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We use two stages because retrieving a small set of documents from a large dataset is much faster than reranking a large set of documents  we'll discuss why this is the case soon  but TL;DR, rerankers are slow, and retrievers are <em>fast</em>.</p><h3 class="text-h3-mobile lg:text-h3 text-text-primary my-[40px]" id="Why-Rerankers">Why Rerankers?</h3><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">If a reranker is so much slower, why bother using them? The answer is that rerankers are much more accurate than <a class="cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline" href="/learn/series/rag/embedding-models-rundown/">embedding models</a>.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">The intuition behind a bi-encoder's inferior accuracy is that bi-encoders must compress all of the possible meanings of a document into a single vector  meaning we lose information. Additionally, bi-encoders have no context on the query because we don't know the query until we receive it (we create embeddings before user query time).</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">On the other hand, a reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Because we are running the reranker at user query time, we have the added benefit of analyzing our document's meaning specific to the user query  rather than trying to produce a generic, averaged meaning.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Rerankers avoid the information loss of bi-encoders  but they come with a different penalty  <em>time</em>.</p><div class="flex flex-col items-center align-middle md:px-0 mt-7 pb-3"><div class="w-full max-w-full"><div class="flex flex-col items-center justify-center"><div class="flex hover:cursor-zoom-in max-w-full items-center justify-center" style="height: auto; cursor: zoom-in;"><img alt="A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time." loading="lazy" width="2760" height="1420" decoding="async" data-nimg="1" class="max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420.png&amp;w=3840&amp;q=75 1x" src="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420.png&amp;w=3840&amp;q=75" style=""></div></div></div><div class="w-full text-center text-sm text-text-secondary mt-4">A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.</div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">When using bi-encoder models with vector search, we frontload all of the heavy transformer computation to when we are creating the initial vectors  that means that when a user queries our system, we have already created the vectors, so all we need to do is:</p><ol class="!marker:text-text-primary ml-8! list-decimal! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0"><li class="pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary"><span class="mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Run a single transformer computation to create the query vector.</span></li><li class="pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary"><span class="mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Compare the query vector to document vectors with <em>cosine similarity</em> (or another lightweight metric).</span></li></ol><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">With rerankers, we are not pre-computing anything. Instead, we're feeding our query and a single other document into the transformer, running a whole transformer inference step, and outputting a single similarity score.</p><div class="flex flex-col items-center align-middle md:px-0 mt-7 pb-3"><div class="w-full max-w-full"><div class="flex flex-col items-center justify-center"><div class="flex hover:cursor-zoom-in max-w-full items-center justify-center" style="height: auto; cursor: zoom-in;"><img alt="A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query." loading="lazy" width="2440" height="1100" decoding="async" data-nimg="1" class="max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&amp;w=3840&amp;q=75 1x" src="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&amp;w=3840&amp;q=75" style=""></div></div></div><div class="w-full text-center text-sm text-text-secondary mt-4">A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.</div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Given 40M records, if we use a <strong>small</strong> reranking model like BERT on a V100 GPU  we'd be waiting more than 50 hours to return a single query result [3]. We can do the same in &lt;100ms with encoder models and vector search.</p><h2 class="text-h2-mobile lg:text-h2 text-text-primary my-[40px]" id="Implementing-Two-Stage-Retrieval-with-Reranking">Implementing Two-Stage Retrieval with Reranking</h2><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Now that we understand the idea and reason behind two-stage retrieval with rerankers, let's see how to implement it (you can follow along with <a href="https://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb" class="cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline">this notebook</a>. To begin we will set up our prerequisite libraries:</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">!pip install <span class="token operator">-</span>qU \
    datasets<span class="token operator">==</span><span class="token number">2.14</span><span class="token number">.5</span> \
    <span class="token string">"pinecone[grpc]"</span><span class="token operator">==</span><span class="token number">5.1</span><span class="token number">.0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><h3 class="text-h3-mobile lg:text-h3 text-text-primary my-[40px]" id="Data-Preparation">Data Preparation</h3><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Before setting up the retrieval pipeline, we need data to retrieve! We will use the <a href="https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked" class="cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline"><code>jamescalam/ai-arxiv-chunked</code></a> dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs  including the Llama 2, GPTQ, and GPT-4 papers.</p><div class="my-50 flex flex-col gap-25 rounded-5 border border-[#ddd] p-25 font-mono"><div class="flex flex-col gap-25"><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class="w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

data <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"jamescalam/ai-arxiv-chunked"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>
data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Downloading data files:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code></pre></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Downloading data:   0%|          | 0.00/153M [00:00&lt;?, ?B/s]</code></pre></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Extracting data files:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code></pre></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Generating train split: 0 examples [00:00, ? examples/s]</code></pre></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Dataset({
    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],
    num_rows: 41584
})</code></pre></div></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">The dataset contains 41.5K pre-chunked records. Each record is 1-2 paragraphs long and includes additional metadata about the paper from which it comes. Here is an example:</p><div class="my-50 flex flex-col gap-25 rounded-5 border border-[#ddd] p-25 font-mono"><div class="flex flex-col gap-25"><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class="w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>{'doi': '1910.01108',
 'chunk-id': '0',
 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be netuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-specic\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',
 'id': '1910.01108',
 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',
 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\nprevalent in Natural Language Processing (NLP), operating these large models in\non-the-edge and/or under constrained computational training or inference\nbudgets remains challenging. In this work, we propose a method to pre-train a\nsmaller general-purpose language representation model, called DistilBERT, which\ncan then be fine-tuned with good performances on a wide range of tasks like its\nlarger counterparts. While most prior work investigated the use of distillation\nfor building task-specific models, we leverage knowledge distillation during\nthe pre-training phase and show that it is possible to reduce the size of a\nBERT model by 40%, while retaining 97% of its language understanding\ncapabilities and being 60% faster. To leverage the inductive biases learned by\nlarger models during pre-training, we introduce a triple loss combining\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\nfor on-device computations in a proof-of-concept experiment and a comparative\non-device study.',
 'source': 'http://arxiv.org/pdf/1910.01108',
 'authors': ['Victor Sanh',
  'Lysandre Debut',
  'Julien Chaumond',
  'Thomas Wolf'],
 'categories': ['cs.CL'],
 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\n  - NeurIPS 2019',
 'journal_ref': None,
 'primary_category': 'cs.CL',
 'published': '20191002',
 'updated': '20200301',
 'references': [{'id': '1910.01108'}]}</code></pre></div></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We'll be feeding this data into Pinecone, so let's reformat the dataset to be more Pinecone-friendly when it does come to the later embed and index process. The format will contain <code>id</code>, <code>text</code> (which we will embed), and <code>metadata</code>. For this example, we won't use metadata, but it can be helpful to include if we want to do <a class="cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline" href="/learn/vector-search-filtering/">metadata filtering</a> in the future.</p><div class="my-50 flex flex-col gap-25 rounded-5 border border-[#ddd] p-25 font-mono"><div class="flex flex-col gap-25"><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class="w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">data <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">{</span>
    <span class="token string">"id"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>x<span class="token punctuation">[</span><span class="token string">"id"</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">-</span><span class="token interpolation"><span class="token punctuation">{</span>x<span class="token punctuation">[</span><span class="token string">"chunk-id"</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">,</span>
    <span class="token string">"text"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"chunk"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"metadata"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">"title"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"url"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"source"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"primary_category"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"primary_category"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"published"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"published"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"updated"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"updated"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"text"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"chunk"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token comment"># drop uneeded columns</span>
data <span class="token operator">=</span> data<span class="token punctuation">.</span>remove_columns<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token string">"title"</span><span class="token punctuation">,</span> <span class="token string">"summary"</span><span class="token punctuation">,</span> <span class="token string">"source"</span><span class="token punctuation">,</span>
    <span class="token string">"authors"</span><span class="token punctuation">,</span> <span class="token string">"categories"</span><span class="token punctuation">,</span> <span class="token string">"comment"</span><span class="token punctuation">,</span>
    <span class="token string">"journal_ref"</span><span class="token punctuation">,</span> <span class="token string">"primary_category"</span><span class="token punctuation">,</span>
    <span class="token string">"published"</span><span class="token punctuation">,</span> <span class="token string">"updated"</span><span class="token punctuation">,</span> <span class="token string">"references"</span><span class="token punctuation">,</span>
    <span class="token string">"doi"</span><span class="token punctuation">,</span> <span class="token string">"chunk-id"</span><span class="token punctuation">,</span>
    <span class="token string">"chunk"</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Map:   0%|          | 0/41584 [00:00&lt;?, ? examples/s]</code></pre></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>Dataset({
    features: ['id', 'text', 'metadata'],
    num_rows: 41584
})</code></pre></div></div></div></div><h3 class="text-h3-mobile lg:text-h3 text-text-primary my-[40px]" id="Embed-and-Index">Embed and Index</h3><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">To store everything in the vector DB, we need to encode everything with an embedding / bi-encoder model. We will use the open source <code>multilingial-e5-large</code> via Pinecone Inference. We need a [free Pinecone API key](https://app.pinecone.io) to authenticate ourselves via the client:</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">from</span> pinecone<span class="token punctuation">.</span>grpc <span class="token keyword">import</span> PineconeGRPC

<span class="token comment"># get API key from app.pinecone.io</span>
api_key <span class="token operator">=</span> <span class="token string">"PINECONE_API_KEY"</span>

embed_model <span class="token operator">=</span> <span class="token string">"multilingual-e5-large"</span>

<span class="token comment"># configure client</span>
pc <span class="token operator">=</span> PineconeGRPC<span class="token punctuation">(</span>api_key<span class="token operator">=</span>api_key<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Now, we create our vector DB to store our vectors. We set <code>dimension</code> equal to the dimensionality of E5 large (<code>1024</code>) and use a <code>metric</code> compatible with E5  ie <code>cosine</code>.</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">import</span> time

index_name <span class="token operator">=</span> <span class="token string">"rerankers"</span>
existing_indexes <span class="token operator">=</span> <span class="token punctuation">[</span>
    index_info<span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> index_info <span class="token keyword">in</span> pc<span class="token punctuation">.</span>list_indexes<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>

<span class="token comment"># check if index already exists (it shouldn't if this is first time)</span>
<span class="token keyword">if</span> index_name <span class="token keyword">not</span> <span class="token keyword">in</span> existing_indexes<span class="token punctuation">:</span>
    <span class="token comment"># if does not exist, create index</span>
    pc<span class="token punctuation">.</span>create_index<span class="token punctuation">(</span>
        index_name<span class="token punctuation">,</span>
        dimension<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>  <span class="token comment"># dimensionality of e5-large</span>
        metric<span class="token operator">=</span><span class="token string">'cosine'</span><span class="token punctuation">,</span>
        spec<span class="token operator">=</span>spec
    <span class="token punctuation">)</span>
    <span class="token comment"># wait for index to be initialized</span>
    <span class="token keyword">while</span> <span class="token keyword">not</span> pc<span class="token punctuation">.</span>describe_index<span class="token punctuation">(</span>index_name<span class="token punctuation">)</span><span class="token punctuation">.</span>status<span class="token punctuation">[</span><span class="token string">'ready'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># connect to index</span>
index <span class="token operator">=</span> pc<span class="token punctuation">.</span>Index<span class="token punctuation">(</span>index_name<span class="token punctuation">)</span>
time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># view index stats</span>
index<span class="token punctuation">.</span>describe_index_stats<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We create a new function, `embed`, to handle embedding with our model. Within the function, we also include the handling of rate limit errors.</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">from</span> pinecone_plugins<span class="token punctuation">.</span>inference<span class="token punctuation">.</span>core<span class="token punctuation">.</span>client<span class="token punctuation">.</span>exceptions <span class="token keyword">import</span> PineconeApiException

<span class="token keyword">def</span> <span class="token function">embed</span><span class="token punctuation">(</span>batch<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token comment"># create embeddings (exponential backoff to avoid RateLimitError)</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># max 5 retries</span>
        <span class="token keyword">try</span><span class="token punctuation">:</span>
            res <span class="token operator">=</span> pc<span class="token punctuation">.</span>inference<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>
                model<span class="token operator">=</span>embed_model<span class="token punctuation">,</span>
                inputs<span class="token operator">=</span>batch<span class="token punctuation">,</span>
                parameters<span class="token operator">=</span><span class="token punctuation">{</span>
                    <span class="token string">"input_type"</span><span class="token punctuation">:</span> <span class="token string">"passage"</span><span class="token punctuation">,</span>  <span class="token comment"># for docs/context/chunks</span>
                    <span class="token string">"truncate"</span><span class="token punctuation">:</span> <span class="token string">"END"</span><span class="token punctuation">,</span>  <span class="token comment"># truncate to max length</span>
                <span class="token punctuation">}</span>
            <span class="token punctuation">)</span>
            passed <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">except</span> PineconeApiException<span class="token punctuation">:</span>
            time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">**</span>j<span class="token punctuation">)</span>  <span class="token comment"># wait 2^j seconds before retrying</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Retrying..."</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> passed<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> RuntimeError<span class="token punctuation">(</span><span class="token string">"Failed to create embeddings."</span><span class="token punctuation">)</span>
    <span class="token comment"># get embeddings</span>
    embeds <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token string">"values"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> res<span class="token punctuation">.</span>data<span class="token punctuation">]</span>
    <span class="token keyword">return</span> embeds<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We're now ready to begin populating the index using the E5 embedding model like so:</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>auto <span class="token keyword">import</span> tqdm

batch_size <span class="token operator">=</span> <span class="token number">96</span>  <span class="token comment"># how many embeddings we create and insert at once</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    passed <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token comment"># find end of batch</span>
    i_end <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> i<span class="token operator">+</span>batch_size<span class="token punctuation">)</span>
    <span class="token comment"># create batch</span>
    batch <span class="token operator">=</span> data<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i_end<span class="token punctuation">]</span>
    embeds <span class="token operator">=</span> embed<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    to_upsert <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"id"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embeds<span class="token punctuation">,</span> batch<span class="token punctuation">[</span><span class="token string">"metadata"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># upsert to Pinecone</span>
    index<span class="token punctuation">.</span>upsert<span class="token punctuation">(</span>vectors<span class="token operator">=</span>to_upsert<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Our index is now populated and ready for us to query!</p><h3 class="text-h3-mobile lg:text-h3 text-text-primary my-[40px]" id="Retrieval-Without-Reranking">Retrieval Without Reranking</h3><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Before reranking, let's see how our results look <em>without</em> it. We will define a function called <code>get_docs</code> to return documents using the first stage of retrieval only:</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_docs</span><span class="token punctuation">(</span>query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> top_k<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token comment"># encode query</span>
    res <span class="token operator">=</span> pc<span class="token punctuation">.</span>inference<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>
        model<span class="token operator">=</span>embed_model<span class="token punctuation">,</span>
        inputs<span class="token operator">=</span><span class="token punctuation">[</span>query<span class="token punctuation">]</span><span class="token punctuation">,</span>
        parameters<span class="token operator">=</span><span class="token punctuation">{</span>
            <span class="token string">"input_type"</span><span class="token punctuation">:</span> <span class="token string">"query"</span><span class="token punctuation">,</span>  <span class="token comment"># for queries</span>
            <span class="token string">"truncate"</span><span class="token punctuation">:</span> <span class="token string">"END"</span><span class="token punctuation">,</span>  <span class="token comment"># truncate to max length</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">)</span>
    xq <span class="token operator">=</span> res<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"values"</span><span class="token punctuation">]</span>
    <span class="token comment"># search pinecone index</span>
    res <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>vector<span class="token operator">=</span>xq<span class="token punctuation">,</span> top_k<span class="token operator">=</span>top_k<span class="token punctuation">,</span> include_metadata<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># get doc text</span>
    docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span>
        <span class="token string">"id"</span><span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"text"</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">"metadata"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> x <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>res<span class="token punctuation">[</span><span class="token string">"matches"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> docs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Let's ask about <strong>R</strong>einforcement <strong>L</strong>earning with <strong>H</strong>uman <strong>F</strong>eedback  a popular fine-tuning method behind the sudden performance gains demonstrated by ChatGPT when it was released.</p><div class="my-50 flex flex-col gap-25 rounded-5 border border-[#ddd] p-25 font-mono"><div class="flex flex-col gap-25"><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class="w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">query <span class="token operator">=</span> <span class="token string">"can you explain why we would want to do rlhf?"</span>
docs <span class="token operator">=</span> get_docs<span class="token punctuation">(</span>query<span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n---\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>docs<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># print the first 3 docs</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>whichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease
the likelihood that their nal answer is correct.
RLHF has emerged as a powerful strategy for ne-tuning Large Language Models, enabling signicant
improvements in their performance (Christiano et al., 2017). The method, rst showcased by Stiennon et al.
(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.
In this paradigm, models are ne-tuned based on feedback from human users, thus iteratively aligning the
models responses more closely with human expectations and preferences.
Ouyang et al. (2022) demonstrates that a combination of instruction ne-tuning and RLHF can help x
issues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai
et al. (2022b) partially automates this ne-tuning-plus-RLHF approach by replacing the human-labeled
ne-tuningdatawiththemodelsownself-critiquesandrevisions,andbyreplacinghumanraterswitha
---
We examine the inuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an
increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of
these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,
previous work shows that the amount of RLHF training can signicantly change metrics on a wide range of
personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important
to control for the amount of RLHF training in the analysis of our experiments.
3.2 Experiments
3.2.1 Overview
We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping
and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often
harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ
[40] (3.2.2) and Windogender [49] (3.2.3). For discrimination, we focus on whether models make disparate
decisions about individuals based on protected characteristics that should have no relevance to the outcome.5
To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course
---
model to estimate the eventual performance of a larger RL policy. The slopes of these lines also
explain how RLHF training can produce such large effective gains in model size, and for example it
explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.
 One can ask a subtle, perhaps ill-dened question about RLHF training  is it teaching the model
new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We
might attempt to make this distinction sharp by associating the latter class of behaviors with the
region where RL reward remains linear inp
KL.
 To make some bolder guesses  perhaps the linear relation actually provides an upper bound on RL
reward, as a function of the KL. One might also attempt to extend the relation further by replacingp
KLwith a geodesic length in the Fisher geometry.
By making RL learning more predictable and by identifying new quantitative categories of behavior, we
might hope to detect unexpected behaviors emerging during RL training.
4.4 Tension Between Helpfulness and Harmlessness in RLHF Training
Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we
found that many RLHF policies were very frequently reproducing the same exaggerated responses to all
remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they
...
</code></pre></div></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We get reasonable performance here  notably relevant chunks of text:</p><div class="w-full overflow-x-auto my-6"><table class="w-full text-text-primary table-auto border-collapse text-sm md:text-base
        [&amp;_td]:border [&amp;_td]:border-border [&amp;_td]:p-3 md:[&amp;_td]:p-4 [&amp;_td]:align-top
        [&amp;>tbody>tr>td:first-of-type]:text-left
        [&amp;_th]:border [&amp;_th]:border-border [&amp;_th]:p-3 md:[&amp;_th]:p-4 [&amp;_th]:align-top [&amp;_th]:font-medium
        [&amp;_th:first-of-type]:text-left undefined"><thead><tr><th class="text-center"><span>Document</span></th><th class="text-center"><span>Chunk</span></th></tr></thead><tbody><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>0</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"enabling significant improvements in their performance"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>0</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"iteratively aligning the models' responses more closely with human expectations and preferences"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>0</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>1</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"increasingly popular technique for reducing harmful behaviors in large language models"</span></td></tr></tbody></table></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">The remaining documents and text cover RLHF but don't answer our specific question of <em>"why we would want to do rlhf?"</em>.</p><h3 class="text-h3-mobile lg:text-h3 text-text-primary my-[40px]" id="Reranking-Responses">Reranking Responses</h3><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We will use Pinecone's rerank endpoint for this. We use the same Pinecone client but now hit <code>inference.rerank</code> like so:</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">rerank_name <span class="token operator">=</span> <span class="token string">"bge-reranker-v2-m3"</span>

rerank_docs <span class="token operator">=</span> pc<span class="token punctuation">.</span>inference<span class="token punctuation">.</span>rerank<span class="token punctuation">(</span>
    model<span class="token operator">=</span>rerank_name<span class="token punctuation">,</span>
    query<span class="token operator">=</span>query<span class="token punctuation">,</span>
    documents<span class="token operator">=</span>docs<span class="token punctuation">,</span>
    top_n<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span>
    return_documents<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">This returns a <code>RerankResult</code> object:</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">RerankResult<span class="token punctuation">(</span>
  model<span class="token operator">=</span><span class="token string">'bge-reranker-v2-m3'</span><span class="token punctuation">,</span>
  data<span class="token operator">=</span><span class="token punctuation">[</span>
    <span class="token punctuation">{</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">0.9071478</span><span class="token punctuation">,</span>
      document<span class="token operator">=</span><span class="token punctuation">{</span><span class="token builtin">id</span><span class="token operator">=</span><span class="token string">"1"</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token string">"RLHF Response ! I..."</span><span class="token punctuation">}</span> <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span> index<span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">0.6954414</span><span class="token punctuation">,</span>
      document<span class="token operator">=</span><span class="token punctuation">{</span><span class="token builtin">id</span><span class="token operator">=</span><span class="token string">"9"</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token string">"team, instead of ..."</span><span class="token punctuation">}</span> <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">(</span><span class="token number">21</span> more documents<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span> index<span class="token operator">=</span><span class="token number">17</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">0.13420755</span><span class="token punctuation">,</span>
      document<span class="token operator">=</span><span class="token punctuation">{</span><span class="token builtin">id</span><span class="token operator">=</span><span class="token string">"17"</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token string">"helpfulness and h..."</span><span class="token punctuation">}</span> <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span> index<span class="token operator">=</span><span class="token number">23</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">0.11417085</span><span class="token punctuation">,</span>
      document<span class="token operator">=</span><span class="token punctuation">{</span><span class="token builtin">id</span><span class="token operator">=</span><span class="token string">"23"</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token string">"responses respons..."</span><span class="token punctuation">}</span> <span class="token punctuation">}</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  usage<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'rerank_units'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We access the text content of the docs via <code>rerank_docs.data[0]["document"]["text"]</code>.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Let's create a function that will allow us to quickly compare original vs. reranked results.</p><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compare</span><span class="token punctuation">(</span>query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> top_k<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> top_n<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># first get vec search results</span>
    top_k_docs <span class="token operator">=</span> get_docs<span class="token punctuation">(</span>query<span class="token punctuation">,</span> top_k<span class="token operator">=</span>top_k<span class="token punctuation">)</span>
    <span class="token comment"># rerank</span>
    top_n_docs <span class="token operator">=</span> pc<span class="token punctuation">.</span>inference<span class="token punctuation">.</span>rerank<span class="token punctuation">(</span>
        model<span class="token operator">=</span>rerank_name<span class="token punctuation">,</span>
        query<span class="token operator">=</span>query<span class="token punctuation">,</span>
        documents<span class="token operator">=</span>docs<span class="token punctuation">,</span>
        top_n<span class="token operator">=</span>top_n<span class="token punctuation">,</span>
        return_documents<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span>
    original_docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    reranked_docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token comment"># compare order change</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[ORIGINAL] -&gt; [NEW]"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> doc <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>top_n_docs<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>doc<span class="token punctuation">.</span>index<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"\t-&gt;\t"</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> i <span class="token operator">!=</span> doc<span class="token punctuation">.</span>index<span class="token punctuation">:</span>
            reranked_docs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"[</span><span class="token interpolation"><span class="token punctuation">{</span>doc<span class="token punctuation">.</span>index<span class="token punctuation">}</span></span><span class="token string">]\n"</span></span><span class="token operator">+</span>doc<span class="token punctuation">[</span><span class="token string">"document"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            original_docs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"[</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">]\n"</span></span><span class="token operator">+</span>top_k_docs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            reranked_docs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>doc<span class="token punctuation">[</span><span class="token string">"document"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            original_docs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">)</span>
    <span class="token comment"># print results</span>
    <span class="token keyword">for</span> orig<span class="token punctuation">,</span> rerank <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>original_docs<span class="token punctuation">,</span> reranked_docs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> orig<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"SAME:\n</span><span class="token interpolation"><span class="token punctuation">{</span>rerank<span class="token punctuation">}</span></span><span class="token string">\n\n---\n"</span></span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"ORIGINAL:\n</span><span class="token interpolation"><span class="token punctuation">{</span>orig<span class="token punctuation">}</span></span><span class="token string">\n\nRERANKED:\n</span><span class="token interpolation"><span class="token punctuation">{</span>rerank<span class="token punctuation">}</span></span><span class="token string">\n\n---\n"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We start with our RLHF query. This time, we do a more standard retrieval-rerank process of retrieving 25 documents (<code>top_k=25</code>) and reranking to the top three documents (<code>top_n=3</code>).</p><div class="my-50 flex flex-col gap-25 rounded-5 border border-[#ddd] p-25 font-mono"><div class="flex flex-col gap-25"><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class="w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="my-50"><div class="codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]"><pre class="relative overflow-auto language-python line-numbers" tabindex="0"><code class="language-python">compare<span class="token punctuation">(</span>query<span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="z-1 absolute right-3 top-3"><button aria-label="Copy code to clipboard" class="group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40 "><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-text-secondary group-hover:fill-text-primary opacity-40" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div></div></div><div class="flex flex-col gap-3 md:flex-row [&amp;>div]:my-0"><div class=" w-20 shrink-0 text-[0.9375rem] text-[#7e7e7e]"></div><div class="grid"><pre class="overflow-auto pb-25 pr-25"><code>0	-&gt;	0
1	-&gt;	23
2	-&gt;	14
ORIGINAL:
[1]
We examine the inuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an
increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of
these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,
previous work shows that the amount of RLHF training can signicantly change metrics on a wide range of
personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important
to control for the amount of RLHF training in the analysis of our experiments.
3.2 Experiments
3.2.1 Overview
We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping
and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often
harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ
[40] (3.2.2) and Windogender [49] (3.2.3). For discrimination, we focus on whether models make disparate
decisions about individuals based on protected characteristics that should have no relevance to the outcome.5
To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course

RERANKED:
[23]
We have shown that its possible to use reinforcement learning from human feedback to train language models
that act as helpful and harmless assistants. Our RLHF training also improves honesty, though we expect
other techniques can do better still. As in other recent works associated with aligning large language models
[Stiennon et al., 2020, Thoppilan et al., 2022, Ouyang et al., 2022, Nakano et al., 2021, Menick et al., 2022],
RLHF improves helpfulness and harmlessness by a huge margin when compared to simply scaling models
up.
Our alignment interventions actually enhance the capabilities of large models, and can easily be combined
with training for specialized skills (such as coding or summarization) without any degradation in alignment
or performance. Models with less than about 10B parameters behave differently, paying an alignment tax on
their capabilities. This provides an example where models near the state-of-the-art may have been necessary
to derive the right lessons from alignment research.
The overall picture we seem to nd  that large models can learn a wide variety of skills, including alignment, in a mutually compatible way  does not seem very surprising. Behaving in an aligned fashion is just
another capability, and many works have shown that larger models are more capable [Kaplan et al., 2020,

---

ORIGINAL:
[2]
model to estimate the eventual performance of a larger RL policy. The slopes of these lines also
explain how RLHF training can produce such large effective gains in model size, and for example it
explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.
 One can ask a subtle, perhaps ill-dened question about RLHF training  is it teaching the model
new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We
might attempt to make this distinction sharp by associating the latter class of behaviors with the
region where RL reward remains linear inp
KL.
 To make some bolder guesses  perhaps the linear relation actually provides an upper bound on RL
reward, as a function of the KL. One might also attempt to extend the relation further by replacingp
KLwith a geodesic length in the Fisher geometry.
By making RL learning more predictable and by identifying new quantitative categories of behavior, we
might hope to detect unexpected behaviors emerging during RL training.
4.4 Tension Between Helpfulness and Harmlessness in RLHF Training
Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we
found that many RLHF policies were very frequently reproducing the same exaggerated responses to all
remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they

RERANKED:
[14]
the model outputs safe responses, they are often more detailed than what the average annotator writes.
Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to
teachthemodelhowtowritemorenuancedresponses. ComprehensivetuningwithRLHFhastheadded
benet that it may make the model more robust to jailbreak attempts (Bai et al., 2022a).
WeconductRLHFbyrstcollectinghumanpreferencedataforsafetysimilartoSection3.2.2: annotators
writeapromptthattheybelievecanelicitunsafebehavior,andthencomparemultiplemodelresponsesto
theprompts,selectingtheresponsethatissafestaccordingtoasetofguidelines. Wethenusethehuman
preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to
sample from the model during the RLHF stage.
BetterLong-TailSafetyRobustnesswithoutHurtingHelpfulness Safetyisinherentlyalong-tailproblem,
wherethe challengecomesfrom asmallnumber ofveryspecic cases. Weinvestigatetheimpact ofSafety

---

</code></pre></div></div></div></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Looking at these, we have dropped the one relevant chunk of text from document <code>1</code> and <em>no relevant</em> chunks of text from document <code>2</code>  the following relevant pieces of information now replace these:</p><div class="w-full overflow-x-auto my-6"><table class="w-full text-text-primary table-auto border-collapse text-sm md:text-base
        [&amp;_td]:border [&amp;_td]:border-border [&amp;_td]:p-3 md:[&amp;_td]:p-4 [&amp;_td]:align-top
        [&amp;>tbody>tr>td:first-of-type]:text-left
        [&amp;_th]:border [&amp;_th]:border-border [&amp;_th]:p-3 md:[&amp;_th]:p-4 [&amp;_th]:align-top [&amp;_th]:font-medium
        [&amp;_th:first-of-type]:text-left undefined"><thead><tr><th class="text-center"><span>Original Position</span></th><th class="text-center"><span>Rerank Position</span></th><th class="text-center"><span>Chunk</span></th></tr></thead><tbody><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>23</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>1</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"train language models that act as helpful and harmless assistants"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>23</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>1</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"RLHF training also improves honesty"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>23</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>1</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"RLHF improves helpfulness and harmlessness by a huge margin"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>23</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>1</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"enhance the capabilities of large models"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>14</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>2</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"the model outputs safe responses"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>14</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>2</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"often more detailed than what the average annotator writes"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>14</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>2</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"RLHF to reach the model how to write more nuanced responses"</span></td></tr><tr><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>14</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>2</span></td><td class="text-center [&amp;>span]:space-y-2 [&amp;_li>span]:mt-0 [&amp;_ul]:mt-0"><span>"make the model more robust to jailbreak attempts"</span></td></tr></tbody></table></div><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">After reranking, we have&nbsp;<em>far more</em>&nbsp;relevant information. Naturally, this can result in significantly better performance for RAG. It means we maximize relevant information while minimizing noise input into our LLM.</p><hr class="my-8 border-solid"><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">Reranking is one of the simplest methods for dramatically improving recall performance in <strong>R</strong>etrieval <strong>A</strong>ugmented <strong>G</strong>eneration (RAG) or any other retrieval-based pipeline.</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">We've explored why <a class="cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline" href="/learn/refine-with-rerank/">rerankers can provide so much better performance</a> than their embedding model counterparts  and how a two-stage retrieval system allows us to get the best of both, enabling search at scale while maintaining quality performance.</p><hr class="my-8 border-solid"><h2 class="text-h2-mobile lg:text-h2 text-text-primary my-[40px]" id="References">References</h2><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">[1] <a href="https://www.anthropic.com/index/100k-context-windows" class="cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline">Introducing 100K Context Windows</a> (2023), Anthropic</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">[2] N. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, P. Liang, <a href="https://arxiv.org/abs/2307.03172" class="cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline">Lost in the Middle: How Language Models Use Long Contexts</a> (2023),</p><p class="mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary">[3] N. Reimers, I. Gurevych, <a href="https://arxiv.org/pdf/1908.10084.pdf" class="cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (2019), UKP-TUDA</p><div class="lg:hidden pt-8"><div class="text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]">Share: <div class="flex items-start"><a href="https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/series/rag/rerankers" target="_blank" aria-label="Share to Twitter" class="flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent"><svg width="14" height="12" viewBox="0 0 14 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z" class="fill-text-secondary group-hover:fill-text-primary transition-colors"></path></svg></a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/series/rag/rerankers" target="_blank" aria-label="Share to LinkedIn" class="flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent"><svg width="13" height="13" viewBox="0 0 13 13" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z" class="fill-text-secondary group-hover:fill-text-primary transition-colors"></path></svg></a><a href="https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/series/rag/rerankers" target="_blank" aria-label="Share to Hacker News" class="flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined"><svg width="13" height="13" viewBox="0 0 13 13" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z" class="fill-text-secondary group-hover:fill-text-primary transition-colors"></path></svg></a></div></div></div><div class="mt-10"><div class="inline-block border px-4 py-3"><p class="text-text-secondary text-sm">Was this article helpful?</p><div class="mt-2 flex gap-4"><button class="text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-thumbs-up h-4 w-4 "><path d="M7 10v12"></path><path d="M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2h0a3.13 3.13 0 0 1 3 3.88Z"></path></svg>Yes</button><button class="text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-thumbs-down h-4 w-4 "><path d="M17 14V2"></path><path d="M9 18.12 10 14H4.17a2 2 0 0 1-1.92-2.56l2.33-8A2 2 0 0 1 6.5 2H20a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2h-2.76a2 2 0 0 0-1.79 1.11L12 22h0a3.13 3.13 0 0 1-3-3.88Z"></path></svg>No</button></div></div></div></div></div></div></article></div><aside class="relative w-full lg:order-1 lg:w-[12.625rem] lg:shrink-0"><div class="lg:sticky lg:top-32"><div class="scrollbar-hidden relative lg:max-h-screen lg:overflow-y-auto lg:py-25 lg:pb-36"><img alt="Learn how to build advanced retrieval augmented generation (RAG) pipelines. (series cover image)" fetchpriority="high" width="248" height="320" decoding="async" data-nimg="1" class="w-full max-w-[12.625rem]" sizes="202px" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=16&amp;q=100 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=32&amp;q=100 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=48&amp;q=100 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=64&amp;q=100 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=96&amp;q=100 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=128&amp;q=100 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=256&amp;q=100 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=384&amp;q=100 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=640&amp;q=100 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=750&amp;q=100 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=828&amp;q=100 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=1080&amp;q=100 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=1200&amp;q=100 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=1920&amp;q=100 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=2048&amp;q=100 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=3840&amp;q=100 3840w" src="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=3840&amp;q=100" style=""><a class="mt-3 block text-body font-semibold text-text-primary hover:underline" href="/learn/series/rag/">Retrieval Augmented Generation</a><div class="mt-25 text-small"><p class="font-semibold text-text-secondary">Chapters</p><ol class="mt-3 flex list-outside list-decimal flex-col gap-3 pl-7 !text-text-secondary"><li class="group"><a class="text-text-secondary hover:text-text-primary decoration-none hover:underline transition-all duration-200" href="/learn/series/rag/rerankers/">Rerankers for RAG</a><div class="hidden pl-4 lg:block"><div class="text-small text-[#71717A]"><ul class="mt-4 flex flex-col gap-3 underline !decoration-none text-text-secondary group-hover:text-text-primary transition-all duration-200 text-sm"><li><a href="#Recall-vs.-Context-Windows" class="hover:underline hover:opacity-50 cursor-pointer">Recall vs. Context Windows</a></li><li><a href="#Power-of-Rerankers" class="hover:underline hover:opacity-50 cursor-pointer">Power of Rerankers</a></li><li><a href="#Implementing-Two-Stage-Retrieval-with-Reranking" class="hover:underline hover:opacity-50 cursor-pointer">Implementing Two-Stage Retrieval with Reranking</a></li><li><a href="#References" class="hover:underline hover:opacity-50 cursor-pointer">References</a></li></ul></div></div></li><li class="group"><a class="text-text-secondary hover:text-text-primary decoration-none hover:underline transition-all duration-200" href="/learn/series/rag/embedding-models-rundown/">Embedding Models</a></li><li class="group"><a class="text-text-secondary hover:text-text-primary decoration-none hover:underline transition-all duration-200" href="/learn/series/rag/ragas/">Agent Evaluation</a></li></ol></div></div></div></aside></div></div><div role="presentation" aria-hidden="true" class="pointer-events-none absolute left-0 top-0 w-full"><span class="z-1 absolute border-l top-0 left-0 h-[7.125rem]"></span><span class="bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]"></span><span class="z-1 absolute border-l top-0 right-0 h-[7.125rem]"></span><span class="bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]"></span></div><div role="presentation" aria-hidden="true" class="pointer-events-none absolute bottom-0 left-0 w-full"><span class="z-1 absolute border-l bottom-0 left-0 h-[7.125rem]"></span><span class="bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]"></span><span class="z-1 absolute border-l bottom-0 right-0 h-[7.125rem]"></span><span class="bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]"></span></div></div></div><div class="container"><div class="relative px-4 sm:px-8 !px-0 !mx-0"><div class="flex flex-col items-center justify-center font-semibold md:flex-row md:items-start w-full border-border border divide-x divide-border"><div class="w-full"></div><div class="w-full"></div></div></div></div></article></main><footer class="border-t border-border"><div class="container"><div class="relative px-4 sm:px-8 border-x pt-24"><div class="xs:flex-row flex flex-col justify-between gap-10"><div class="w-32 shrink-0 "><svg enable-background="new 0 0 1077 220" class="w-full -translate-y-[4px]" viewBox="0 0 1077 220" xmlns="http://www.w3.org/2000/svg"><g fill="var(--text-primary)"><path d="m246.4 51.4h55.2c39.9 0 50.1 23.5 50.1 42.6s-10.3 42.6-50.1 42.6h-34.1v67.2h-21.1zm21.2 67.2h27.9c16.8 0 33.5-3.8 33.5-24.5s-16.8-24.5-33.5-24.5h-27.9z"></path><path d="m379.4 50.7c8 0 14.5 6.3 14.6 14 .1 7.8-6.2 14.2-14.2 14.4s-14.6-5.9-14.9-13.7c-.1-3.9 1.4-7.6 4.1-10.4 2.6-2.7 6.4-4.3 10.4-4.3zm-9.8 51.1h19.6v102.1h-19.6z"></path><path d="m412 101.8h19.9v15.8h.5c6.9-12 20.3-19.2 34.4-18.3 20.3 0 37.8 11.9 37.8 39v65.7h-19.6v-60.2c0-19.2-11.3-26.3-23.9-26.3-16.5 0-29.1 10.3-29.1 34v52.5h-20z"></path><path d="m540.9 160c0 17.8 17 29.5 35.3 29.5 11.5-.3 22.2-6 28.7-15.2l15.1 11.2c-11 14.1-28.4 22-46.5 21.1-33.2 0-53.8-23.2-53.8-53.6-.6-14.2 4.9-28 15.1-38.1s24.2-15.7 38.8-15.5c36.9 0 51 27.5 51 53.8v6.9h-83.7zm62.8-15.5c-.5-17-10.2-29.5-30.2-29.5-17.1-.1-31.4 12.8-32.5 29.5z"></path><path d="m714.6 129c-6.5-7.5-16-11.7-26.1-11.4-21.6 0-32.7 17-32.7 36.2-.5 9.1 2.8 17.9 9.2 24.5s15.3 10.3 24.6 10.2c9.8.3 19.2-3.9 25.4-11.4l14.1 13.6c-10.3 10.6-24.8 16.3-39.7 15.7-14.7.7-29-4.6-39.4-14.8-10.4-10.1-16-24.1-15.3-38.4-.7-14.4 4.9-28.4 15.3-38.6s24.7-15.7 39.5-15.2c15.1-.4 29.7 5.5 40.2 16.1z"></path><path d="m787.9 99.2c30.2.5 54.4 24.7 54.1 54.2s-25 53.2-55.3 53c-30.2-.2-54.7-24.1-54.7-53.6 0-14.4 5.9-28.1 16.4-38.2 10.6-10.1 24.8-15.6 39.5-15.4zm0 89.2c21.1 0 34.4-14.7 34.4-35.6 0-20.8-13.3-35.5-34.4-35.5s-34.5 14.7-34.5 35.5 13.3 35.6 34.5 35.6z"></path><path d="m859.7 101.8h19.9v15.8c6.9-12.1 20.4-19.2 34.5-18.3 20.3 0 37.8 11.9 37.8 39v65.7h-19.9v-60.2c0-19.2-11.3-26.3-23.8-26.3-16.6 0-29.1 10.3-29.1 34v52.5h-19.3v-102.2z"></path><path d="m988.8 160c0 17.8 17 29.5 35.3 29.5 11.5-.4 22.2-6 28.7-15.2l15.1 11.2c-11 14-28.3 21.8-46.4 20.8-33.1 0-53.8-23.2-53.8-53.6-.6-14.2 4.9-28.1 15.1-38.2 10.2-10.2 24.3-15.7 38.9-15.4 36.9 0 51 27.5 51 53.8v6.9zm62.7-15.6c-.5-17-10.1-29.5-30.2-29.5-17.1-.1-31.4 12.8-32.5 29.5z"></path><path clip-rule="evenodd" d="m127 6.4c-2.1-2.5-5.6-3.1-8.4-1.5l-2.6 1.4-28.3 16.1 6.6 11.6 18.4-10.5-4.5 24.6 13.1 2.4 4.6-24.7 13.6 16.2 10.2-8.6-20.6-24.6h-.1zm-39.7 207.5c6.8 0 12.3-5.4 12.3-12s-5.5-12-12.3-12-12.3 5.4-12.3 12c-.1 6.6 5.5 12 12.3 12zm16.5-65.9-4.4 24.7-13.2-2.4 4.4-24.6-18.4 10.6-6.7-11.6 28.1-16.1 2.6-1.5c2.8-1.6 6.3-1 8.4 1.5l2 2.4 20.9 24.5-10.2 8.7zm10.7-59-4.4 24.7-13.2-2.4 4.4-24.5-18.3 10.5-6.6-11.6 28-16v-.2h.2l2.6-1.5c2.8-1.6 6.3-1 8.4 1.5l2 2.3 20.8 24.6-10.2 8.7zm-86.3 97.6h-.1l-2.7-.8c-2.9-.8-4.8-3.6-4.6-6.6l2.4-33.4 12.7.9-1.5 20.3 19.7-13.4 7.1 10.5-19.3 13.1 19.7 5.7-3.5 12.2zm130.7 13.8-.9 2.9c-.9 2.8-3.5 4.7-6.5 4.5l-2.8-.2-.2.1-.1-.1-31-2.1.8-12.7 20.6 1.4-13.5-18.9 10.3-7.4 13.8 19.4 6-19.6 12.1 3.7zm36.4-68.8 1.5 2.7c1.5 2.7.9 6.1-1.5 8.1l-2.2 1.9v.1h-.1l-24.1 20.4-8.4-9.9 15.8-13.4-23.7-4.2 2.3-12.8 23.9 4.2-10-18 11.3-6.3zm-24.5-55.8-21.4 11.5-6.2-11.4 21.1-11.3-19.3-7.9 4.9-12 29.4 11.9.1-.1.1.2 2.7 1.1c2.9 1.2 4.5 4.2 4 7.2l-.5 3-5.5 30.5-12.8-2.3zm-143.6 26.8 23.8 4-2.2 12.8-24-4.1 10.2 18-11.3 6.4-15.4-27.1-1.5-2.6c-1.5-2.7-.9-6.1 1.4-8.1l2.2-1.9v-.1h.1l23.8-20.5 8.5 9.9zm35.9-55.4 15.8 17.6-9.7 8.7-16.2-18-3.7 20.5-12.8-2.3 5.6-30.4.6-3.1c.5-3 3.1-5.2 6.1-5.3l2.8-.1.1-.1.1.1 31.8-1.3.5 13z" fill-rule="evenodd"></path></g></svg></div><div class="flex"><a href="https://x.com/pinecone" aria-label="Twitter" class="hover:border-text-primary last:border-r-border group flex h-8 w-8 items-center justify-center border border-r-transparent transition-colors undefined"><svg width="13" height="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z" class="fill-secondary dark:fill-alpha4"></path></svg></a><a href="https://www.linkedin.com/company/pinecone-io" aria-label="LinkedIn" class="hover:border-text-primary last:border-r-border group flex h-8 w-8 items-center justify-center border border-r-transparent transition-colors undefined"><svg width="13" height="13" viewBox="0 0 13 13" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z" class="fill-secondary dark:fill-alpha4"></path></svg></a><a href="https://www.youtube.com/@pinecone-io" aria-label="YouTube" class="hover:border-text-primary last:border-r-border group flex h-8 w-8 items-center justify-center border border-r-transparent transition-colors undefined"><svg width="16" height="11" viewBox="0 0 16 11" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M15.0117 1.66797C15.3398 2.81641 15.3398 5.27734 15.3398 5.27734C15.3398 5.27734 15.3398 7.71094 15.0117 8.88672C14.8477 9.54297 14.3281 10.0352 13.6992 10.1992C12.5234 10.5 7.875 10.5 7.875 10.5C7.875 10.5 3.19922 10.5 2.02344 10.1992C1.39453 10.0352 0.875 9.54297 0.710938 8.88672C0.382812 7.71094 0.382812 5.27734 0.382812 5.27734C0.382812 5.27734 0.382812 2.81641 0.710938 1.66797C0.875 1.01172 1.39453 0.492188 2.02344 0.328125C3.19922 0 7.875 0 7.875 0C7.875 0 12.5234 0 13.6992 0.328125C14.3281 0.492188 14.8477 1.01172 15.0117 1.66797ZM6.34375 7.49219L10.2266 5.27734L6.34375 3.0625V7.49219Z" class="fill-secondary dark:fill-alpha4"></path></svg></a><a href="https://github.com/pinecone-io" aria-label="GitHub" class="hover:border-text-primary last:border-r-border group flex h-8 w-8 items-center justify-center border border-r-transparent transition-colors undefined"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4.51172 11.1328C4.51172 11.0781 4.45703 11.0234 4.375 11.0234C4.29297 11.0234 4.23828 11.0781 4.23828 11.1328C4.23828 11.1875 4.29297 11.2422 4.375 11.2148C4.45703 11.2148 4.51172 11.1875 4.51172 11.1328ZM3.66406 10.9961C3.69141 10.9414 3.77344 10.9141 3.85547 10.9414C3.9375 10.9688 3.96484 11.0234 3.96484 11.0781C3.9375 11.1328 3.85547 11.1602 3.80078 11.1328C3.71875 11.1328 3.66406 11.0508 3.66406 10.9961ZM4.89453 10.9688C4.94922 10.9414 5.03125 10.9961 5.03125 11.0508C5.05859 11.1055 5.00391 11.1328 4.92188 11.1602C4.83984 11.1875 4.75781 11.1602 4.75781 11.1055C4.75781 11.0234 4.8125 10.9688 4.89453 10.9688ZM6.67188 0.46875C10.4727 0.46875 13.5625 3.36719 13.5625 7.14062C13.5625 10.1758 11.7031 12.7734 8.96875 13.6758C8.61328 13.7578 8.47656 13.5391 8.47656 13.3477C8.47656 13.1289 8.50391 11.9805 8.50391 11.0781C8.50391 10.4219 8.28516 10.0117 8.03906 9.79297C9.57031 9.62891 11.1836 9.41016 11.1836 6.78516C11.1836 6.01953 10.9102 5.66406 10.4727 5.17188C10.5273 4.98047 10.7734 4.26953 10.3906 3.3125C9.81641 3.12109 8.50391 4.05078 8.50391 4.05078C7.95703 3.88672 7.38281 3.83203 6.78125 3.83203C6.20703 3.83203 5.63281 3.88672 5.08594 4.05078C5.08594 4.05078 3.74609 3.14844 3.19922 3.3125C2.81641 4.26953 3.03516 4.98047 3.11719 5.17188C2.67969 5.66406 2.46094 6.01953 2.46094 6.78516C2.46094 9.41016 4.01953 9.62891 5.55078 9.79297C5.33203 9.98438 5.16797 10.2852 5.11328 10.7227C4.70312 10.9141 3.71875 11.2148 3.11719 10.1484C2.73438 9.49219 2.05078 9.4375 2.05078 9.4375C1.39453 9.4375 2.02344 9.875 2.02344 9.875C2.46094 10.0664 2.76172 10.8594 2.76172 10.8594C3.17188 12.0898 5.08594 11.6797 5.08594 11.6797C5.08594 12.2539 5.08594 13.1836 5.08594 13.375C5.08594 13.5391 4.97656 13.7578 4.62109 13.7031C1.88672 12.7734 0 10.1758 0 7.14062C0 3.36719 2.89844 0.46875 6.67188 0.46875ZM2.65234 9.90234C2.67969 9.875 2.73438 9.90234 2.78906 9.92969C2.84375 9.98438 2.84375 10.0664 2.81641 10.0938C2.76172 10.1211 2.70703 10.0938 2.65234 10.0664C2.625 10.0117 2.59766 9.92969 2.65234 9.90234ZM2.35156 9.68359C2.37891 9.65625 2.40625 9.65625 2.46094 9.68359C2.51562 9.71094 2.54297 9.73828 2.54297 9.76562C2.51562 9.82031 2.46094 9.82031 2.40625 9.79297C2.35156 9.76562 2.32422 9.73828 2.35156 9.68359ZM3.22656 10.668C3.28125 10.6133 3.36328 10.6406 3.41797 10.6953C3.47266 10.75 3.47266 10.832 3.44531 10.8594C3.41797 10.9141 3.33594 10.8867 3.28125 10.832C3.19922 10.7773 3.19922 10.6953 3.22656 10.668ZM2.92578 10.2578C2.98047 10.2305 3.03516 10.2578 3.08984 10.3125C3.11719 10.3672 3.11719 10.4492 3.08984 10.4766C3.03516 10.5039 2.98047 10.4766 2.92578 10.4219C2.87109 10.3672 2.87109 10.2852 2.92578 10.2578Z" class="fill-secondary dark:fill-alpha4"></path></svg></a><button aria-label="Chat" class="hover:border-text-primary last:border-r-border group flex h-8 w-8 items-center justify-center border border-r-transparent transition-colors cursor-pointer"><svg width="18" height="15" viewBox="0 0 18 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.625 10.75V10.3125V9.30078V9H1.3125H0V7.6875V1.5625V0.25H1.3125H10.0625H11.375V1.5625V7.6875V9H10.0625H6.125L3.9375 10.0938L2.625 10.75ZM5.52344 7.85156L5.79688 7.6875H6.125H10.0625V1.5625H1.3125V7.6875H2.625H3.9375V8.64453L5.52344 7.85156ZM7 11.1875V9.875H8.3125V11.1875H11.375H11.6758L11.9492 11.3242L13.5625 12.1445V11.1875H14.875H16.1875V5.0625H12.25V3.75H16.1875H17.5V5.0625V11.1875V12.5H16.1875H14.875V12.8008V13.8125V14.25L13.5625 13.5938L11.375 12.5H8.3125H7V11.1875Z" class="fill-secondary dark:fill-alpha4"></path></svg></button></div></div><div class="xxs:grid-cols-3 grid select-none grid-cols-2 gap-8 pb-8 pt-16 md:grid-cols-4"><div class="shrink-0"><span class="text-tertiary text-sm/[1.5] font-medium">Product</span><div class="text-text-primary [&amp;>a:focus]:outline-hidden [&amp;>a:focus]:text-brand-blue [&amp;>a:hover]:text-brand-blue mt-4 flex flex-col gap-1 text-sm/[1.3] [&amp;>a:focus]:underline [&amp;>a:hover]:underline [&amp;>a]:py-2 dark:[&amp;>a:hover]:text-alpha4"><a href="/product/">Vector Database</a><a href="/product/assistant/">Assistant</a><a href="https://docs.pinecone.io/">Documentation</a><a href="/pricing/">Pricing</a><a href="/security/">Security</a><a href="/integrations/">Integrations</a></div></div><div class="shrink-0"><span class="text-tertiary text-sm/[1.5] font-medium">Resources</span><div class="text-text-primary [&amp;>a:focus]:outline-hidden [&amp;>a:focus]:text-brand-blue [&amp;>a:hover]:text-brand-blue mt-4 flex flex-col gap-1 text-sm/[1.3] [&amp;>a:focus]:underline [&amp;>a:hover]:underline [&amp;>a]:py-2 dark:[&amp;>a:hover]:text-alpha4"><a href="https://community.pinecone.io/">Community Forum</a><a href="/learn/">Learning Center</a><a href="/blog/">Blog</a><a href="/customers/">Customer Case Studies</a><a href="https://status.pinecone.io/">Status</a><a href="/learn/vector-database/">What is a Vector DB?</a><a href="/learn/retrieval-augmented-generation/">What is RAG?</a></div></div><div class="shrink-0"><span class="text-tertiary text-sm/[1.5] font-medium">Company</span><div class="text-text-primary [&amp;>a:focus]:outline-hidden [&amp;>a:focus]:text-brand-blue [&amp;>a:hover]:text-brand-blue mt-4 flex flex-col gap-1 text-sm/[1.3] [&amp;>a:focus]:underline [&amp;>a:hover]:underline [&amp;>a]:py-2 dark:[&amp;>a:hover]:text-alpha4"><a href="/company/">About</a><a href="/partners/">Partners</a><a href="/careers/">Careers</a><a href="/newsroom/">Newsroom</a><a href="/contact/">Contact</a></div></div><div class="shrink-0"><span class="text-tertiary text-sm/[1.5] font-medium">Legal</span><div class="text-text-primary [&amp;>a:focus]:outline-hidden [&amp;>a:focus]:text-brand-blue [&amp;>a:hover]:text-brand-blue mt-4 flex flex-col gap-1 text-sm/[1.3] [&amp;>a:focus]:underline [&amp;>a:hover]:underline [&amp;>a]:py-2 dark:[&amp;>a:hover]:text-alpha4"><a href="/legal/">Customer Terms</a><a href="/terms/">Website Terms</a><a href="/privacy/">Privacy</a><a href="/cookies/">Cookies</a><a href="#">Cookie Preferences</a></div></div></div><div class="pb-6"></div></div></div><div class="border-t"><div class="container"><div class="relative px-4 sm:px-8 border-x py-4"><div class="text-tertiary flex flex-wrap justify-between gap-x-10 gap-y-1 text-[0.8125rem]"><p> Pinecone Systems, Inc. | San Francisco, CA</p><p>Pinecone is a registered trademark of Pinecone Systems, Inc.</p></div></div></div></div></footer><script src="https://www.googletagmanager.com/gtag/js?id=UA-155294302-1" id="gtm-preload" data-nscript="afterInteractive"></script><script id="gtm-script" data-nscript="afterInteractive">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-5RCSPVG');</script><script id="campaign" data-nscript="afterInteractive">window.heap.clearEventProperties(); window.heap.addEventProperties({"Created At": "2023-10-27T11:36:56Z"});</script><inkeep-portal><div id="inkeep-shadow:r0:"></div></inkeep-portal><inkeep-portal><div id="inkeep-shadow:r1:"></div></inkeep-portal><textarea tabindex="-1" aria-hidden="true" style="min-height: 0px !important; max-height: none !important; height: 0px !important; visibility: hidden !important; overflow: hidden !important; position: absolute !important; z-index: -1000 !important; top: 0px !important; right: 0px !important; border-width: 0px; box-sizing: border-box; font-family: Inter, sans-serif; font-size: 16px; font-style: normal; font-weight: 400; letter-spacing: normal; line-height: 22px; padding: 8px 12px; tab-size: 4; text-indent: 0px; text-rendering: auto; text-transform: none; width: 100%; word-break: normal;"></textarea></body></html>