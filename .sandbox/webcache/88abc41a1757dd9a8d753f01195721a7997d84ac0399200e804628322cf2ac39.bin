<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>ğŸ“ˆ Reranker | LocalAI</title>
<meta name="viewport" content="width=device-width,initial-scale=1"><meta name="keywords" content="Documentation,Hugo,Hugo Theme,Bootstrap"><meta name="author" content="Colin Wilson - Lotus Labs"><meta name="email" content="support@aigis.uk"><meta name="website" content="https://lotusdocs.dev"><meta name="Version" content="v0.1.0"><link rel="icon" href="https://localai.io/favicon.ico" sizes="any"><link rel="icon" type="image/svg+xml" href="https://localai.io/favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="https://localai.io/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://localai.io/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://localai.io/favicon-16x16.png"><link rel="manifest" crossorigin="use-credentials" href="https://localai.io/site.webmanifest"><meta property="og:title" content="ğŸ“ˆ Reranker"><meta property="og:description" content="A reranking model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks.
Given a query and a set of documents, it will output similarity scores.
We can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.






  



  
    
      
    
  

LocalAI supports reranker models, and you can use them by using the rerankers backend, which uses rerankers."><meta property="og:type" content="article"><meta property="og:url" content="https://localai.io/features/reranker/"><meta property="og:image" content="https://localai.io/opengraph/card-base-2_hu_fe6c53bd05999a5b.png"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2024-05-25T16:11:59+02:00"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://localai.io/opengraph/card-base-2_hu_fe6c53bd05999a5b.png"><meta name="twitter:title" content="ğŸ“ˆ Reranker"><meta name="twitter:description" content="A reranking model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks.
Given a query and a set of documents, it will output similarity scores.
We can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.






  



  
    
      
    
  

LocalAI supports reranker models, and you can use them by using the rerankers backend, which uses rerankers."><link rel="alternate" type="application/atom+xml" title="Atom feed for LocalAI" href="/index.xml"><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><script type="text/javascript" src="https://localai.io/docs/js/flexsearch.bundle.min.f5159d5a2151ffbb653996ec17eaff7da4e04c286bd879fc41839d36a5586f3f20eaead0b6089de48f9adc669cdee771.js" integrity="sha384-9RWdWiFR/7tlOZbsF+r/faTgTChr2Hn8QYOdNqVYbz8g6urQtgid5I+a3Gac3udx" crossorigin="anonymous"></script><link rel="preconnect" href="https://fonts.gstatic.com/"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link href="https://fonts.googleapis.com/css?family=Inter:300,400,600,700|Fira+Code:500,700&amp;display=block" rel="stylesheet"><link rel="stylesheet" href="/docs/scss/style.min.180e5a086197f384396217fc68e7fce6d6aa38d0a772aa0fb28b4c989d6e9aca0836b43ef1dd74b4397ac68717b133e8.css" integrity="sha384-GA5aCGGX84Q5Yhf8aOf85taqONCncqoPsotMmJ1umsoINrQ+8d10tDl6xocXsTPo" crossorigin="anonymous"></head><body><div class="content"><div class="page-wrapper toggled"><nav id="sidebar" class="sidebar-wrapper"><div class="sidebar-brand d-md-flex justify-content-between align-items-center" style="text-align:center;height:calc(35%)"><ul><li><a href="/" aria-label="HomePage" alt="HomePage"><img style="width:calc(65%);height:calc(65%)" src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png"></a></li><li><a href="https://github.com/go-skynet/LocalAI/releases"><img src="https://img.shields.io/github/release/go-skynet/LocalAI?&amp;label=Latest&amp;style=for-the-badge"></a></li><li><a href="https://hub.docker.com/r/localai/localai" target="_blank"><img src="https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker"></a>
<a href="https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;tag=latest" target="_blank"><img src="https://img.shields.io/badge/quay.io-images-important.svg?"></a></li></ul></div><div class="sidebar-content" style="height:calc(65%)"><ul class="sidebar-menu"><li><a class="sidebar-root-link" href="https://localai.io/docs/overview/"><i class="material-icons me-2">info</i>
Overview</a></li><li class="sidebar-dropdown"><button class="btn">
<i class="material-icons me-2">rocket_launch</i>
Getting started</button><div class="sidebar-submenu"><ul><li><a class="sidebar-nested-link" href="https://localai.io/basics/getting_started/">Quickstart</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/getting-started/models/">Install and Run Models</a></li><li><a class="sidebar-nested-link" href="https://localai.io/basics/try/">Try it out</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/getting-started/customize-model/">Customizing the Model</a></li><li><a class="sidebar-nested-link" href="https://localai.io/basics/build/">Build LocalAI from source</a></li><li><a class="sidebar-nested-link" href="https://localai.io/basics/container/">Run with container images</a></li><li><a class="sidebar-nested-link" href="https://localai.io/basics/kubernetes/">Run with Kubernetes</a></li></ul></div></li><li><a class="sidebar-root-link" href="https://localai.io/basics/news/"><i class="material-icons me-2">newspaper</i>
News</a></li><li class="sidebar-dropdown current active"><button class="btn">
<i class="material-icons me-2">feature_search</i>
Features</button><div class="sidebar-submenu d-block"><ul><li><a class="sidebar-nested-link" href="https://localai.io/backends/">Backends</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/gpu-acceleration/">âš¡ GPU acceleration</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/text-generation/">ğŸ“– Text generation (GPT)</a></li><li class="current"><a class="sidebar-nested-link" href="https://localai.io/features/reranker/">ğŸ“ˆ Reranker</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/text-to-audio/">ğŸ—£ Text to audio (TTS)</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/image-generation/">ğŸ¨ Image generation</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/object-detection/">ğŸ” Object detection</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/embeddings/">ğŸ§  Embeddings</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/gpt-vision/">ğŸ¥½ GPT Vision</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/constrained_grammars/">âœï¸ Constrained Grammars</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/distribute/">ğŸ†•ğŸ–§ Distributed Inference</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/audio-to-text/">ğŸ”ˆ Audio to text</a></li><li><a class="sidebar-nested-link" href="https://localai.io/features/openai-functions/">ğŸ”¥ OpenAI functions and tools</a></li><li><a class="sidebar-nested-link" href="https://localai.io/stores/">ğŸ’¾ Stores</a></li><li><a class="sidebar-nested-link" href="https://localai.io/models/">ğŸ–¼ï¸ Model gallery</a></li></ul></div></li><li><a class="sidebar-root-link" href="https://localai.io/docs/integrations/"><i class="material-icons me-2">sync</i>
Integrations</a></li><li class="sidebar-dropdown"><button class="btn">
<i class="material-icons me-2">settings</i>
Advanced</button><div class="sidebar-submenu"><ul><li><a class="sidebar-nested-link" href="https://localai.io/advanced/">Advanced usage</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/advanced/fine-tuning/">Fine-tuning LLMs for text generation</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/advanced/installer/">Installer options</a></li></ul></div></li><li class="sidebar-dropdown"><button class="btn">
<i class="material-icons me-2">menu_book</i>
References</button><div class="sidebar-submenu"><ul><li><a class="sidebar-nested-link" href="https://localai.io/model-compatibility/">Model compatibility table</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/reference/architecture/">Architecture</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/reference/binaries/">LocalAI binaries</a></li><li><a class="sidebar-nested-link" href="https://localai.io/docs/reference/nvidia-l4t/">Running on Nvidia ARM64</a></li></ul></div></li><li><a class="sidebar-root-link" href="https://localai.io/faq/"><i class="material-icons me-2">quiz</i>
FAQ</a></li></ul></div><ul class="sidebar-footer list-unstyled mb-0"></ul></nav><main class="page-content bg-transparent"><div id="top-header" class="top-header d-print-none"><div class="header-bar d-flex justify-content-between"><div class="d-flex align-items-center"><a href="/" class="logo-icon me-3" aria-label="HomePage" alt="HomePage"><div class="small"><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"></path></svg></div><div class="big header-logo"><svg version="1.0" width="1024pt" height="1024pt" viewBox="0 0 1024 1024" preserveAspectRatio="xMidYMid" id="svg12" xmlns:svg="http://www.w3.org/2000/svg"><defs id="defs12"></defs><g transform="matrix(0.17799229,0,0,-0.17799229,-413.52,1737.5728)" fill="#000" stroke="none" id="g12"><path d="M0 5120V0h5120 5120v5120 5120H5120 0zm3596 4831c61-37 155-176 204-304 35-89 75-244 90-342 14-101 12-297-5-454-16-148-23-301-15-301 3 0 14 14 25 32 10 18 55 80 99 138 179 236 317 380 416 434l53 29-6-32c-10-49-56-159-98-233-20-37-40-76-44-87-6-21-6-21 27-2 56 33 195 71 281 77 80 6 290-18 310-35 4-4-55-39-131-78l-138-71 60-12c267-50 496-159 627-297 25-26 63-69 84-95l37-48h-65c-60 0-74 5-169 54-94 50-255 113-264 103-2-2 17-19 44-37 26-18 70-55 99-82l52-48h-628-627l-149-60c-82-33-149-59-149-57-8 24-69 129-135 232-106 163-163 270-191 360-32 99-40 344-15 465 28 139 257 770 279 770 6 0 25-9 42-19zm1998-306c-1-239-33-372-136-580-71-142-234-374-263-375-5 0-48 16-95 35s-79 34-73 35c33 1 249 158 236 171s-204 58-301 71l-92 12 78 81c180 184 327 357 464 544 55 75 114 148 131 163 29 26 32 27 42 10 6-10 10-79 9-167zM3433 8105c-6-39-5-40 31-24 16 8 23 17 19 28s-1 10 9-3c15-19 13-21-35-43-27-13-51-22-53-20-4 4 26 97 31 97 2 0 1-16-2-35zm2699 3c20-37 19-247-2-353-25-124-39-175-50-175-5 0-20 10-35 23l-25 22 25 99c14 54 25 118 25 142 0 29 4 44 13 44 7 0 8 3 2 8-5 4-10 31-10 61-1 40-6 57-20 67-29 22-2025 21-2025-1 0-34 70-414 84-452 24-67 44-91 104-120 56-28 60-28 278-31 243-4 321 7 393 53 58 36 85 92 127 258 20 82 45 158 55 170 16 20 26 22 111 22 123-1 168-10 168-35 0-43-8-47-66-37-64 11-114 3-143-21-13-11-31-56-51-132-59-218-112-291-242-332-67-21-91-23-303-23-210 0-236 2-301 22-160 51-230 168-269 450-20 141-21 143-55 143-36 0-384-113-588-192-215-82-198-79-173-31 30 59 95 91 651 317l135 55 1087 1h1087zm-1516-350-115-223-55-3c-31-2-56 0-56 3s52 104 116 225l116 220h55 54zm294 218c0-2-47-1e2-105-217-60-123-112-217-122-221-10-3-31-7-46-7-37-1-36 1 76 219 116 225 120 230 163 230 19 0 34-2 34-4zm1028-4c-1-4-25-63-52-130l-50-123-30 7c-17 3-33 8-35 10s18 57 45 124l49 120h38c21 0 37-3 35-8zm-507-241c275-69 471-147 551-219 79-71 91-136 46-242-33-76-34-76-97-17-32 29-88 68-127 87-63 32-76 35-159 35-99 0-134-12-192-68-37-35-40-52-17-86 14-19 22-22 48-16 90 20 116 22 171 14 67-11 164-53 229-99 128-92 119-270-17-343-36-19-57-22-152-22-129 1-224 24-352 86-88 42-127 81-144 143-13 45-36 47-83 6-44-39-68-101-64-165 3-55 18-67 38-30 7 14 18 25 24 25s41-20 78-45c147-98 254-139 417-157 130-14 245-3 354 33 76 26 93 20 82-29-11-52-70-108-158-152-171-83-335-79-672 16-66 19-159 41-205 49-99 17-326 20-420 4-36-6-120-11-187-12-68-1-123-3-123-4s26-26 58-56c31-29 77-73 102-96l45-42-60 6c-76 9-212 48-274 79-51 27-134 88-193 144-21 20-38 34-38 32 0-3 17-56 38-117l38-113-25 17c-180 116-370 347-421 511-26 81-34 49-28-111 7-186 77-470 78-317l1 55 62-120c119-229 216-361 418-566 232-235 430-395 604-489 60-32 60-32 20-26-64 10-308 36-335 36-66-1 414-266 565-313 44-14 125-32 180-41 104-16 255-26 255-16 0 3-16 12-36 19-34 12-268 163-278 179-2 4 81 6 185 4 121-2 189 0 189 6s-4 12-9 14c-31 11-275 438-259 453 2 2 38-14 81-36 42-22 77-38 77-36 0 3-11 29-24 58-57 130-113 351-130 519-5 51-3 46 37-65 124-346 259-568 422-698l50-40-55 7c-30 4-75 11-1e2 16s-63 12-84 15l-39 6 41-49c54-64 159-162 214-2e2 41-28 224-123 258-134 8-3-30-3-85-1-55 3-111 1-124-4-22-9-20-12 40-51 95-61 228-120 337-149 89-23 118-26 357-29 150-2 252 0 240 5-11 4-74 25-140 46-104 33-266 97-287 114-7 6 101 4 1095-14 409-8 428-9 423-27-3-10-9-26-12-37-5-15 2-19 48-24 29-4 118-7 198-8 97 0 136-4 117-9-16-5-35-6-43-3s-14 0-14-6c0-8 60-11 198-11 109 0 201-2 204-5s-4-23-14-45l-20-40-87-1c-47-1-135-3-196-5-60-3-315-9-565-14-250-4-637-14-860-20-235-7-554-10-760-6-195 3-512 7-703 8l-348 1-20-29c-10-16-19-32-19-35 0-7 171-14 690-30 206-6 383-15 393-20 9-6 17-19 17-29 0-19 9-20 363-22 340-1 326-2-228-10-324-4-606-12-625-16-26-6-39-17-54-45l-20-37h-136-136l-20-44-21-44 201-6c479-16 978-24 1336-22 212 2 257 1 101-1-177-2-289-7-297-14-7-5-20-31-29-57l-17-47h-797-796l-5-62-5-62-45 31c-70 48-174 134-219 183-23 25-43 45-44 45-10 0 19-148 44-227 55-170 126-302 226-421l45-52-59 21c-218 80-429 215-575 367-96 99-85 98 15-1 93-94 196-173 312-240 77-45 236-117 257-117 6 0-5 19-24 43-19 23-54 74-79 114s-53 74-65 77c-24 6-217 178-290 259-122 133-220 289-315 498-34 74-67 150-74 169-13 34-14 33-14-44-1-72 22-373 34-456l5-35-25 40c-67 104-186 363-240 520-53 158-145 603-145 704 0 58-8 31-29-99-47-290-54-374-48-620 5-225 13-294 52-455 5-16 5-27 1-24-3 3-15 46-26 95-63 290-56 624 20 1061 11 61 20 114 20 118s-37-44-81-107c-45-62-111-151-148-197-36-47-71-94-78-105-12-21-12-21-13 2 0 39 29 312 46 417 23 155 125 718 130 727 12 18-11 3-130-88-69-52-126-92-126-88s13 50 30 103c41 134 112 431 155 651 20 102 44 218 54 258 11 39 17 72 16 72-4 0-102-63-201-130l-72-48 40 87c87 188 191 397 206 414 8 9 47 27 86 38 39 12 140 43 223 70 84 27 159 49 165 48 13 0-196-69-375-125l-88-27-30-59c-17-33-28-63-25-66 5-4 332 63 401 83 17 5 25-3 44-42 73-151 172-256 305-323 105-52 170-64 331-57 148 6 239 27 372 84 76 32 84 38 63 44l-25 7 25 1c52 2 163 66 221 128 75 79 120 168 159 320 12 47 33 73 59 73 9 0 87-18 172-39zm-1571-13c0-41 37-166 67-231 38-79 82-130 156-178 72-47 137-69 230-79 66-7 68-7 22-9-154-4-330 1e2-402 236-48 91-104 283-82 283 5 0 9-10 9-22zm24e2-483c102-205 63-418-80-441-19-3-47-1-62 5-28 10-28 12-28 97 0 48-5 116-11 150-10 57-9 66 9 91 32 46 70 128 83 178l12 48 19-24c10-13 36-60 58-104zm-1530-6c-25-4-76-7-115-6l-70 1 80 6c44 3 96 7 115 7 31 0 29 0-10-8zm-222-16c-21-2-55-2-75 0-21 2-4 4 37 4s58-2 38-4zm2793-545-18-33-114 1c-63 1-155 2-206 3l-92 1 10 30 11 30h214 214zm801-315 26-5-16-44-16-44h-251c-137-1-373-2-522-3-306-2-280-10-247 70l15 34 492-1c271-1 504-4 519-7zM4015 6098c75-113 349-363 515-470l75-48h-112c-61 0-114 3-116 8-3 4 40 6 95 4l101-3-84 56c-130 88-206 154-352 304l-134 138 23-90c12-49 19-86 15-81-11 12-65 224-58 224 3 0 17-19 32-42zm-1073-40c-8-8-9-4-5 13 4 13 8 18 11 10 2-7-1-18-6-23zm4346-32-22-46-780 2-781 3 779 5 780 5 13 33 13 32h-770-771l-11-31c-6-16-12-28-14-26s1 18 7 35l11 32 781 2c430 2 783 2 785 2 2-1-7-22-20-48zm-4398-1c-7-9-15-13-19-10-3 3 1 10 9 15 21 14 24 12 10-5zm3760-322c-368-7-690-7-690 0 0 4 188 6 418 5 229-1 352-4 272-5zm223 0c-13-2-35-2-50 0-16 2-5 4 22 4 28 0 40-2 28-4zm170 0c-7-2-21-2-30 0-10 3-4 5 12 5 17 0 24-2 18-5zm155 0c-16-2-40-2-55 0-16 2-3 4 27 4s43-2 28-4zm605 0c-7-2-19-2-25 0-7 3-2 5 12 5s19-2 13-5zM6173 5603c-46-2-120-2-165 0-46 1-9 3 82 3s128-2 83-3zm114 1c-3-3-12-4-19-1-8 3-5 6 6 6 11 1 17-2 13-5zm211-1c-38-2-98-2-135 0-38 2-7 3 67 3s105-1 68-3zm190 0c-21-2-55-2-75 0-21 2-4 4 37 4s58-2 38-4zm2292-121c0-4-7-26-16-49l-17-44-585 1c-321 1-586-1-588-3s-12-30-22-63l-18-59-754 3c-414 1-755 4-757 5-1 2 7 27 19 55l22 52h420 421l27 50 27 49-27 4c-15 2 395 4 911 4 515 0 937-2 937-5zm-765-395c83-1-223-4-678-5-735-2-828-4-834-17-3-9-3-19 1-22 3-4 1e2-10 214-14l207-7-222 2c-123 0-223 1-223 2s5 15 11 32l11 32 286 2c289 3 949 0 1227-5zM7115 4980c-9-16-18-30-21-30-2 0 2 14 11 30 9 17 18 30 21 30 2 0-2-13-11-30zM3625 4539c13-46 22-85 20-87-3-4-55 159-55 174 0 20 13-11 35-87zm-58 44c-3-10-5-4-5 12 0 17 2 24 5 18 2-7 2-21 0-30zm-13-87c-4-31-9-54-12-52-2 3-1 30 3 61 3 31 9 55 11 52 3-2 2-30-2-61zm143-163c-3-2-12 15-22 39-28 69-27 78 0 19 14-29 24-55 22-58zm-163 0c-6-76-8-83-16-55-5 20-5 39 2 51 5 11 10 39 10 62s3 40 6 36c3-3 3-45-2-94zm2254 50c-65-2-171-2-235 0-65 1-12 3 117 3s182-2 118-3zM3877 4011c31-47 84-120 119-162 63-76 171-176 180-167 2 3-5 27-16 54-29 67-70 240-69 289 1 33 3 30 9-20 16-120 62-270 107-350 15-26 11-25-47 20-96 76-194 187-280 317-71 108-180 304-180 325 0 4 28-43 61-106s85-153 116-2e2zm2666 172c-13-2-33-2-45 0-13 2-3 4 22 4s35-2 23-4zm80 0c-7-2-19-2-25 0-7 3-2 5 12 5s19-2 13-5zm74 1c-3-3-12-4-19-1-8 3-5 6 6 6 11 1 17-2 13-5zm81-1c-10-2-28-2-40 0-13 2-5 4 17 4 22 1 32-1 23-4zm944-10c-95-2-248-2-340 0-92 1-14 3 173 3s262-2 167-3zm1027-29c3-2-3-20-12-39l-17-35h-436c-411 0-436 1-429 18 4 9 12 30 17 46l11 28 431-7c237-4 433-9 435-11zm-2735-81-612-3-27-65c-16-36-30-63-32-61s9 34 26 70l30 66 613-2 613-3zM4719 3961c56-34 134-78 174-98l72-38 455-5 455-5-451-3-451-2-112 60c-124 67-291 168-291 177 0 3 11-2 23-10 13-9 70-43 126-76zm674-58c-7-2-21-2-30 0-10 3-4 5 12 5 17 0 24-2 18-5zm290-10c-7-2-19-2-25 0-7 3-2 5 12 5s19-2 13-5zm495 0c-81-2-216-2-3e2.0-84 1-18 3 147 3s234-2 153-3zm365-10c-13-2-35-2-50 0-16 2-5 4 22 4 28 0 40-2 28-4zm1e2.0c-7-2-19-2-25 0-7 3-2 5 12 5s19-2 13-5zm455 0c-82-2-214-2-295 0-82 1-15 3 147 3s229-2 148-3zm-725-70c-18-2-48-2-65 0-18 2-4 4 32 4s50-2 33-4zm-385-10c-10-2-28-2-40 0-13 2-5 4 17 4 22 1 32-1 23-4zm80 0c-10-2-28-2-40 0-13 2-5 4 17 4 22 1 32-1 23-4zm139 0c-20-2-52-2-70 0-17 2 0 4 38 4 39 0 53-2 32-4zm-235-153-404-5-13-32-14-33 392-3 392-2-402-3-402-2 14 28c8 15 16 35 19 45 5 16 32 17 413 15l408-3zm1578-32-19-43-319-3-319-2 15 45 14 45h324 323zm1217-388c18-10 40-31 50-47 17-26 18-82 18-1088 0-997-1-1063-18-1088-9-15-29-35-44-44-26-17-207-18-3673-18-3497 0-3647 1-3673 18-16 10-37 32-48 50-19 31-19 55-17 1094l3 1062 30 31c17 18 39 36 50 40 11 5 1656 9 3655 9 3586 1 3635 1 3667-19z" id="path1"></path><path d="m3552 9557c-72-188-131-385-143-476-23-171 4-313 91-486 50-101 158-252 175-248 6 2 20 53 33 115 71 355 70 721-4 975-27 93-88 233-101 233-4 0-27-51-51-113z" id="path2"></path><path d="m6896 6674c-3-9-6-18-6-20s89-4 199-4c190 0 199 1 204 20 5 20 0 20-193 20-171 0-199-2-204-16z" id="path3"></path><path d="m7313 5393c9-2 23-2 30 0 6 3-1 5-18 5-16 0-22-2-12-5z" id="path4"></path><path d="m7463 5393c15-2 39-2 55 0 15 2 2 4-28 4s-43-2-27-4z" id="path5"></path><path d="m7618 5383c12-2 32-2 45 0 12 2 2 4-23 4s-35-2-22-4z" id="path6"></path><path d="m7323 5283c9-2 23-2 30 0 6 3-1 5-18 5-16 0-22-2-12-5z" id="path7"></path><path d="m7428 5283c7-3 16-2 19 1 4 3-2 6-13 5-11 0-14-3-6-6z" id="path8"></path><path d="m1507 3112c-16-17-17-102-17-1011 0-665 3-999 10-1012 11-19 62-19 3598-19 2777 0 3591 3 36e2 12s12 249 12 1020c0 986 0 1008-19 1018-14 7-1149 10-3594 10-3450 0-3574-1-3590-18zm653-892v-640h205 205v-135-135h-355-355v775 775h150 150zm1258 626c67-18 114-49 150-101 56-79 56-71 59-602 1-271-2-529-7-574-14-118-54-181-150-231-43-22-55-23-293-26-280-3-313 2-384 63-22 19-51 58-64 87l-24 53v570 570l28 57c31 64 79 105 152 130 67 23 450 26 533 4zm1144-15c26-12 65-42 87-66 58-63 71-118 71-297v-148h-147-148l3 92c4 102-9 164-36 172-9 3-63 6-119 7-158 3-143 56-143-508 0-567-16-513 148-513 144 0 146 2 149 149l2 106 146 3 145 3v-130c0-72-5-153-11-181-15-72-64-137-129-174l-55-31h-255c-291 0-301 2-377 89-68 77-67 64-70 649-4 568-1 612 45 687 29 47 111 105 161 113 14 3 130 4 256 3 219-1 232-2 277-25zm1109-416c48-242 115-579 149-750 33-170 63-320 65-332l5-23h-158-159l-22 133c-13 72-25 142-28 155-5 22-8 22-163 22h-158l-10-47c-6-27-18-96-28-155l-17-108h-148c-94 0-149 4-149 10 0 10 174 957 245 1339 19 101 35 187 35 192 0 6 95 8 227 7l227-3zm689-195v-640h215 215v-135-135h-360-360v775 775h145 145zm1315 618c3-13 52-264 110-558 57-294 124-633 148-753l43-217-155 2-155 3-26 150-26 150-161 3c-160 2-161 2-166-20-3-13-16-83-28-155l-23-133h-153c-142 0-154 1-149 18 3 9 48 247 1e2 527 53 281 117 621 143 758l47 247h223 223zm765-753v-775h-150-150v775 775h150 150z" id="path9"></path><path d="m3110 2593c-108-7-1e2 36-1e2-513 0-432 2-479 17-491 17-15 194-26 238-15 57 14 55-3 55 506 0 539 6 507-90 513-30 3-84 2-120 0z" id="path10"></path><path d="m5297 2207c-32-177-57-326-57-332 0-9 240-16 240-7 0 2-25 142-55 311-30 170-55 318-55 330 0 11-3 21-7 21-5 0-34-145-66-323z" id="path11"></path><path d="m7386 2208c-32-178-59-327-60-332 0-5 53-10 121-10 68-1 123 1 123 2 0 2-25 140-55 307-30 166-55 314-55 329 0 80-22-8-74-296z" id="path12"></path></g><path style="fill:#fff;stroke-width:1.61514" d="m388.3033 1032.2948c-20.71096-17.301-45.27669-43.2443-58.38474-61.6589-11.072-15.55418-33.46614-57.80895-43.39625-81.88298-3.01819-7.31707-6.03272-13.31332-6.69897-13.32487-1.95247-.0337-1.40079 24.75105 1.29348 58.11165 1.37766 17.05815 2.27248 31.71195 1.98848 32.56395-.78972 2.36917-12.80508-19.62698-23.53365-43.08218-12.96414-28.3428-20.79049-49.58497-26.46888-71.84167-6.76534-26.517-17.37487-80.62553-18.56152-94.66365-.55492-6.56464-1.64295-11.93571-2.41786-11.93571-.7749.0-.95287-1.81703-.39547-4.03785.55739-2.22083.38953-4.02758-.37302-4.01502-.76254.0126-13.37888 16.07913-28.03629 35.70348-17.629 23.60295-26.84316 34.7169-27.22097 32.83357-2.07325-10.33507 16.05308-129.48461 29.82122-196.0234 1.51659-7.32942.54523-6.88403-22.72283 10.41915-10.36333 7.70663-19.26074 13.59374-19.77203 13.08245-.5113-.51129 1.34431-8.76373 4.12356-18.33874 7.20702-24.82946 18.19753-71.30929 26.84032-113.51007 4.09344-19.98738 8.54196-40.58853 9.88557-45.78031 1.34361-5.19179 2.20068-10.16637 1.9046-11.05463-.29609-.88827-10.58825 5.03694-22.87147 13.16714-12.28321 8.13019-22.54778 14.5584-22.81014 14.28492-.26236-.27349 6.30687-14.67012 14.5983-31.99252 12.34638-25.79398 15.62741-31.41282 18.1252-31.03978 2.43485.36363 30.81942-5.14402 70.03777-13.58991 3.88554-.83677 4.75341.0873 11.05023 11.76616 15.48867 28.72712 37.4065 48.23215 65.33311 58.14095 17.84309 6.33101 59.21525 5.86977 85.06634-.94835 17.00919-4.48612 40.63194-14.58061 39.71468-16.97092-.4457-1.16146 1.92757-2.64588 5.9613-3.72864 9.68831-2.6006 24.78351-12.94263 34.59881-23.70435 9.65005-10.58055 19.42214-29.62875 24.37971-47.52203 3.57011-12.88552 7.16307-19.62079 10.46681-19.62079 4.94171.0 55.72594 13.36116 73.98866 19.46617 27.39258 9.15701 44.55449 17.67808 55.1997 27.40721 13.86553 12.6723 15.66214 23.71167 6.95446 42.73176-4.83371 10.55823-2.91266 10.82459-19.71904-2.7342-5.19131-4.18817-13.92002-9.82098-19.39712-12.51735-9.17362-4.51615-11.2792-4.89969-26.71917-4.86702-19.82745.0419-26.52849 2.42277-35.81817 12.72582-4.62627 5.13094-5.88342 7.67706-5.10887 10.34708 1.37858 4.75209 7.04231 9.44926 10.25887 8.50808 10.37842-3.03675 23.66238-4.16161 31.66841-2.68161 12.32864 2.27907 30.83972 10.32378 40.26674 17.49949 19.48079 14.82849 20.97723 40.36687 3.22283 55.00154-8.74622 7.20937-17.39143 9.0287-37.52362 7.89659-19.39093-1.09041-35.54676-5.10624-53.01853-13.17868-16.48419-7.61618-22.87535-12.98468-27.86253-23.40428-5.67119-11.84865-9.24734-12.42612-18.66024-3.01323-4.1907 4.19071-7.71185 9.58078-8.78569 13.44887-2.22444 8.01268-2.29783 20.18785-.14115 23.41423 1.36972 2.04906 2.2258 1.73769 5.55079-2.01893 2.16222-2.4429 4.26363-4.44164 4.66981-4.44164s8.37729 4.63204 17.71361 10.2934c31.79671 19.28098 53.71653 25.60268 89.10978 25.69939 15.8895.0434 22.53493-.74238 36.88209-4.3612 19.15027-4.83033 22.90386-4.64154 21.21735 1.06707-3.76546 12.74559-18.65751 24.68116-40.02737 32.08087-26.82557 9.28885-44.41195 8.10049-104.48216-7.06012l-39.57098-9.98699-36.34069.0103c-39.16468.0111-93.64111 2.20926-93.73748 3.78243-.0326.53184 7.41726 8.03383 16.5552 16.67111l16.61445 15.70414-5.65299.0394c-7.36152.0513-34.00998-7.1066-44.76909-12.02524-10.65065-4.86904-20.5517-11.6761-34.71958-23.87003-6.14204-5.2863-11.47485-9.30396-11.85068-8.92814-.37582.37583 1.93006 8.48762 5.12418 18.0262 3.19413 9.53857 5.38473 17.76564 4.86802 18.28234-1.52017 1.52017-18.65497-12.14789-31.49364-25.12178-19.95379-20.16396-37.90692-46.94999-43.78482-65.32687-1.08022-3.37724-2.80376-5.85118-3.8301-5.49765-2.77839.95704-1.95736 39.13313 1.24026 57.66961 5.2136 30.22295 10.63706 42.34657 11.94479 26.70143.50135-5.99789.82627-5.58332 12.40171 15.82377 21.71999 40.16799 34.95735 57.79704 70.97771 94.52557 38.10485 38.85401 68.36531 64.34063 97.74168 82.32211 15.62967 9.56707 15.73868 9.5223-14.46874 5.9394-23.29329-2.76278-32.30284-3.02873-32.30284-.95348.0 3.91973 51.50471 33.42368 82.46175 47.23733 10.90841 4.86757 25.03298 9.64455 35.29454 11.93685 24.66343 5.50942 63.13961 9.10432 63.13961 5.89927.0-.7113-3.31382-2.90325-7.36405-4.87102-4.05022-1.96778-16.57503-9.56243-27.83293-16.87703l-20.46889-13.29937 33.72447-.80753 33.72448-.8076-4.30958-5.118c-8.24182-9.78787-44.62176-72.56733-44.62176-77.002.0-.60272 5.88972 1.86546 13.08827 5.48481 7.19853 3.61936 13.48438 6.18453 13.96854 5.70037.48415-.48415-1.34899-6.52629-4.07367-13.42698-9.51018-24.08618-21.52674-71.06893-21.30298-83.29114.0358-1.95365 4.27231 6.98671 9.41452 19.86747 12.46275 31.218 17.65562 42.27397 28.23379 60.11167 11.10164 18.7204 25.15989 36.3228 37.24697 46.6371 11.00484 9.39083 10.77186 9.4566-16.71887 4.72358-10.16278-1.74975-18.80674-2.8524-19.20878-2.45033-1.26564 1.26563 17.48825 21.61853 30.42073 33.01448 11.32802 9.98212 34.4235 24.3873 51.98071 32.42152l6.96969 3.18938-11.50992-1.01183c-13.36877-1.17525-26.45213.38295-26.43178 3.1479.0271 3.67793 26.07377 18.5391 48.47533 27.65798 24.26104 9.87585 30.84393 10.81252 82.33703 11.71515 30.00174.5259 47.04857.26902 46.03157-.69375-.88838-.8409-12.8808-5.35905-26.64983-10.04033-13.76912-4.68135-29.39562-10.54695-34.72558-13.0347l-9.69086-4.52325 33.71054-.1389c43.38588-.1788 207.14335 2.7951 220.04178 3.996l9.8658.9186-.17498 5.95275-.17497 5.95268 8.88322 1.05165c4.8858.5784 17.10285 1.1235 27.14898 1.21132 10.2975.0901 17.8758.7905 17.3721 1.60568-1.7218 2.78587 4.4406 3.39967 34.3492 3.42157 16.5835.012 31.7621.45375 33.7301.98138l3.5782.95925-3.1003 6.07725-3.1005 6.07717-27.1276 1.07903c-107.15948 4.26225-251.50605 6.52717-418.46093 6.5661l-181.36473.042-3.3876 5.34982c-1.96333 3.10065-2.78481 5.94293-1.95388 6.76043.78854.7758 21.7845 2.17372 46.65769 3.1065 24.87318.93285 67.02661 2.50395 93.67428 3.4914 26.64765.98745 49.724 2.4771 51.28075 3.3102 1.55675.83317 2.83046 2.5017 2.83046 3.70792.0 3.20363 3.63309 4.12185 17.49487 4.42178 6.81188.14737-12.46619.7854-42.8402 1.41787-30.37399.63248-57.82445 1.63755-61.00103 2.2335-4.38686.82298-6.73788 2.6079-9.7777 7.42328l-4.00211 6.33975-24.35067.44587-24.3507.44588-3.33807 7.71607c-1.83594 4.24388-3.03186 8.0223-2.65762 8.39655 1.03876 1.03875 95.9203 3.86655 170.97465 5.09565l67.02838 1.09763-4.53321 9.4401-4.53322 9.44017H583.14747c-127.13881.0-141.1573.25208-142.04091 2.5548-.53922 1.40513-.98038 6.12946-.98038 10.49851.0 4.3689-.4804 7.9436-1.06757 7.9436-2.61387.0-26.83991-19.54048-39.5428-31.8947-7.71687-7.50511-14.41947-12.83483-14.98669-11.91698-2.56171 4.14487 7.69271 42.95998 17.42865 65.97118 2.33773 5.5252 4.25043 10.4313 4.25043 10.9022.0 2.4087-4.04289-.2661-17.9049-11.8458z" id="path13"></path><path style="fill:#fff;stroke-width:1.61514" d="m355.05406 421.62226c-22.15321-2.54868-39.59314-14.30198-48.63897-32.77926-6.04069-12.33888-8.90612-23.2277-13.2547-50.36877-1.8413-11.49217-4.20443-21.92702-5.25141-23.18855-1.3313-1.60412-3.86472-2.02278-8.42679-1.39258-9.07 1.25293-66.08936 19.96806-102.09803 33.51102-16.50454 6.2074-30.38086 10.91358-30.83627 10.45818-.4554-.4554 1.14955-3.42072 3.56656-6.58958 5.6722-7.43663 27.9304-18.20839 91.38694-44.22631l47.68436-19.55115h191.56267c176.44971.0 191.73756.21135 193.77937 2.67882 1.70083 2.05542 2.19749 9.29085 2.13414 31.09149-.0698 24.0326-.65966 30.77805-3.82618 43.75651-4.99915 20.48989-5.04441 20.55596-10.72962 15.66574l-4.65695-4.00572 3.63151-15.11708c1.99732-8.31439 4.08768-19.11455 4.64526-24.00035.55759-4.88581 1.52545-9.24669 2.15082-9.69085 1.28209-.91062-1.28033-20.49609-3.1697-24.22713-2.02842-4.00562-50.45784-5.0313-212.00487-4.49001-104.31393.34951-148.65237 1.01809-149.89452 2.26024-1.95456 1.95456-.1876 14.10347 8.31122 57.14459 5.72718 29.00448 9.56249 37.55882 19.72081 43.98545 11.16735 7.065 19.81401 8.04705 64.50501 7.32622 44.97684-.72543 51.97729-1.97508 63.2868-11.29734 7.94205-6.5465 11.01861-13.2364 19.40745-42.20073 8.6857-29.9893 10.04722-33.16113 14.74023-34.33899 2.00472-.50316 11.58278-.66744 21.28458-.36507 18.803.58602 23.28604 2.22616 21.78405 7.96975-.78929 3.01829-1.77745 3.19583-13.15817 2.36422-22.65981-1.6558-24.72434.30701-34.02158 32.34549-9.8321 33.88156-20.16489 47.26471-41.74556 54.06934-9.31197 2.93616-15.71318 3.49474-45.55974 3.97559-19.09906.30769-39.93798-.0402-46.30872-.77318z" id="path14"></path><path style="fill:#fff;stroke-width:1.61514" d="m225.72518 277.27438c-1.83613-3.70595-11.08035-19.09394-20.54272-34.19552-30.90205-49.3185-36.66484-64.15935-38.88444-100.1388-1.36409-22.11159.95113-42.06571 7.39908-63.770308 10.014-33.708396 39.335-112.759581 43.29126-116.715842 2.6544-2.654398 11.12175 3.275841 19.60242 13.728864 16.14315 19.897551 29.01876 50.591928 37.43022 89.230475 4.41311 20.271851 4.72573 61.869291.78293 104.176651-2.98872 32.06993-3.31457 44.11879-1.17206 43.33964.81424-.2961 6.73431-7.80651 13.15571-16.68979 20.26686-28.03693 35.48538-46.25999 54.72634-65.53079 20.32182-20.35334 35.62173-31.421643 35.62173-25.7696.0 4.77523-5.52652 18.092-16.57515 39.93971-5.48094 10.83806-9.55955 20.11136-9.06357 20.60734s5.55908-1.19341 11.25132-3.75422c18.99804-8.54675 30.57629-10.7267 56.38109-10.61544 12.88076.0555 27.73652 1.04419 33.01282 2.197l9.59325 2.09603-6.36296 3.94899c-3.49965 2.17195-14.35794 8.00914-24.12955 12.97153-20.38921 10.35438-21.07058 9.11777 8.94537 16.2351 23.86774 5.65947 48.53884 15.21447 66.77706 25.86243 13.00859 7.59477 37.74481 30.20897 46.13841 42.18039l3.82376 5.45364-10.35019-.6642c-8.6108-.55259-14.15019-2.45143-32.96217-11.2991-42.19276-19.84412-52.3725-21.02598-30.46774-3.53728 7.96216 6.357 15.4381 12.71661 16.61318 14.1325 1.95153 2.35145-7.43168 2.57436-108.3648 2.57436H280.89447l-18.50402 7.44437c-10.1772 4.09441-21.83916 8.76188-25.91544 10.37218l-7.41141 2.9278zm17.75927-39.97998c5.80406-21.27448 9.54882-53.13015 10.43996-88.81001 1.27526-51.05987-2.84802-80.48948-15.88225-113.358388-6.67457-16.831506-9.48275-22.204866-11.60453-22.204866-3.40199.0-22.32968 50.984152-31.47985 84.794954-3.87095 14.30358-4.36042 18.92317-4.27836 40.37854.0844 22.05107.49901 25.46023 4.61666 37.95584 7.72944 23.45617 23.16073 51.61548 36.79482 67.14385 2.73301 3.11273 5.77824 5.39119 6.76718 5.06325.98893-.32792 3.0708-5.26135 4.62637-10.96317z" id="path15"></path><path style="fill:#fff;stroke-width:1.61514" d="m223.79996 274.00911c-3.01576-5.46337-12.75571-21.56243-21.64431-35.77568-28.04556-44.84602-33.66712-59.7856-35.85763-95.29337-1.98877-32.23755 2.29585-52.57337 23.79181-112.921479 11.38913-31.9740576 24.7091-65.375246 26.97404-67.640184 2.58513-2.585125 11.61189 3.853342 20.00366 14.267891 15.68739 19.4687319 28.64184 50.586973 36.95347 88.766962 4.41311 20.27185 4.72573 61.86929.78293 104.17665-2.95821 31.74252-3.30272 43.60883-1.26604 43.60883.76256.0 3.9281-3.81576 7.03454-8.47949 23.23829-34.88781 70.26618-87.12279 88.08372-97.83664 7.74948-4.65983 8.4795-4.82713 8.4795-1.94324.0 4.64121-5.39199 17.70136-16.5211 40.01643-5.42626 10.88027-9.45223 20.19601-8.94659 20.70167.50565.50563 4.44399-.86181 8.75189-3.03879 4.30787-2.17696 13.64703-5.67819 20.75365-7.7805 11.15619-3.30029 16.2304-3.81098 37.14827-3.73878 23.70739.0818 44.54283 2.88307 41.76258 5.61483-.76122.74794-10.64101 6.18477-21.95508 12.08183-11.31409 5.89708-20.99043 11.40052-21.50302 12.22988-1.08667 1.75826-2.36785 1.34931 23.56202 7.52117 23.91072 5.69127 48.58686 15.25725 66.76129 25.88073 13.07073 7.64022 38.16434 30.54687 46.20744 42.18039l3.7705 5.45364-10.35019-.66047c-8.57332-.54708-14.01024-2.39464-31.66988-10.762-28.16171-13.34337-45.48451-19.30117-43.82435-15.07244.2961.75424 7.4431 7.11386 15.88222 14.13249l15.34385 12.76112-112.25237.80758-112.25236.80757-17.76657 7.39031c-9.7716 4.06468-20.68889 8.5347-24.26062 9.93339l-6.49406 2.5431zm19.6856-36.71471c7.1393-26.15828 9.90784-53.62125 10.00034-99.19977.0961-47.373242-1.29745-59.76127-9.74534-86.629243-5.06182-16.098753-15.04248-38.544251-17.13918-38.544251-3.57865.0-20.62976 45.580498-30.81445 82.372237-4.75575 17.179957-5.13519 20.392637-5.05509 42.801257.0788 22.04739.49306 25.46233 4.60422 37.95584 2.48469 7.55079 7.42999 19.54322 10.98955 26.64985 11.64717 23.25347 28.52359 46.88658 32.53247 45.55726.98893-.32793 3.07131-5.26136 4.62748-10.96318z" id="path16"></path><path style="fill:#fff;stroke-width:1.61514" d="m392.38996 1035.8999c-15.13576-12.2758-39.30543-36.04348-50.69893-49.8557-18.02207-21.84802-33.09611-48.27817-56.65212-99.33128l-5.58923-12.11355-.51588 12.11355c-.28374 6.66248.67245 27.26168 2.12485 45.77595 1.45241 18.51435 2.33964 33.96353 1.97165 34.33155-1.21618 1.21613-16.98653-28.86262-26.75772-51.03494-11.69211-26.53118-18.37169-45.23176-23.78746-66.59686-6.21694-24.52567-16.80633-79.21537-17.94387-92.67247-.55492-6.56464-1.64295-11.93571-2.41786-11.93571-.7749.0-.95287-1.81703-.39547-4.03785.55739-2.22082.38953-4.02758-.37302-4.01502-.76254.0126-13.37888 16.07913-28.03629 35.70348-17.629 23.60295-26.84316 34.7169-27.22097 32.83357-2.07325-10.33507 16.05308-129.48461 29.82122-196.0234 1.51659-7.32942.54523-6.88403-22.72283 10.41915-10.36333 7.70663-19.26074 13.59374-19.77203 13.08245-.5113-.51129 1.33793-8.76373 4.10939-18.33873 7.63584-26.3808 15.54565-59.7204 25.39746-107.04951 4.90013-23.5407 9.53249-45.34512 10.29414-48.45426 3.23432-13.20281 3.37321-14.53628 1.51412-14.53628-1.02924.0-11.30073 6.2168-22.82555 13.81511s-21.16054 13.59453-21.4127 13.32492c-.25217-.26961 6.23357-14.54891 14.41274-31.73178l14.87123-31.24161 7.17067-.49381c3.94388-.27161 20.90912-3.47471 37.70055-7.11803 16.79143-3.64331 31.08287-6.6242 31.75876-6.6242.67588.0 4.23751 5.57781 7.91473 12.39513 16.42326 30.44767 40.20957 50.6062 70.39843 59.66158 11.23677 3.37056 47.78551 3.76315 65.32881.70174 20.85026-3.63849 55.71674-16.65451 54.32541-20.28024-.49141-1.28058 1.06907-2.42398 4.14875-3.03992 8.68328-1.73665 22.77758-10.93761 33.80463-22.06813 12.16239-12.27651 20.02605-26.6277 26.43602-48.24578 4.34839-14.66532 7.72696-21.11807 11.05712-21.11807 4.9417.0 55.72593 13.36116 73.98865 19.46617 27.39258 9.15702 44.55449 17.67808 55.1997 27.40721 13.86553 12.6723 15.66214 23.71168 6.95446 42.73176-4.83138 10.55314-2.9355 10.80829-19.59561-2.63733-5.12344-4.13489-13.92937-9.7677-19.56873-12.51735-9.38836-4.57757-11.62329-4.99936-26.49069-4.99936-18.36975.0-25.17485 2.05931-33.62887 10.1765-7.90467 7.58973-8.84921 10.00947-6.03298 15.45546 1.32662 2.56536 3.37321 5.03313 4.548 5.48395 1.17481.45081 7.56131-.21992 14.19224-1.49051 14.65778-2.80868 24.98421-1.38909 42.19869 5.8011 23.79945 9.94061 35.27078 22.94875 35.27078 39.99598.0 10.58582-4.10341 19.30789-12.23835 26.01339-8.74379 7.2074-17.39175 9.0281-37.52362 7.90011-19.28268-1.08041-30.47972-3.64169-48.23683-11.03398-19.00042-7.90991-26.41727-13.63891-32.1233-24.81304-6.4261-12.58429-9.70757-13.22628-19.18116-3.75268-4.1907 4.1907-7.71186 9.58077-8.7857 13.44886-2.22444 8.01269-2.29783 20.18786-.14115 23.41423 1.36972 2.04907 2.2258 1.7377 5.5508-2.01893 2.16222-2.4429 4.33752-4.44164 4.83402-4.44164.49649.0 6.58259 3.55967 13.52466 7.91036 33.75854 21.15693 56.26604 27.94005 93.13452 28.06809 15.92178.0553 22.50579-.72069 36.88209-4.34686 19.15026-4.83032 22.90386-4.64153 21.21734 1.06708-3.76546 12.74558-18.6575 24.68115-40.02737 32.08086-26.82474 9.28856-44.45504 8.09993-104.48216-7.04419l-39.57098-9.9833-36.34069 48e-5c-39.1111 5e-4-93.6479 2.1957-93.72632 3.77262-.0265.53184 7.4234 8.02699 16.55521 16.6559l16.60329 15.68893-5.653.0547c-7.41168.0716-34.01594-7.09405-45.02513-12.12723-10.18732-4.65744-25.00056-14.97123-37.00626-25.76581-4.69833-4.22437-8.86842-7.35468-9.26685-6.95625-.39843.39843 1.88895 8.52872 5.08308 18.0673 3.19412 9.53858 5.38473 17.76564 4.86801 18.28234-1.51834 1.51836-18.66372-12.05161-31.23915-24.72469-19.73199-19.88524-38.17955-47.39844-44.0117-65.64032-1.06504-3.33123-2.57395-6.05678-3.35313-6.05678-6.6031.0.16413 73.95258 8.12912 88.83531 2.50187 4.67477 4.84409 2.49057 4.84409-4.51727.0-5.39068 1.13295-3.86338 12.26636 16.53594 23.19887 42.50643 40.95007 65.32331 82.43132 105.95488 35.3382 34.61437 56.53258 52.02502 85.37209 70.13107 16.21502 10.18005 16.57434 10.01917-14.51772 6.50212-24.41149-2.76127-31.49527-2.97232-31.49527-.9381.0 3.9078 51.50139 33.36645 81.79682 46.78748 19.36821 8.58022 30.12457 11.84362 50.64482 15.36525 18.26703 3.135 48.45426 5.0919 48.45426 3.14115.0-.83258-3.31382-3.12368-7.36405-5.09145-4.05022-1.96778-16.57395-9.56243-27.83051-16.87703l-20.46644-13.29937 32.67592-.43718c21.22249-.28402 32.67594-1.02487 32.67594-2.11365.0-.92205-2.11767-4.35937-4.70593-7.63852-8.46669-10.72665-42.13319-69.49345-42.13319-73.54583.0-.60272 5.88972 1.86546 13.08827 5.48481 7.19853 3.61936 13.48438 6.18453 13.96854 5.70038.48415-.48416-1.34899-6.52629-4.07367-13.42699-9.52451-24.12245-21.55152-71.17052-21.29193-83.29114.0418-1.95364 4.07765 6.62331 8.96845 19.0599 18.431 46.86727 37.37385 79.10213 59.15526 100.66389 6.57339 6.50715 12.99641 12.49223 14.27339 13.3002 3.74235 2.36798-3.70055 1.8507-24.23174-1.68412-10.16278-1.74975-18.79221-2.86688-19.17648-2.48265-1.23748 1.2375 17.91062 21.9978 30.08509 32.61817 13.97623 12.19215 19.10427 15.54173 41.06791 26.8248l16.57069 8.51265-17.22347-.0435c-9.47291-.024-17.83127.5724-18.57413 1.32525-3.41057 3.45668 26.83353 21.31148 51.94902 30.66848 23.14963 8.6247 29.25022 9.41677 79.14196 10.2756 29.34012.50505 46.23566.23782 45.22399-.71535-.88838-.83693-12.8808-5.35185-26.64983-10.03313-13.76912-4.68135-29.39562-10.54695-34.72558-13.0347l-9.69085-4.52325 34.72555-.1392c43.93558-.1761 205.90206 2.78138 219.02676 3.99938l9.8658.91552-.17498 5.95275-.17497 5.95268 8.88322 1.05165c4.8858.5784 17.10285 1.1235 27.14898 1.21132 10.2975.0901 17.8758.7905 17.3721 1.60568-1.7218 2.78587 4.4406 3.39967 34.3492 3.42157 16.5835.012 31.7621.45375 33.7301.98138l3.5782.95925-3.1003 6.07725-3.1005 6.07717-27.1276 1.08863c-105.1158 4.218-251.54753 6.5112-418.46093 6.55312l-181.36473.0457-3.3876 5.34983c-1.96333 3.10065-2.78481 5.94292-1.95388 6.76042.78854.7758 21.7845 2.16668 46.65769 3.09083 24.87318.92415 67.02839 2.49682 93.67823 3.49485 26.64985.99802 49.72618 2.49465 51.28076 3.32587 1.55457.8313 2.8265 2.49825 2.8265 3.70448.0 3.2043 3.63481 4.1223 17.49487 4.4184 6.81188.1455-12.54446.7983-43.01412 1.45057-30.46965.65228-57.92012 1.65885-61.00103 2.2368-4.20322.78855-6.60742 2.64413-9.63035 7.43273l-4.0287 6.3819h-23.93078-23.93079l-3.78025 8.07097c-2.07914 4.4391-3.45206 8.39925-3.05094 8.80035 1.08129 1.08128 94.49883 3.8913 171.02351 5.14448l67.02838 1.09762-4.53321 9.4401-4.53322 9.44018H584.10526c-106.81527.0-140.56323.46042-142.04091 1.93815-1.06599 1.06597-1.93817 5.7903-1.93817 10.49844.0 4.7081-.50023 8.5603-1.1116 8.5603-2.49359.0-31.09499-23.11362-40.83543-33.00027-12.50813-12.69585-15.04354-13.39492-13.5804-3.7443 2.13579 14.08733 8.68935 37.18447 15.04676 53.03047 3.60949 8.9967 6.56269 16.6443 6.56269 16.9948.0 1.9897-3.81094-.3435-13.81824-8.4598z" id="path17"></path><path style="fill:#fff;stroke-width:1.61514" d="m394.09464 1037.4132c-13.65598-10.6837-43.23456-39.68543-53.39888-52.35743-17.78879-22.17765-35.32089-52.9929-52.68596-92.6034-3.52579-8.04247-7.1756-15.3495-8.1107-16.23787-1.98377-1.88453-.82412 36.88785 2.04301 68.307.9879 10.82572 1.49677 20.58142 1.13083 21.6792-1.58289 4.7487-30.79746-56.22983-41.45412-86.5257-2.7677-7.86833-6.87787-21.58718-9.1337-30.48638-6.21694-24.52567-16.80633-79.21537-17.94387-92.67247-.55492-6.56464-1.70789-11.93571-2.56218-11.93571-.8637.0-1.08826-1.8526-.50583-4.17319.66494-2.64933.44832-3.80292-.59323-3.15922-.90235.55768-13.01601 16.06337-26.91924 34.45712-13.90323 18.39367-26.00178 34.16992-26.88567 35.05822-5.19086 5.217 8.02058-89.05832 24.76257-176.70276 2.61389-13.68371 4.42166-25.21032 4.01729-25.6147-.8178-.81779-4.55164 1.70658-26.46861 17.89483-8.27694 6.1135-15.4493 10.71513-15.93857 10.22586-.48927-.48926 1.70768-10.19168 4.8821-21.56091 10.04728-35.98442 15.28685-58.52268 29.34974-126.24951 4.12582-19.86991 7.887-37.13174 8.35818-38.35962 1.72783-4.50267-3.62451-1.93288-24.12424 11.58258-11.52482 7.59831-21.15552 13.59453-21.40156 13.32493-.24605-.26961 6.19588-14.5504 14.31539-31.73509l14.76274-31.24488 7.5451-.59451c4.1498-.32697 21.15886-3.5286 37.79789-7.11473 16.63905-3.58612 30.85669-6.52022 31.59478-6.52022.73808.0 3.27187 3.81577 5.63063 8.47949 19.09144 37.74746 51.36686 62.13846 87.10368 65.82554 28.70167 2.96124 64.23496-2.43718 91.67921-13.92841 11.32151-4.74046 14.38365-6.8135 13.10951-8.87509-.48681-.78767.34052-1.43213 1.83851-1.43213 6.34249.0 24.21361-10.81383 34.56954-20.91806 13.0815-12.76357 20.58471-25.87563 27.62404-48.27381 2.88663-9.18486 6.48846-18.23112 8.00405-20.10281l2.75564-3.40306 25.6512 6.17674c51.80879 12.47543 88.48308 26.85809 103.95431 40.76807 13.3994 12.04721 15.86207 21.70357 9.72101 38.11703-5.85319 15.64403-5.79777 15.62892-18.63383 5.08429-6.18484-5.08076-16.31236-11.67284-22.50561-14.64906-10.49246-5.04227-12.36506-5.4089-27.45742-5.37585-18.49061.0405-25.78905 2.48724-34.23511 11.47708-6.8963 7.3403-7.55004 12.1611-2.35384 17.35731 2.8567 2.85669 3.77399 2.89869 16.25056.74404 15.76439-2.72244 24.96391-1.45364 42.46606 5.85692 23.6903 9.89531 35.16431 22.93143 35.16431 39.95166.0 10.58582-4.10341 19.30789-12.23835 26.01339-8.74379 7.2074-17.39175 9.0281-37.52362 7.90011-19.28268-1.08041-30.47972-3.64169-48.23683-11.03398-19.04748-7.9295-26.37741-13.61454-32.513-25.21683-3.97732-7.52103-6.20974-10.09464-8.75637-10.09464-4.77681.0-16.85074 13.10218-18.91108 20.5216-2.14602 7.72796-2.17287 19.91276-.0508 23.08724 1.36403 2.04054 2.28306 1.67159 5.82594-2.33879 4.80057-5.43403 1.88709-6.38023 26.22029 8.51538 26.83357 16.42621 51.20604 23.06305 84.69442 23.06305 17.24619.0 23.7186-.73763 37.63336-4.28888 18.42319-4.70184 22.46482-4.45133 20.76942 1.28733-3.82729 12.95484-20.42749 26.01813-41.61179 32.74578-26.83251 8.52138-43.51029 7.2723-102.89774-7.70647l-39.57098-9.98066-35.53312-.0431c-35.44056-.043-88.27613 1.90619-92.87066 3.42622-1.66805.55185 2.85992 5.77421 14.53628 16.76554 16.81566 15.82911 16.91349 15.96453 11.57509 16.02275-6.34417.0691-27.32434-5.09783-40.34248-9.93557-10.69031-3.97268-23.15962-12.10434-39.333-25.65039-6.33321-5.3044-11.76272-9.39654-12.06559-9.09366-.30288.30288 2.06269 8.35497 5.25681 17.89356 3.19412 9.53857 5.38473 17.76564 4.86802 18.28233-1.51835 1.51836-18.66373-12.0516-31.23915-24.72468-19.732-19.88525-38.17956-47.39844-44.01171-65.64033-4.48967-14.04283-7.43023-2.47181-5.75952 22.66353 1.3293 19.99901 6.34527 49.84625 9.74384 57.98016 2.85531 6.83371 5.63576 5.65308 5.63576-2.39307.0-5.44858.95057-4.20574 11.28218 14.75121 21.97155 40.31446 36.98155 60.47689 71.01882 95.39723 35.99592 36.92982 67.54236 63.55951 97.63 82.41376 16.36274 10.25363 16.72024 10.0902-14.37895 6.5724-25.48879-2.88315-31.9648-3.00832-31.16282-.6024 1.68201 5.046 69.64909 42.78638 92.53822 51.38408 15.77247 5.92455 32.63699 9.75097 54.09735 12.27427 22.54471 2.6508 33.92788 3.01298 33.92788 1.0794.0-.7113-3.31382-2.90325-7.36404-4.87102-4.05023-1.96778-16.57396-9.56243-27.83052-16.87703l-20.46644-13.29937 32.67592-.43718c22.14397-.29632 32.67594-1.00207 32.67594-2.18955.0-.96382-1.83866-3.9375-4.08591-6.60817-7.66763-9.11243-42.75321-70.44371-42.75321-74.73461.0-.4738 5.88973 2.09985 13.08827 5.7192s13.4483 6.22061 13.88834 5.78056c.44005-.44005-1.39209-6.48458-4.07142-13.43233-9.69429-25.13811-21.49335-71.52996-21.20288-83.36598.0479-1.95365 4.25696 6.98671 9.35339 19.86747 18.78666 47.48173 35.73076 76.09394 59.05725 99.72546 17.02383 17.24648 17.38791 16.8603-11.02636 11.69543-9.69794-1.76273-17.91153-2.92613-18.25241-2.58525-1.1472 1.1472 18.61407 22.93845 28.91061 31.8804 14.43456 12.53565 18.97986 15.51667 40.4003 26.4969l18.99445 9.73657-16.83761-.0457c-20.76244-.0562-23.48492 1.62038-13.90186 8.56238 21.24342 15.38865 59.70125 30.08625 85.2177 32.568 18.83242 1.83165 84.49577 2.15175 84.49577.4119.0-.66735-10.35713-4.7085-23.01578-8.98043-12.65865-4.27192-28.83025-10.23142-35.93688-13.24342l-12.92113-5.4762 35.53312-.0799c46.35422-.10418 207.07449 2.87385 220.00067 4.07647l10.03222.93338-.3414 5.9349-.34132 5.9349 8.88322 1.05165c4.8858.5784 16.98225 1.1235 26.88073 1.21132 14.4042.12788 18.1833.6435 18.9273 2.58248.7834 2.0415 5.9998 2.42272 33.1511 2.42272 17.7218.0 33.3503.4332 34.7299.96263 2.2137.84945 2.1435 1.6782-.5963 7.04872l-3.1049 6.08618-26.3252 1.04647c-14.4788.57563-56.8513 1.7463-94.16105 2.60153-37.30973.85522-104.17665 2.41155-148.593 3.45847-52.35428 1.23405-129.42927 1.54455-219.14527.8829l-138.38818-1.0206-3.5738 5.78258c-3.87466 6.26932-2.71935 8.60782 4.27817 8.6595 1.84116.0135 21.82855.73065 44.41641 1.59345 22.58784.86272 62.87322 2.38867 89.52306 3.3909 26.64985 1.00222 49.72619 2.5023 51.28077 3.33352 1.55457.8313 2.82649 2.49825 2.82649 3.70448.0 3.20512 3.63671 4.12282 17.49488 4.41472 6.81188.14348-13.31682.816-44.73042 1.49453-37.52817.8106-58.69506 1.88782-61.72105 3.1413-2.53296 1.04917-6.06066 4.37407-7.83932 7.3887l-3.23395 5.48115h-24.00551-24.00552l-3.78026 8.07097c-2.07913 4.4391-3.46585 8.38545-3.08159 8.76968 1.06716 1.06717 97.70641 3.98932 171.86173 5.19675l66.22082 1.0782-4.53322 9.43905-4.53322 9.43905H585.2912c-92.67016.0-139.95684.55657-142.04092 1.6719-2.50362 1.33995-3.1241 3.42502-3.1241 10.49842.0 4.8546-.54511 8.8189-1.21135 8.8097-2.29843-.032-27.72605-20.36795-39.9669-31.96407-6.75096-6.3954-12.99978-11.628-13.88628-11.628-3.36735.0 5.94588 38.51397 14.06033 58.14517 9.54702 23.0969 9.51399 23.1744-5.02734 11.798z" id="path18"></path><path style="fill:#fff;stroke-width:1.61514" d="m712.52876 795.05752c-14.22324-.3813-18.9297-1.01722-18.34756-2.4792.43063-1.0815 2.03186-5.05529 3.55827-8.83072l2.77531-6.8643h74.91494 74.91488L855.35805 768l5.01338-8.8833h160.55247c109.289.0 160.5525.53228 160.5525 1.66703.0.91687-1.0532 4.18755-2.3404 7.26817l-2.3402 5.60108h-95.1389c-52.3264.0-99.54405.46042-104.92815 1.02322l-9.78923 1.02315-2.94487 10.2828-2.94488 10.2828-114.69524-.34725c-63.08235-.19102-123.30397-.5781-133.82577-.86018zm180.24094-1.47735c-1.11038-.44805-2.9274-.44805-4.03785.0-1.11038.44805-.2019.81465 2.01892.81465 2.22083.0 3.12938-.3666 2.01893-.81465zm17.68582-.8685c-1.5075-1.5075-2.36887-1.5075-3.87637.0-1.50743 1.50743-1.0767 1.93815 1.93822 1.93815 3.01493.0 3.44558-.43072 1.93815-1.93815zm38.05546-16.93815c-1.54426-.40425-4.45148-.41887-6.46058-.0322-2.0091.38633-.74565.71708 2.80762.735 3.55336.018 5.19713-.29819 3.65296-.70245zm-57.28148-2.28787c-1.27747-1.27748-2.6274-1.29143-4.76467-.0487-2.61435 1.51957-2.39446 1.73947 1.78252 1.78252 3.47468.036 4.28205-.4335 2.98215-1.7334zm30.61275.6426c-1.99875-.38498-5.26935-.38498-7.2681.0-1.99875.38497-.36345.69997 3.63405.69997s5.6328-.315 3.63405-.69997z" id="path19"></path><path style="fill:#fff;stroke-width:1.61514" d="m844.1121 618.7574c-1.85663-1.17601-1.73813-2.67552.72015-9.11237l2.94247-7.70474 90.4764 1.13704c49.76198.62537 90.73498 1.39565 91.05108 1.71173.3161.31608-.2832 3.43824-1.3319 6.93816l-1.9065 6.36347-41.26865 1.06191c-58.24943 1.49885-138.04673 1.27469-140.68305-.3952z" id="path20"></path><path style="fill:#fff;stroke-width:1.61514" d="m844.04265 618.71448c-1.95263-1.23587-1.85865-2.58475.63472-9.11345l2.92643-7.66288 90.5619 1.13811c49.809.62596 90.8204 1.39672 91.1365 1.7128s-.2833 3.43846-1.332 6.93863l-1.9067 6.36395-39.79513 1.05794c-57.09225 1.51776-139.53892 1.26555-142.22572-.4351z" id="path21"></path><path style="fill:#fff;stroke-width:1.61514" d="m388.9003 1033.0855c-22.94087-19.2606-48.68452-46.98408-61.73949-66.48768-10.63267-15.88477-26.43984-45.84285-42.15069-79.8849l-5.59056-12.11355-.49344 10.49843c-.2714 5.7741.6813 26.38357 2.1171 45.79875 1.43579 19.41525 2.27032 35.6406 1.85451 36.0564-2.33955 2.33955-31.15773-58.60523-41.37888-87.50813-8.75708-24.76297-24.27113-94.77352-26.84316-121.13565-.60669-6.21832-1.79235-12.17967-2.63481-13.24743-.84246-1.06778-1.00344-3.60593-.35775-5.64034 1.01411-3.19518.83621-3.41857-1.30636-1.6404-1.36419 1.13219-14.00657 17.39198-28.09418 36.1329-17.20368 22.88633-25.80135 33.12705-26.18495 31.18905-.79005-3.99142 2.34475-33.423 7.04845-66.17551 3.54804-24.70548 20.33665-121.25258 22.77185-130.95517.53963-2.15005.35724-3.89786-.40531-3.88403-.76254.0138-10.35183 6.61438-21.30951 14.66789-10.95768 8.05352-20.31818 14.24764-20.8011 13.76471-.48293-.48293 2.11434-11.62607 5.77169-24.76253 8.5737-30.79499 12.73519-48.5922 25.41229-108.67944 5.71626-27.09402 10.79535-50.53376 11.28688-52.08834.49154-1.55457.36347-2.82649-.2846-2.82649-1.81871.0-14.53577 7.63371-30.26692 18.16843-7.8477 5.2554-14.50762 9.31365-14.79983 9.01833-.29222-.2953 6.12863-14.60042 14.26854-31.78914l14.79983-31.25223 7.3612-.57267c4.04865-.31496 19.31174-3.18697 33.91798-6.38222 14.60623-3.19525 28.7895-6.20997 31.51839-6.69938 4.81271-.86313 5.20038-.4608 12.91884 13.40731 15.50042 27.85038 43.15257 50.90756 69.48614 57.9396 12.66126 3.38104 50.10863 3.39589 67.67011.0269 21.82881-4.1877 52.08349-15.68752 51.4047-19.53895-.16323-.92615 2.74402-2.58396 6.46056-3.68402 10.12426-2.99671 28.40741-16.22327 36.79043-26.61526 8.97022-11.11993 15.77509-24.72529 21.55336-43.0929 4.87125-15.48446 7.90827-20.99684 11.5681-20.99684 4.97264.0 55.15198 13.19908 73.66187 19.37589 25.84713 8.62526 44.33787 17.7557 54.54664 26.93427 13.3994 12.04721 15.86207 21.70357 9.72101 38.11703-5.8104 15.52968-5.73267 15.50788-18.60188 5.21637-6.20242-4.96007-16.28077-11.5325-22.39635-14.60539-10.25796-5.15431-12.2596-5.58637-25.84227-5.57806-18.89143.0115-25.53015 1.95048-34.27386 10.01016-7.59076 6.99692-9.06397 12.5185-4.87024 18.25377 2.08885 2.85666 3.14004 2.92675 16.73843 1.11625 17.10548-2.27743 25.69676-1.05289 42.83373 6.10525 23.63212 9.87118 35.10756 22.92226 35.10756 39.92804.0 10.58582-4.10341 19.30789-12.23835 26.01339-8.74379 7.2074-17.39175 9.0281-37.52362 7.90011-19.28268-1.08041-30.47972-3.64169-48.23683-11.03398-17.81272-7.41548-26.45406-13.78388-31.47735-23.1979-7.85341-14.71783-10.14111-15.3165-19.84173-5.19237-9.57657 9.99466-13.89481 28.30827-8.43543 35.77444 2.03071 2.77716 2.32461 2.70711 5.48167-1.30644 1.83475-2.33251 3.78226-4.24093 4.32779-4.24093.54555.0 9.23023 4.96117 19.29931 11.02481 32.43869 19.53473 54.57524 25.32365 92.284 24.13319 17.18858-.54263 26.01834-1.64484 36.71216-4.58271 17.50572-4.8093 19.5631-3.72982 14.16074 7.42995-6.54956 13.5296-26.42273 25.36248-51.66062 30.75977-20.87598 4.46446-36.26193 2.5924-90.44795-11.00511l-39.57098-9.93-33.11041-.0916c-44.59465-.12338-95.28686 2.32581-95.31565 4.60518-.0122.97008 7.0742 8.45334 15.74764 16.62948 15.73091 14.82894 15.75659 14.86588 10.38601 14.94006-2.96114.0409-11.18218-1.40607-18.26897-3.21549-27.03978-6.90387-37.38269-12.25511-59.37181-30.71796-6.45795-5.42234-12.18831-9.41221-12.73413-8.86641-.54581.54583 1.3999 8.29097 4.32382 17.21141 2.92392 8.92046 4.90653 16.6287 4.4058 17.12943-1.49294 1.49295-18.73751-12.18428-31.21011-24.75374-20.13338-20.28973-37.85772-46.74058-43.98464-65.64032-4.47673-13.80938-7.5036-2.41919-5.82349 21.91396 1.40007 20.27733 6.45074 50.75988 9.69824 58.53224 2.92851 7.00892 5.71321 5.93668 5.73209-2.20714.0127-5.46378.96738-4.20862 11.4362 15.03539 23.16972 42.59104 43.99271 69.32866 85.11015 109.28511 33.01014 32.07807 56.96565 51.63927 84.1784 68.73695 15.49924 9.73807 15.84613 9.59902-15.18652 6.08873-11.54827-1.30628-23.54069-2.361-26.64984-2.34383l-5.653.0315 5.11806 4.6581c11.72409 10.67055 74.04588 43.76722 95.82831 50.89072 24.88283 8.13743 80.7571 15.6783 80.7571 10.89916.0-.7113-3.31382-2.90326-7.36405-4.87103-4.05022-1.96778-16.57408-9.56242-27.83082-16.87702l-20.46676-13.29938 31.42026-.43987c17.28115-.24195 32.0692-1.08885 32.86237-1.88205.84604-.84608-.58478-4.00208-3.46176-7.63583-8.84142-11.16682-42.3075-69.47636-42.3075-73.71428.0-.55369 5.94285 1.8672 13.20635 5.37975 7.26349 3.51254 13.49653 6.09627 13.85119 5.7416.35465-.35465-1.92755-7.51288-5.07159-15.90715-9.85407-26.30956-20.54166-68.85771-20.29857-80.81025.0397-1.95364 4.34334 7.13853 9.56357 20.20485 19.33432 48.39404 38.66669 80.27862 62.71541 103.43548 13.33202 12.8376 13.27151 12.86865-14.9779 7.6785-9.65046-1.773-17.82255-2.94742-18.16018-2.60985-1.11863 1.1187 18.37865 22.65495 28.02235 30.95288 15.4575 13.3005 20.47362 16.63875 40.7911 27.14662l19.34804 10.00658-16.76271-.045c-20.88255-.0555-23.4105 1.60455-13.48108 8.85412 9.94766 7.26293 40.1198 21.53783 55.98815 26.48888 7.94771 2.47972 20.77559 4.83592 31.49527 5.78482 23.50382 2.08065 83.05948 2.15535 80.99585.10163-.8439-.83985-12.21727-5.21535-25.27405-9.7233-13.05681-4.50795-28.6833-10.33605-34.72555-12.95138l-10.9859-4.755 35.53311-.057c47.29527-.0759 207.32509 2.89793 220.00069 4.08825l10.03223.94215-.3414 5.9349-.34133 5.9349 9.69083 1.03613c5.32995.56977 17.42632 1.11487 26.88087 1.21132 13.6759.13943 17.38.67058 18.1197 2.598.7833 2.0415 5.9997 2.42273 33.151 2.42273 17.7218.0 33.3503.4332 34.73.96262 2.214.8496 2.1446 1.6761-.5911 7.03845l-3.0996 6.07575-23.9078 1.03418c-13.1492.5688-56.9777 1.74922-97.39662 2.62312-40.41893.87383-86.5716 2.11628-102.56153 2.76098-15.98992.64462-113.29665 1.20292-216.23722 1.24072l-187.16468.0683-3.0022 5.59778c-2.31408 4.31475-2.53761 5.8578-.97516 6.73215 1.11488.62392 23.09377 1.91925 48.84196 2.87857 117.81893 4.38945 139.06039 5.4117 141.70449 6.8196 1.55457.82778 2.8265 2.48498 2.8265 3.68258.0 2.89627 4.93207 4.06687 18.57413 4.40827 6.2183.15563-14.39607.83805-45.80968 1.51658-36.49158.78817-58.73046 1.90252-61.58763 3.08602-2.45959 1.0188-5.81367 4.3437-7.45353 7.3887l-2.98153 5.53643h-24.39133-24.39133l-3.78025 8.07097c-2.07914 4.4391-3.4753 8.36483-3.10259 8.72393 1.08885 1.04902 100.00524 4.062 171.88273 5.23545l66.22082 1.08105-4.53322 9.44115-4.53322 9.44115H586.3101c-85.68995.0-139.46645.59632-142.04092 1.57515-3.75724 1.42845-4.143 2.406-4.143 10.49842.0 4.9078-.54511 8.9169-1.21135 8.9092-1.91579-.023-28.00791-20.7118-41.22965-32.6926-6.61728-5.99617-12.44869-10.90222-12.95865-10.90222-2.9546.0 6.78692 39.32172 14.29677 57.70892 10.03787 24.5769 9.87975 24.7003-10.12299 7.9066z" id="path22"></path><path style="fill:#fff;stroke-width:1.61514" d="m989.67825 1008.2953c-5.9727-.7412-5.95268-.6902-3.23033-8.2362l2.01893-5.59615 75.10405 1.15612c41.3073.63593 75.5968 1.64895 76.199 2.2512.6021.60226.092 3.45723-1.1334 6.34423l-2.2282 5.2492-70.7404-.2584c-38.9072-.1422-73.1026-.5517-75.98965-.91z" id="path23"></path><path style="fill:#fff;stroke-width:1.61514" d="m388.9003 1033.0855c-23.62422-19.8343-48.77192-47.04835-62.23647-67.35033-11.51858-17.3679-30.43833-53.37907-40.88068-77.81085-2.56283-5.99625-5.33765-10.90222-6.16628-10.90222-1.68049.0-.38383 36.19717 2.38266 66.51337.94973 10.40753 1.45937 20.23215 1.13253 21.83243-1.21992 5.97315-26.18805-44.60438-39.1838-79.37393-10.33034-27.6384-24.30253-87.80655-28.44794-122.50477-.97715-8.17905-2.47441-16.18829-3.32724-17.79843-.85283-1.61014-1.07479-4.4267-.49322-6.25904 1.99316-6.27989-2.26226-1.37279-25.87404 29.83644-12.91385 17.06903-24.98798 32.85173-26.83138 35.07255l-3.35165 4.03785 1.01889-14.53627c2.15778-30.78412 13.74528-105.70093 26.62785-172.15732 1.77043-9.13304 2.90415-16.92033 2.51937-17.30511-.38478-.38479-9.8878 5.98095-21.11784 14.14609-11.23003 8.16512-20.80989 14.45404-21.28856 13.97537-.47867-.47868 2.05326-11.26788 5.62652-23.97599 8.53344-30.34871 38.06924-162.50392 36.67265-164.08837-.76058-.86289-6.8658 2.69868-26.38127 15.38992-10.10087 6.56876-18.57783 11.72806-18.83769 11.4651s6.24792-14.65098 14.46174-31.97338c10.62571-22.4089 15.64529-31.37341 17.39891-31.0729 1.35558.2323 16.9216-2.59149 34.59116-6.27509 17.66957-3.68358 34.0013-7.02691 36.29273-7.42961 3.7863-.6654 4.88847.55466 12.08586 13.37851 14.01038 24.96284 35.72855 44.72456 60.55867 55.10332 9.32057 3.8959 12.89969 4.42165 34.2875 5.03659 27.92575.80291 44.71108-1.2316 64.52757-7.82125 15.75798-5.24007 31.14906-12.53793 29.63514-14.05187-.54497-.54495 2.09117-2.03264 5.85807-3.30598 26.06508-8.81088 47.81093-34.78484 58.43517-69.7969 4.66256-15.36546 7.89149-20.98248 12.06171-20.98248 5.08353.0 53.91495 12.91386 73.24464 19.37013 46.08831 15.39385 67.49905 31.50006 67.49905 50.77608.0 5.87321-4.95973 20.8621-8.50949 25.71667-1.86791 2.55453-3.03167 2.0113-13.32379-6.21932-6.20242-4.96007-16.28077-11.53249-22.39635-14.60539-10.21973-5.13511-12.29506-5.58707-25.65488-5.58707-18.46439.0-25.9976 2.30116-34.6391 10.58117-7.35738 7.04961-8.71554 12.18978-4.67932 17.70963 2.11018 2.88585 3.08258 2.94815 16.55521 1.06064 16.88394-2.36543 25.62279-1.11711 43.00388 6.143 23.63212 9.87119 35.10756 22.92227 35.10756 39.92804.0 10.67036-4.10937 19.30899-12.47048 26.21509-8.70437 7.18967-16.11853 8.73487-36.79135 7.66779-19.67864-1.01576-30.74984-3.51532-48.73697-11.00336-17.89127-7.44816-26.31477-13.72953-31.98006-23.84738-6.47849-11.57018-9.12987-13.14108-14.92194-8.84091-7.72567 5.73573-13.92704 17.5155-14.62761 27.7858-.42942 6.29518.10384 10.13405 1.7042 12.26852 2.2579 3.01145 2.41576 2.97438 5.15835-1.21135 1.55604-2.37485 3.48231-4.31788 4.28059-4.31788.79828.0 9.47025 4.81516 19.27107 10.70036 21.83688 13.11267 36.9795 19.13598 56.69317 22.55103 21.01494 3.64046 51.92948 2.51653 72.01857-2.61831 18.36554-4.6943 20.29454-3.74083 14.91482 7.37217-6.54957 13.52959-26.42274 25.36248-51.66063 30.75976-20.80669 4.44964-39.57181 2.12369-93.67823-11.61153l-36.34069-9.22527-29.07255-.14076c-38.68621-.18731-97.57836 2.36754-98.76311 4.28452-.52098.84296 6.2245 8.41057 14.98996 16.8169l15.93718 15.28425-8.88328-.78728c-11.04869-.97919-34.85867-7.86503-45.84146-13.25733-4.54619-2.23206-15.52545-9.86438-24.39835-16.96072-8.87291-7.09632-16.383-12.15108-16.6891-11.23279s1.75938 8.6212 4.58996 17.11759c2.83057 8.49638 4.75893 15.83553 4.28525 16.30921-3.29673 3.29673-37.41275-28.71173-52.78009-49.51953-9.37914-12.69961-22.60469-36.86734-22.60469-41.3067.0-1.04863-1.03875-2.76868-2.30833-3.82234-1.92565-1.59815-2.49182-.69215-3.41515 5.4651-1.33456 8.89948 1.53615 43.24251 5.0275 60.14522 4.71194 22.81199 9.04896 30.40346 10.09314 17.66699.50779-6.19375.64801-6.02427 11.81485 14.27989 6.21553 11.3014 14.41997 25.38769 18.23211 31.30284 30.74274 47.70241 101.21326 115.91838 152.716 147.83018 14.17341 8.78205 14.02849 8.82338-17.72479 5.0571-11.1041-1.317-22.64042-2.4009-25.63625-2.40862-4.89072-.0127-5.2206.2601-3.23028 2.6703 3.54076 4.28782 22.04931 15.6207 47.79733 29.2665 32.51053 17.22975 48.93746 24.02565 69.07048 28.57485 18.33575 4.14315 50.30458 7.98412 58.18753 6.99112l4.82696-.60802-5.45156-3.17048c-7.22918-4.20435-42.45131-26.6586-47.27717-30.13942-3.62656-2.61578-2.33976-2.74568 28.00249-2.82653 17.49205-.0465 32.18526-.702 32.65156-1.4565.46631-.7545-1.22952-4.2069-3.76853-7.67197-10.17345-13.884-42.11263-69.93081-42.11263-73.89912.0-.5968 5.06956 1.46788 11.26569 4.58821 16.61088 8.36511 17.01081 8.09058 11.14751-7.65163-9.77847-26.25387-20.90041-70.29915-20.74921-82.17138.0269-2.10921 4.96218 8.15751 10.96741 22.81492 19.60246 47.84531 34.34861 72.53611 57.62728 96.49055 16.76328 17.24992 17.21565 16.77495-11.30599 11.87055-9.77162-1.68023-17.98652-2.84693-18.25532-2.5926-.97831.92557 20.6189 24.44512 29.01366 31.59622 14.82231 12.62625 20.0133 16.04505 40.2694 26.5212l19.68676 10.1817-17.10586-.21997c-20.21597-.25988-23.55658 1.3626-15.38565 7.4724 8.15024 6.09442 28.0784 16.51335 42.39003 22.16264 25.33192 9.99938 34.70247 11.40788 82.9201 12.46373 29.4483.64477 44.01262.41895 44.01262-.68243.0-.9054-10.80682-5.36452-24.01515-9.90922-13.20835-4.54463-29.01655-10.4925-35.12935-13.21748l-11.11418-4.95457 86.41008.90683c97.42448 1.02247 154.236 2.2008 169.33868 3.51232l10.24725.8898-.99818 5.32073c-.77887 4.15147-.38377 5.55645 1.79775 6.3936 1.53773.5901 13.71923 1.52129 27.07005 2.06932 16.95828.69615 24.92388 1.64617 26.42998 3.15232 1.6493 1.64926 9.5 2.15596 33.4037 2.15596 17.1864.0 32.3767.4332 33.7563.96262 3.9885 1.53053-2.6874 13.57275-7.5263 13.57635-1.9807.0-25.7691.70935-52.8631 1.57297-154.16965 4.91445-295.75547 6.8937-430.86762 6.02318l-139.33455-.89768-3.17238 6.2184-3.17238 6.21833 4.41228.88177c2.42676.48495 24.03627 1.62045 48.02112 2.5233 23.98486.90285 64.66863 2.44013 90.40836 3.4161 25.73975.97598 48.08928 2.46473 49.66562 3.3084 1.57635.8436 2.86609 2.5656 2.86609 3.82658.0 2.59117 5.77464 3.86737 20.18927 4.46205 5.32997.21982-15.93237.90667-47.24963 1.52617-61.92579 1.22513-63.28967 1.43318-69.01272 10.52956l-3.25815 5.1786-24.43203.0622-24.43201.0623-3.54154 8.23252c-1.94786 4.5279-3.30928 8.4519-3.0254 8.72003.89474.8451 107.68868 3.9615 174.19937 5.08342l63.85358 1.07708-3.63903 7.4214c-2.00147 4.08172-3.66176 8.307-3.68954 9.38947-.0392 1.52858-31.63216 2.0697-141.4348 2.42272-158.32425.50903-144.4269-.73875-144.4711 12.97193-.0136 4.2196-.50513 7.672-1.0923 7.672-2.01997.0-25.50191-18.394-39.46907-30.91698-7.69395-6.8985-14.32845-12.20325-14.74332-11.78842-2.50198 2.502 6.71909 37.9009 15.80452 60.6721 3.0566 7.661 5.55746 14.384 5.55746 14.9401.0 2.7255-4.31622.01-17.3079-10.9003z" id="path25"></path><path style="fill:#fff;stroke-width:1.61514" d="m388.9003 1033.0855c-23.73218-19.9249-48.77297-47.04303-62.1911-67.35033-12.151-18.38962-20.23469-33.34927-37.90083-70.13895l-9.30695-19.38172-.56324 8.0757c-.30978 4.44165.64179 24.67042 2.11459 44.95282 1.4728 20.2824 2.3192 37.2357 1.8809 37.674-2.19311 2.19308-28.11571-51.57322-39.05852-81.01162-6.77153-18.2169-14.69251-48.0417-21.01879-79.14203-4.14753-20.38942-10.45365-57.65166-11.00196-65.00943-.14894-1.99874-.75381-3.63407-1.34414-3.63407-.59033.0-11.85595 14.23588-25.0347 31.6352-33.11344 43.71833-29.5355 39.73035-29.57845 32.96835-.0682-10.7355 6.26007-60.6731 12.0756-95.29125 6.1887-36.83958 16.35717-93.13257 18.01901-99.75384.9187-3.66039.72496-3.8187-2.53686-2.07302-1.94234 1.0395-11.55156 7.718-21.35384 14.84112-9.80227 7.1231-18.20229 12.57113-18.6667 12.10671-.46441-.46441 1.76854-10.51702 4.96212-22.33914 8.51078-31.5056 15.79657-62.59951 26.79757-114.36542 5.38031-25.31735 10.18517-47.30347 10.67748-48.85805.4923-1.55457.23601-2.82649-.56953-2.82649-.80554.0-11.11628 6.21268-22.91276 13.80597-11.79647 7.59328-21.66396 13.5895-21.92775 13.32492-.26378-.26458 6.18221-14.56203 14.32443-31.77212l14.80402-31.29107 6.80732-.59976c3.74403-.32987 19.00712-3.18729 33.91798-6.34983 14.91087-3.16254 29.3667-6.15137 32.12408-6.64186 4.79552-.85302 5.28089-.41041 11.16705 10.18349 16.08057 28.94173 38.22468 49.12839 65.00112 59.25527 8.34624 3.15655 12.31359 3.49934 39.95406 3.45212 28.28187-.0483 31.92651-.39465 46.48943-4.41772 18.51982-5.11619 40.6295-14.88066 39.14245-17.28678-.56516-.91445.008-1.66263 1.27347-1.66263 1.26556.0 7.3707-2.53625 13.56695-5.63612 20.67558-10.34356 37.53588-31.05708 46.54758-57.18544 7.39711-21.44702 9.58213-26.44028 12.32324-28.16142 3.27666-2.0574 41.51381 7.05367 73.34357 17.47617 48.22454 15.79087 70.15243 31.91659 70.15243 51.58993.0 5.89557-4.96391 20.86782-8.53995 25.75834-1.91192 2.61472-2.90368 2.17995-12.46855-5.46592-5.71528-4.56863-15.08913-10.93371-20.83078-14.14462-9.14498-5.11417-12.12922-5.92673-24.06821-6.55336-19.54589-1.0259-29.54249 1.59756-38.3899 10.07486-7.6878 7.36621-9.19865 13.19588-4.60932 17.7852 2.99983 2.99985 5.95732 3.12208 19.30399.79786 16.45121-2.86486 36.59633 2.45055 55.4357 14.62702 13.10187 8.46814 18.50371 16.80342 19.33331 29.83206.77146 12.11546-2.86453 20.54585-12.17305 28.22437-7.70637 6.35692-14.29376 8.18153-29.52321 8.17752-23.67737-.006-44.57697-5.17176-66.45936-16.42602-13.69463-7.04325-16.43699-9.53471-22.35444-20.30919-5.62226-10.23698-8.10064-11.44852-14.33642-7.00826-11.1426 7.93424-18.72865 31.6014-12.69398 39.60306 2.272 3.01256 2.42842 2.97593 5.17203-1.21135 1.55604-2.37485 3.59311-4.31788 4.52679-4.31788.93367.0 6.84629 3.20675 13.13916 7.12612 18.20577 11.33906 36.57742 19.90391 51.1951 23.8671 11.49242 3.11585 17.5865 3.67718 40.19598 3.7025 22.97063.0257 28.82448-.51591 42.40161-3.92313 19.22796-4.82534 21.13254-3.94902 15.71958 7.23265-6.54957 13.5296-26.42274 25.36248-51.66063 30.75977-20.53254 4.391-38.87322 2.18308-92.87066-11.18016-19.54321-4.83653-41.19388-9.26575-48.11259-9.84269-13.73659-1.14549-107.883 1.91047-113.88268 3.69659-3.42727 1.02032-2.50004 2.32193 12.11356 17.0044l15.82493 15.89949-8.88328-.87205c-10.55529-1.03619-34.12477-7.73631-44.69828-12.70644-4.14516-1.94846-15.22607-9.53974-24.62424-16.86951-9.39816-7.32976-17.29754-12.73953-17.55416-12.0217-.25663.71783 1.79735 8.37269 4.56438 17.01081 2.76704 8.63812 4.6255 16.11115 4.12991 16.60673-1.63052 1.63053-14.31045-8.23033-28.53482-22.19079-18.45951-18.11703-32.48856-37.36001-42.39056-58.14512-7.80744-16.38845-8.12746-16.79598-9.51251-12.11356-4.02669 13.61292 6.57116 87.80897 12.27531 85.94013.98437-.3225 2.0235-2.97264 2.30916-5.88921.47606-4.86034 1.58474-3.39981 13.28697 17.50387 24.11919 43.08405 37.62443 60.59807 77.38814 100.35946 32.35132 32.34942 57.07091 53.14652 84.7394 71.29292 21.99509 14.42557 22.45862 14.00317-11.18942 10.197-11.54827-1.30628-23.10823-2.38231-25.68882-2.39116-4.5829-.0158-4.62431.0585-1.78154 3.1998 8.10837 8.95965 79.55082 46.77915 100.95931 53.44493 21.49952 6.69405 75.91168 13.7193 75.91168 9.80107.0-.98482-3.08896-3.36375-6.86435-5.28637-3.7754-1.9227-15.938-9.3873-27.028-16.58805l-20.16366-13.0923 31.74332-.80753c17.45883-.44415 32.12894-1.1952 32.60025-1.66897.47132-.47378-2.41768-5.56148-6.42001-11.30603-10.85736-15.5835-39.40068-66.35565-39.40068-70.0849.0-.42256 5.47854 1.83346 12.17451 5.01337 6.69597 3.1799 12.67316 5.28296 13.28265 4.67347.60951-.60949-.94502-6.69943-3.45447-13.53318-9.61977-26.19656-20.4705-69.32444-20.3387-80.83929.0286-2.50046 4.59241 6.76635 11.02325 22.38274 19.88191 48.28039 34.75027 73.07857 58.34658 97.31322 15.8845 16.31422 16.29817 15.91987-11.67735 11.13022-9.54953-1.63492-17.36277-2.40165-17.36277-1.70377.0 2.11672 16.34265 20.11162 26.64984 29.34412 12.89487 11.55045 22.33194 17.8401 43.60883 29.06438l17.76656 9.37245-10.49842-.2676c-21.47428-.54743-26.12199-.0345-25.55691 2.82195 1.44676 7.3131 49.80084 30.14617 74.81874 35.32987 14.91483 3.0903 97.71609 5.62673 97.71609 2.99325.0-.8352-8.17665-4.33087-18.17032-7.76812-20.41352-7.02105-50.05155-18.48203-51.28079-19.83015-1.56582-1.7172 231.46431 1.75597 254.94119 3.7998l10.24725.89205-.99818 5.32072c-.77887 4.15148-.38377 5.55645 1.79775 6.3936 1.53773.5901 13.72898 1.51058 27.09165 2.04563 14.84428.5943 24.74298 1.6635 25.44548 2.74852.7542 1.1649 11.8681 1.9206 32.3029 2.19645 17.1342.2313 32.7952.792 34.8023 1.24598l3.6493.8253-3.171 6.21562-3.171 6.21555-23.8978.99578c-13.1438.54772-39.161 1.39755-57.8159 1.8885-18.65485.49102-62.62713 1.7235-97.71603 2.73885-35.08897 1.01535-150.19492 1.92472-255.79107 2.0208l-191.99294.17475-3.03639 5.91855-3.0364 5.91855 5.25015.88035c4.21623.70695 168.59236 7.34212 181.89016 7.34212 1.98143.0 5.45201 1.99875 7.71243 4.44165 3.64502 3.9393 5.60007 4.5477 17.28566 5.37968 8.81942.62797 1.42731 1.0653-22.35733 1.32285-19.54322.21157-46.24707.9285-59.3419 1.59315-21.86374 1.10977-24.13757 1.5174-27.83344 4.98945-2.21356 2.07952-4.44953 5.11965-4.96883 6.75577-.86297 2.71898-3.07029 2.97488-25.66412 2.97488h-24.71996l-3.3338 7.6719c-1.83361 4.21957-2.83478 8.16075-2.22484 8.7582.60994.59745 29.0913 1.86487 63.29193 2.81655 34.20063.9516 74.53881 2.09227 89.64039 2.53477 15.10157.4425 40.29273.81038 55.98035.8175l28.52295.0127-3.89204 8.06775c-2.14061 4.4373-3.89202 8.75145-3.89202 9.58695.0.9333-54.10138 1.71503-140.27016 2.02688-115.96037.4197-140.73285.89182-142.94006 2.72437-1.81521 1.50713-2.66991 4.73858-2.66991 10.09468.0 4.3329-.4804 7.878-1.06757 7.878-2.01997.0-25.50191-18.39403-39.46907-30.917-7.69395-6.8985-14.3473-12.18443-14.7852-11.7465-2.52978 2.52975 9.30945 46.3824 17.56819 65.0727 6.7598 15.2979 5.17193 15.2505-13.47223-.4026z" id="path26"></path><path style="fill:#fff;stroke-width:1.61514" d="m388.90674 1033.135c-7.8066-6.524-22.25104-20.2057-32.09877-30.4037-26.52842-27.47188-41.45268-51.15927-69.53606-110.3654-6.81568-14.36895-7.73277-15.61665-8.31024-11.30595-.56367 4.2078.9974 34.55212 3.86374 75.1041.81032 11.46405.71198 11.94037-1.83403 8.8833-4.15246-4.98608-27.0801-52.90725-34.38926-71.87723-11.71701-30.41002-22.61203-74.36265-30.22656-121.93987-6.19684-38.71917-2.31858-38.9243-33.10372 1.75103-23.58993 31.1685-26.66329 34.63642-27.13477 30.618-1.4012-11.94233 12.42671-106.09146 25.50009-173.62054 2.65399-13.70885 4.57641-25.17422 4.27206-25.47856-.30435-.30436-9.79544 6.07332-21.09133 14.1726-11.29588 8.09929-20.91138 14.35256-21.36778 13.89617-.45639-.45642 2.50821-13.41656 6.58801-28.80033 8.8226-33.26751 15.57383-62.55191 26.74168-115.99575 4.5479-21.76404 8.6147-40.56042 9.03732-41.76973 1.16304-3.32799-2.07401-1.65126-24.44534 12.66217-11.13746 7.12588-20.53056 12.95614-20.87355 12.95614-.99523.0 3.21213-9.45646 16.94845-38.0933 7.35481-15.33296 13.54247-26.33734 14.48256-25.75634.90275.55793 16.37219-2.03025 34.37656-5.7515 18.00436-3.72126 34.57956-7.0867 36.83377-7.47876 3.68765-.64137 4.97831.77341 12.87357 14.11157 19.09755 32.26317 46.23377 53.21617 75.52672 58.31732 15.81538 2.75413 52.79915 1.00323 70.63049-3.34382 15.84174-3.862 42.48172-14.68738 41.18612-16.73631-.44417-.70243 3.4334-3.00081 8.61682-5.1075 12.55357-5.10215 20.32174-10.67873 31.41565-22.55252 11.10349-11.88404 17.99929-24.39789 24.62807-44.69264 5.66786-17.3528 8.51669-22.4367 12.57267-22.4367 5.26651.0 53.17207 12.63006 72.81097 19.19625 46.6359 15.59252 67.99998 31.59987 67.99998 50.94996.0 5.87573-4.9602 20.86274-8.51291 25.72137-1.87507 2.56429-3.01274 2.0578-13.32381-5.93168-6.20053-4.80446-15.63457-11.1973-20.96454-14.20631-9.45692-5.33885-10.13921-5.47092-28.26499-5.47092-17.35306.0-19.07072.29192-26.12792 4.4405-13.36546 7.85688-17.72678 16.94767-11.03572 23.003 2.96363 2.68204 4.32476 2.75037 17.4141.87421 12.33785-1.76844 15.56867-1.69141 24.64629.58763 12.57708 3.15763 30.86129 11.91382 38.97085 18.66289 16.86241 14.0335 16.9539 39.42828.19189 53.25512-7.70637 6.35692-14.29376 8.18153-29.5232 8.17753-23.65665-.006-44.57757-5.17232-66.40464-16.39757-13.73121-7.06168-17.24539-10.24759-22.70439-20.5835-5.25744-9.95426-7.82324-11.18998-14.0412-6.76241-9.87478 7.03147-16.92684 25.2012-14.12063 36.38202 1.24325 4.95351 5.03236 5.42304 7.4412.9221.95084-1.77666 2.51081-3.23029 3.4666-3.23029.95577.0 7.82283 3.67555 15.26013 8.16789 18.62973 11.25289 37.95432 19.94175 52.33951 23.53328 19.05577 4.75764 58.68842 4.29182 79.54989-.93498 19.53945-4.89557 21.77319-3.8436 15.78938 7.43599-7.16056 13.49777-27.1821 25.48628-50.66392 30.33658-22.29678 4.60553-38.70991 2.66794-94.74475-11.18469l-36.34069-8.98396-27.45741-.13076c-37.39344-.17808-95.3135 1.95784-97.96965 3.61286-1.60151.99787 1.83782 5.37944 13.1428 16.74338l15.31194 15.3918h-5.38796c-6.53725.0-21.01807-3.52407-37.44865-9.11354-11.67772-3.97261-22.59773-10.70576-42.56024-26.24218-5.01093-3.89991-9.44737-6.75414-9.85877-6.34273-.4114.4114 1.47327 7.85418 4.18815 16.53951 2.71488 8.68533 4.53754 16.19011 4.05034 16.67729-1.73681 1.73683-14.89753-8.71508-30.01029-23.83339-18.42648-18.43326-32.75867-38.28676-41.25072-57.14211-3.4461-7.65157-6.76066-13.91194-7.36569-13.91194-5.7758.0.21852 66.18446 7.59279 83.83357 2.5509 6.10514 4.90416 5.351 5.76344-1.84699.44296-3.71066 2.27818-1.21173 11.38798 15.50646 5.97295 10.9615 15.10916 26.39814 20.30268 34.30364 30.03839 45.72409 93.8346 107.78153 145.12152 141.16625 19.83202 12.90945 20.59075 12.47587-15.15369 8.65957-13.61242-1.45327-25.2727-2.115-25.91175-1.4703-2.81076 2.83538 23.53185 19.05855 62.63621 38.57475 14.65741 7.3152 31.73754 14.93483 37.95583 16.93245 21.30064 6.84285 76.71925 14.31023 76.71925 10.33755.0-.92137-4.67758-4.24635-10.39462-7.38885s-17.89117-10.659-27.05363-16.70347l-16.65901-10.98983 32.30284-.9279c17.76656-.51037 32.50669-1.05547 32.75584-1.21132.24916-.15593-3.08889-5.55285-7.41791-11.99325-12.10443-18.0081-39.06663-66.11874-39.06663-69.70947.0-.40854 5.5189 1.8071 12.26422 4.92363 6.74534 3.11652 12.69848 5.23218 13.22921 4.70145s-.71994-5.49006-2.77926-11.02073c-9.47474-25.44606-21.20331-71.73674-21.02718-82.99085.0479-3.05516 3.02501 2.70487 8.89671 17.21269 19.88198 49.12443 37.03146 78.10646 60.45011 102.15865 15.6678 16.09163 15.79408 15.9078-8.04328 11.70885-18.30781-3.22492-21.58527-3.18735-19.67543.22538 2.91803 5.21422 30.35863 31.76392 39.8647 38.57047 5.32998 3.81638 18.04922 11.35448 28.26499 16.75125l18.57413 9.81248-18.17034.027c-20.54908.0308-22.60803 1.5063-11.94513 8.56267 19.21989 12.7191 53.72859 26.9868 72.91674 30.14753 16.36296 2.69542 94.93546 4.20202 92.66604 1.7769-1.00088-1.06958-10.8129-5.08695-21.80445-8.92755-20.58161-7.19145-45.28114-16.7358-46.53901-17.9835-1.80846-1.79385 233.9217 1.41262 254.60281 3.4632l10.00417.99187.49425 5.9379.49418 5.9379 10.49842 1.02263c5.7741.56242 17.52428 1.10752 26.11145 1.21132 10.9152.132 15.815.79628 16.2843 2.2077.4994 1.5024 8.696 2.13045 32.0336 2.4546 17.2493.23955 33.0046.80025 35.0117 1.2459l3.6493.8103-3.1903 6.25335-3.1902 6.25335-24.6862 1.02623c-112.03955 4.65727-323.62096 7.86487-465.35126 7.05472-112.32978-.64207-135.7751-.40222-137.58505 1.40775-2.70468 2.70465-4.6481 8.52195-3.29802 9.87203 1.15391 1.15387 29.71501 2.79607 75.2012 4.3239 17.3224.58185 47.12177 1.73947 66.22083 2.57265 19.09905.8331 37.48581 1.52167 40.85945 1.53007 4.58675.0113 7.15975 1.1223 10.201 4.40385 3.5788 3.8616 5.65408 4.5102 17.28565 5.40293 9.51868.7305 3.72491 1.10527-20.69942 1.33882-18.65489.17843-45.39718.921-59.42732 1.65023-23.39411 1.21597-25.836 1.63275-29.44859 5.02657-2.16657 2.03543-4.3641 5.0394-4.8834 6.67553-.86294 2.7189-3.07003 2.97487-25.65469 2.97487h-24.71052l-3.3429 8.04383c-1.83858 4.42417-2.844 8.34367-2.23425 8.71005.60972.36637 30.90798 1.4709 67.32943 2.45437 36.42145.98355 76.39622 2.14658 88.83281 2.58458 12.43659.43792 35.81072.80145 51.9425.80775l29.33052.0113-3.89204 8.06775c-2.14061 4.4373-3.89202 8.745-3.89202 9.57262.0.91403-54.69327 1.70978-139.30599 2.02688-109.74758.41122-140.07708.96247-142.94006 2.59777-3.00198 1.71465-3.63408 3.47055-3.63408 10.0946.0 4.4104-.5451 8.0119-1.21135 8.0034-2.07255-.026-22.27357-15.74675-37.47353-29.16147-7.98729-7.04918-14.90656-12.81668-15.37614-12.81668-3.57895.0 3.95039 32.10655 13.02636 55.54705 7.52138 19.4255 7.78659 20.3645 5.75123 20.3645-.95822.0-8.12942-5.3378-15.93601-11.8618z" id="path27"></path><path style="fill:#fff;stroke-width:1.61514" d="m394.64346 1038.305c-4.7435-3.6806-19.18924-17.3083-32.10166-30.2839-31.75892-31.9144-46.73839-54.48453-71.1309-107.1757-5.65451-12.2145-10.84748-22.20818-11.53993-22.20818-1.79586.0-1.50125 12.36465 1.17375 49.2618 1.28805 17.7666 2.33472 34.1199 2.32594 36.34073-.0157 3.9633-.0626 3.97822-2.54276.8076-3.89211-4.9758-26.99195-53.05965-33.75276-70.25873-11.29309-28.72875-23.15712-76.1793-29.88365-119.52045-4.19241-27.01302-6.04745-34.30107-8.53001-33.51261-1.05782.33594-13.18893 15.40169-26.95801 33.47939-22.53804 29.59065-25.08882 32.4066-25.57742 28.23645-1.37104-11.70195 12.4518-105.46727 26.1823-177.60389 2.02902-10.65994 3.06964-19.62955 2.3125-19.93248-.75714-.30293-10.12385 5.77222-20.81491 13.50032-10.69106 7.72811-19.83995 13.64945-20.33087 13.15855-.49092-.49093 2.0778-12.01718 5.70827-25.6139 8.76787-32.83726 14.43756-57.10197 24.45737-104.67085 4.58434-21.76404 9.31322-43.8084 10.50862-48.98747 2.65356-11.49649 4.02897-11.8204-22.67234 5.33937-11.0918 7.12822-20.48989 12.9604-20.88465 12.9604-1.08908.0 2.78698-8.76761 16.605-37.56033l12.59186-26.23778 5.98744-.4052c3.29308-.22285 18.51133-2.99545 33.81833-6.16132 15.30699-3.16586 30.03314-6.14163 32.72477-6.61281 4.71369-.82516 5.21915-.30255 13.72871 14.19473 12.97643 22.10725 30.54794 39.21318 50.23933 48.90817 17.68471 8.70702 26.58487 10.64426 48.90248 10.64426 30.01418.0 49.86573-3.85409 76.45643-14.84366 6.44038-2.66172 11.70978-5.74684 11.70978-6.85582.0-1.10899 2.78135-2.84965 6.18077-3.86815 16.02206-4.80032 38.70489-25.2189 48.37305-43.54435 2.67607-5.07232 7.12404-16.07504 9.88438-24.45047 5.83551-17.70615 8.72575-22.72777 13.08121-22.72777 5.3466.0 52.57536 12.52681 73.25738 19.43057 27.132 9.05678 45.15088 18.09103 55.38212 27.76732 13.35092 12.62675 15.03271 23.67357 6.45614 42.40725-3.92609 8.57574-4.50148 9.13657-7.28013 7.09617-35.65917-26.18505-39.57589-27.88692-61.8195-26.86131-13.17449.60745-16.70685 1.38088-22.91022 5.0163-12.62298 7.39756-17.2236 17.02438-10.87959 22.76563 2.8452 2.57486 4.4113 2.65361 17.4204.87583 12.3547-1.68836 15.71076-1.59258 24.73494.70588 13.65264 3.47733 29.56041 10.9755 38.0583 17.93889 17.76072 14.55357 18.27412 39.77118 1.09815 53.93949-7.70637 6.35692-14.29376 8.18153-29.52321 8.17752-22.34001-.006-43.82109-5.02919-63.79811-14.91904-14.20609-7.03291-19.71976-11.73419-25.24606-21.52622-2.7183-4.81657-5.57847-9.15054-6.35592-9.63103-2.59506-1.60384-10.5538 3.80633-14.96653 10.1739-7.56368 10.9144-9.91319 32.69332-3.52694 32.69332 1.28431.0 3.11307-1.45363 4.06391-3.23028.95083-1.77666 2.5108-3.23029 3.4666-3.23029.95577.0 7.73457 3.62223 15.06399 8.0494 34.9299 21.09867 52.24549 26.2626 88.87635 26.50508 22.06143.14604 28.27243-.39982 42.45252-3.73098 20.40771-4.79416 22.50707-3.85796 16.54605 7.37868-7.16056 13.49777-27.1821 25.48627-50.66392 30.33658-22.6708 4.68278-37.21016 2.96845-94.68125-11.16384l-36.27723-8.92066-39.63444.12085c-45.20353.13782-84.87837 1.7834-86.69061 3.59564-.67752.67752 5.63548 8.19975 14.02887 16.71609l15.2607 15.48424-9.79073-.94289c-5.38492-.51859-17.74075-3.54292-27.45742-6.72072-17.7747-5.81314-26.41375-10.83384-48.31263-28.07753-4.95395-3.90086-9.32259-6.77705-9.70809-6.39155-.3855.38548 1.50437 7.82307 4.19971 16.52793 2.69534 8.70486 4.49603 16.23162 4.00153 16.72613-1.74935 1.74934-14.86698-8.65793-30.0236-23.8201-18.42648-18.43326-32.75866-38.28676-41.25071-57.14211-3.4461-7.65157-6.74865-13.91194-7.33902-13.91194-5.64699.0.29111 66.42203 7.47525 83.61609 2.68685 6.43053 5.55881 5.53881 5.61353-1.74297.0352-4.69205.6348-4.06839 6.6905 6.95968 23.32739 42.48166 37.07501 61.97819 65.59423 93.02401 32.05969 34.89999 69.10716 67.21681 102.73751 89.61871 22.10679 14.72588 22.83439 14.19765-14.05448 10.20398-13.16-1.42478-24.42561-2.08755-25.0347-1.47285-4.14562 4.18372 72.40468 46.17225 100.1664 54.94215 21.98 6.9435 77.19935 14.83065 77.19935 11.02665.0-.80423-4.906-4.27478-10.90221-7.71233s-18.17035-11.04225-27.05363-16.89922l-16.15142-10.6491 32.30284-.96075c17.76656-.52838 32.50669-1.07355 32.75584-1.21133.24915-.13785-3.08704-5.52-7.41375-11.9604-13.28218-19.77075-39.0708-65.70951-39.0708-69.59913.0-.46922 5.51891 1.69677 12.26423 4.81329 6.74533 3.11653 12.63151 5.29917 13.08037 4.85029.44888-.44888-2.21172-9.74389-5.91247-20.65558-8.07961-23.82292-15.6985-54.29993-17.01589-68.0668l-.95978-10.02976 9.04995 21.80442c20.87798 50.30218 36.80965 77.1847 60.24898 101.662 15.9925 16.70062 16.26843 16.31647-8.6671 12.06607-23.18276-3.95167-23.25882-3.71647-4.89233 15.1272 19.38873 19.89248 29.11617 27.17273 53.85477 40.30628l18.9682 10.07002-17.26745.0248c-26.0784.0367-25.31144 2.77718 5.15389 18.4128 20.2039 10.36913 36.94083 16.59293 52.92135 19.67925 14.75334 2.84933 94.05659 5.20148 94.05659 2.7897.0-.90847-6.72308-4.02217-14.94008-6.91935-8.21704-2.89717-23.66184-8.62942-34.32177-12.73837l-19.38171-7.47075 29.88013-.51803c35.1584-.6096 220.94368 2.64165 229.75393 4.02068 5.58945.87495 6.05677 1.36095 6.05677 6.29932.0 2.94323.58793 5.71463 1.3065 6.1587.71858.44408 12.52928 1.31993 26.2461 1.94625 15.20098.69405 25.38918 1.83503 26.09118 2.92193.7511 1.16302 12.9231 2.064 35.0001 2.59072 18.6167.44415 34.1989 1.17098 34.6272 1.61513.4283.44415-.4269 3.35145-1.9005 6.46057l-2.6791 5.65298-16.1407 1.04745c-17.8686 1.15957-119.67913 3.8643-234.99238 6.24292-41.3073.85208-142.83829 1.55903-225.62453 1.57103l-150.52039.0217-2.5143 3.58965c-3.96019 5.65395-3.03588 7.4481 4.35006 8.44372 3.77539.50895 36.66372 1.9992 73.08517 3.3117 111.61634 4.0221 108.97615 3.82065 113.45983 8.6586 3.32682 3.58973 5.64895 4.293 17.06924 5.16945 9.80166.75218 5.07729 1.08578-18.2767 1.29053-33.46061.2934-64.84046 1.41667-78.65488 2.81565-8.87728.89902-15.74845 4.98082-17.45945 10.37167-.94933 2.99108-2.53924 3.18818-25.72239 3.18818h-24.71052l-3.35299 8.06812-3.35299 8.0682 3.54143.7488c4.38094.92625 100.29216 3.71363 177.36605 5.1546l57.53441 1.07565-4.23472 9.38573-4.23471 9.3858-110.78209.0533c-128.79092.0622-168.64271 1.09687-172.01252 4.4667-1.27467 1.2747-2.27795 5.42827-2.27795 9.43065.0 3.9341-.54511 7.1458-1.21135 7.1373-2.07255-.026-22.27357-15.7467-37.47353-29.16143-7.9873-7.04917-14.90656-12.81667-15.37615-12.81667-3.59614.0 5.27094 36.4268 14.34465 58.9292 3.18912 7.9089 5.7984 14.9654 5.7984 15.6811.0 2.6144-3.1254 1.1576-11.56474-5.3906z" id="path28"></path><path style="fill:#fff;stroke-width:1.61514" d="m503.11672 187.27227c-1.77666-.8144-6.78374-2.96315-11.12684-4.775l-7.89655-3.2943 14.35712-9.60149c19.54034-13.06783 25.59224-18.37411 24.42505-21.41575-.96653-2.51874-24.72686-8.89173-46.40862-12.4477-5.77413-.947-12.90745-2.11783-15.85181-2.60183l-5.35338-.88002 19.08209-19.74506c25.66613-26.55785 46.37215-50.869471 72.32345-84.917308 22.60993-29.6640009 30.6859-38.695279 32.7481-36.6218328.67551.6791993 1.19223 14.7990829 1.14825 31.3775198-.11293 42.575415-3.88296 59.355876-21.64977 96.363431-9.22505 19.21544-25.63038 45.4558-38.624 61.77918-6.90117 8.66968-10.2052 9.97415-17.17309 6.78016z" id="path29"></path><path style="fill:#fff;stroke-width:1.61514" d="m503.11672 187.27227c-1.77666-.8144-6.77642-2.96003-11.11058-4.76804l-7.88029-3.28731 13.70567-9.25515c19.06463-12.87393 26.09762-19.03029 25.00195-21.88557-.92122-2.40067-25.25316-8.87541-46.36659-12.33817-5.77413-.947-12.89745-2.11595-15.8296-2.59767l-5.33117-.87584 19.7232-20.5045c24.83789-25.821803 48.95643-54.254427 73.8009-87.00169 20.31146-26.7723387 28.53003-35.851556 30.58512-33.7879767 2.09801 2.1066846 1.25978 59.0228027-1.06068 72.0201827-6.12617 34.314014-29.50356 81.621134-58.06484 117.501574-6.90117 8.66968-10.2052 9.97415-17.17309 6.78016z" id="path30"></path><path style="fill:#fff;stroke-width:1.61514" d="m228.27904 282.11436c-1.44814-2.81463-11.22728-19.08539-21.73142-36.15725-30.88963-50.20341-35.29103-60.83016-38.63721-93.2857-3.58807-34.80175.36444-57.80064 19.21411-111.803153 13.59443-38.9467823 29.25103-78.016522 31.26393-78.016522 8.56988.0 25.15062 20.268363 37.65657 46.0315455C276.9376 51.923545 283.03085 96.435352 277.07847 162.53472c-1.50938 16.7611-2.50862 34.83562-2.22054 40.1656l.52377 9.69084 13.83153-18.57413c20.54491-27.58942 36.09656-45.50827 57.47258-66.22081 18.52345-17.94851 29.64227-26.42877 29.64227-22.60799.0.99098.72681 1.35258 1.61514.80357.88833-.54902 1.61514-.2451 1.61514.67537.0 3.6675-8.67123 24.48838-17.01804 40.86275-4.85333 9.52104-8.82423 17.65647-8.82423 18.07874.0.42226 5.2694-1.42751 11.70978-4.11062 16.75389-6.97979 30.08651-9.42211 51.28075-9.39381 23.17104.031 39.5261 2.0662 42.78403 5.32413 2.81125 2.81127 2.64872 2.92491-26.35088 18.42527l-15.06213 8.05072 15.06213 3.6831c20.25559 4.953 49.32928 15.03395 62.47051 21.66087 12.56936 6.33853 28.30084 17.64079 40.15513 28.84939 8.17772 7.7323 21.45856 23.77646 21.45856 25.92341.0.58419-3.77122 1.06216-8.38052 1.06216-6.7759.0-11.95579-1.67929-27.05362-8.77066-25.24224-11.85612-43.36088-18.99042-46.63012-18.36084-1.89362.36469.43987 3.10488 7.78066 9.1367 5.76702 4.73869 12.81059 11.08947 15.65237 14.11285l5.16685 5.49709H393.19179 282.62396L256.768 276.86515l-25.85596 10.36671zm15.48382-43.86107c7.19769-23.30343 9.00686-43.4218 9.00686-100.15866.0-46.384187-.39193-54.976157-3.02068-66.220813-5.74607-24.579125-18.6037-56.589984-22.24295-55.376901-3.98574 1.328581-23.71626 56.299694-31.56252 87.936154-3.57235 14.40391-4.31775 20.9484-4.20183 36.89185.16898 23.24349 3.55217 37.73915 13.91266 59.61037 11.15909 23.55707 27.70312 47.79657 32.62227 47.79657 1.35907.0 3.53097-4.14831 5.48619-10.47857z" id="path31"></path><path style="fill:#fff;stroke-width:1.61514" d="m225.95185 278.2245c-2.89803-5.11685-12.69915-21.35978-21.78024-36.09542-28.13589-45.6553-32.97681-57.60144-36.26473-89.49196-4.12031-39.96407 1.38247-65.625413 30.82635-143.7538395C214.92331-34.07659 216.34589-37.148265 220.05186-37.148265c4.76041.0 20.42614 17.207768 28.44076 31.2402954 14.11699 24.7169966 22.73403 49.8284516 28.56481 83.2425616 3.08144 17.658593 3.01958 53.735568-.14983 87.388018-1.38158 14.66945-2.2901 31.396-2.01893 37.17014l.49303 10.49841 13.8308-18.57413c20.34702-27.32512 35.96513-45.36145 57.48694-66.38791 18.11296-17.69605 29.62864-26.39871 29.62864-22.39105.0.96357.72681 1.30274 1.61514.75373.88833-.54902 1.61514-.13692 1.61514.91578.0 3.62703-11.77655 31.41629-18.94807 44.71197-3.95301 7.32871-6.37262 13.32493-5.37694 13.32493.9957.0 6.28514-1.88341 11.7543-4.18535 14.86544-6.25677 28.9799-8.68019 50.52654-8.67527 23.0616.005 38.63215 2.00021 42.015 5.38305 2.77698 2.77699 1.9899 3.32688-26.37859 18.42977l-15.0513 8.01304 15.0513 3.68039c20.24572 4.95057 49.32097 15.03255 62.45968 21.65821 12.56936 6.33853 28.30084 17.64079 40.15513 28.84939 8.17772 7.7323 21.45856 23.77646 21.45856 25.92341.0.58419-3.77122 1.06216-8.38052 1.06216-6.7759.0-11.95579-1.67929-27.05362-8.77066-25.04872-11.76522-43.35484-18.99159-46.55101-18.37607-1.76197.33933 1.59447 4.09943 10.20337 11.43043 23.09925 19.67041 35.48347 17.33144-91.76552 17.33144H282.86385l-24.73792 9.85413c-13.60585 5.41975-25.22549 10.15138-25.82141 10.51472-.59593.36333-3.45462-3.52592-6.35267-8.64277zm14.3691-31.51157c2.18088-2.62791 7.698-24.57044 10.05109-39.97476 1.01772-6.66246 2.22089-33.55458 2.67371-59.76025.91233-52.79793-.49195-68.42364-8.41887-93.678236-5.10453-16.262673-14.43467-37.148264-16.5951-37.148264-3.78003.0-24.03027 55.922973-32.16721 88.83281-5.44881 22.03769-5.68252 49.08675-.59105 68.40801 5.55177 21.0681 20.94346 50.91666 35.30992 68.47527 6.05096 7.39544 7.16234 7.94848 9.73751 4.84542z" id="path32"></path><path style="fill:#fff;stroke-width:1.61514" d="m225.91966 278.2245c-2.91576-5.11685-12.68703-21.29579-21.71396-35.95321-27.81868-45.17041-32.84227-57.43972-36.28026-88.60866-4.31068-39.08084.57744-62.955935 28.31901-138.318781 15.76612-42.830301 20.13128-52.492114 23.7157-52.492114 1.69579.0 6.64633 3.563068 11.00119 7.917929 20.81909 20.8190882 38.37067 61.168896 46.00293 105.757358 3.17123 18.52665 3.14815 53.352918-.0588 88.787408-1.38276 15.27838-2.29128 32.07057-2.01893 37.31596l.49518 9.53709 13.97873-18.62793c20.87748-27.82113 36.03158-45.29341 57.33901-66.11043 18.11297-17.69605 29.62864-26.39871 29.62864-22.39105.0.96357.72681 1.30274 1.61514.75373.88833-.54902 1.61514-.2451 1.61514.67537.0 3.5727-8.55377 24.31355-16.51909 40.05479-4.59292 9.07666-7.96509 16.8887-7.49371 17.3601.47139.47138 5.50311-1.0308 11.18161-3.33818 15.53151-6.31102 29.33609-8.66493 50.78702-8.66002 23.0616.005 38.63215 2.00021 42.015 5.38305 2.7988 2.79879 1.22108 3.90331-26.37726 18.46611l-15.04995 7.94141 14.24239 3.52446c48.11451 11.90659 76.14711 25.49562 102.03285 49.46125 8.92089 8.25918 22.84676 24.81471 22.84676 27.16098.0.58419-3.72581 1.06216-8.27957 1.06216-6.78677.0-12.68376-1.99932-32.70663-11.08888-29.65475-13.46207-39.47294-17.20799-41.01825-15.64965-.64645.65189 2.45871 4.20919 6.90035 7.9051 4.44164 3.69593 11.34637 9.79198 15.34384 13.54678l7.26814 6.82691-110.91295.0374-110.91297.0374-24.75896 9.8784c-13.61743 5.43311-25.24654 10.16474-25.84246 10.51472-.59593.34997-3.46911-3.55019-6.38486-8.66704zm15.83841-34.45733c3.20315-6.57343 8.51482-35.71998 10.1865-55.89606 2.13052-25.71405 2.04064-83.52177-.15776-101.461015-.97975-7.994955-3.48805-20.427851-5.57398-27.628662-4.64536-16.036115-15.38883-41.362255-17.89247-42.178871-3.51689-1.147102-22.00549 48.803883-31.5175 85.151378-11.30787 43.20988-4.33173 82.06315 22.08718 123.01339 10.49573 16.26878 16.96286 24.2168 19.13132 23.51217 1.00497-.32656 2.68649-2.35711 3.73671-4.51233z" id="path33"></path><path style="fill:#fff;stroke-width:1.61514" d="m213.9434 258.721c-41.11641-67.34973-44.85892-76.77368-46.9419-118.20365-1.64492-32.71712 4.70636-59.582765 30.35582-128.403785 14.76011-39.603368 19.05321-49.26183 21.89646-49.26183 12.89751.0 39.99838 43.8880295 50.36621 81.564669 10.44501 37.957078 12.11915 65.889846 7.28275 121.511276-1.38426 15.9197-2.29278 32.83697-2.01894 37.59395l.4979 8.64906 14.40394-19.27146c19.74057-26.41152 36.02655-45.10811 57.742-66.28887 18.98709-18.51957 26.60988-23.97819 30.29999-21.69756 3.01588 1.8639-1.11522 13.86272-13.23729 38.44788-5.37011 10.89127-9.4092 20.15694-8.97576 20.59038.43342.43344 5.43409-1.0998 11.11259-3.40718 15.53151-6.31102 29.33609-8.66493 50.78702-8.66002 22.90253.005 38.62837 1.99641 41.94266 5.3107 2.90905 2.90905 2.6267 3.11654-22.44054 16.49026-20.25918 10.80856-21.08847 9.29673 9.57358 17.45321 35.07759 9.33107 58.42868 20.66613 80.24658 38.95322 11.46606 9.61048 30.38751 30.54031 30.38751 33.61291.0.90718-4.20717 1.25873-10.09464.84353-7.85826-.55419-13.85175-2.50383-27.05363-8.80033-24.10549-11.49689-43.34219-19.12652-44.72853-17.74017-.65373.65371 2.41974 4.2346 6.82991 7.9575s11.28915 9.84915 15.28663 13.61388l7.26814 6.84497-110.70734.0374-110.70733.0374-18.50403 7.44437c-10.1772 4.09442-21.86284 8.77122-25.96807 10.39292l-7.46406 2.94856zm27.40139-14.60459c1.31271-2.5385 3.86923-11.89015 5.68116-20.78145 7.72113-37.88822 8.76702-123.708148 1.8725-153.647239-4.93556-21.432402-17.55455-53.536302-21.04337-53.536302-3.67405.0-27.92793 69.078191-33.19355 94.539571-5.91022 28.57824-2.61229 58.65362 9.50505 86.68086 8.38508 19.39456 29.44608 51.36001 33.83943 51.36001.52362.0 2.02607-2.07695 3.33878-4.61545z" id="path34"></path><path style="fill:#fff;stroke-width:1.61514" d="m218.40499 266.03407c-7.01141-11.73703-16.99395-28.24479-22.18343-36.68391-13.50569-21.96294-22.7646-42.4852-25.74296-57.05896-4.08742-20.00063-4.90289-49.26185-1.88342-67.58275 3.63007-22.02584 25.06482-86.641794 42.90516-129.339365 5.96937-14.286602 7.60738-15.028009 16.67984-7.549726 20.24257 16.685633 41.57802 64.241259 48.85707 108.899881 3.06914 18.82986 3.01885 51.9801-.13619 89.78968-1.38518 16.59971-2.2937 33.66516-2.01893 37.92324l.49957 7.74196 14.40394-19.27318c19.73725-26.40941 36.0237-45.10704 57.742-66.29058 18.98709-18.51957 26.60988-23.97819 30.29999-21.69756 2.94712 1.82142-.97853 13.38079-13.00807 38.30319-5.21858 10.8117-9.48833 20.01486-9.48833 20.45148.0.4366 7.44984-1.81117 16.5552-4.99509 15.2567-5.33489 18.32878-5.84352 39.16719-6.48484 24.71817-.7607 45.19989 1.34221 48.87591 5.01823 2.05404 2.05406.96106 3.15715-8.72763 8.80842-6.09165 3.55317-15.2549 8.50653-20.36278 11.00747-5.10789 2.50095-9.28707 5.0662-9.28707 5.70059.0 1.34251 4.59383 2.84798 26.51752 8.69022 40.06448 10.67642 70.76821 27.78235 93.05193 51.84191 8.52148 9.20059 16.10247 18.97346 16.10247 20.75816.0.68587-4.54258.92053-10.09464.52145-8.11143-.58306-14.3707-2.72431-31.86018-10.89913-23.37486-10.92572-38.77204-16.7776-40.25157-15.29808-.50559.5056 2.33921 3.58396 6.32179 6.84081 3.98258 3.25686 10.87514 9.37071 15.31678 13.58635l8.07571 7.66481-110.70734.0348-110.70733.0348-18.50403 7.44436c-10.1772 4.09442-21.91414 8.79145-26.08209 10.43787l-7.57806 2.99349zm25.37895-29.46418c6.61272-21.4095 8.74982-45.13157 8.87153-98.47526.096-42.057138-.34117-52.178245-2.73173-63.24782-3.68983-17.085913-9.49783-34.758778-15.82198-48.143869-4.0694-8.612884-5.48383-10.339363-7.26176-8.863813-4.8433 4.019587-27.44034 69.560457-32.36864 93.882632-3.7172 18.34517-2.46943 50.90481 2.55811 66.75143 5.18096 16.33018 14.70574 35.55615 25.92312 52.3263 9.3499 13.97826 13.78336 18.5494 16.41389 16.92365.66112-.4086 2.64897-5.42756 4.41746-11.15325z" id="path35"></path><path style="fill:#fff;stroke-width:1.61514" d="m226.63084 279.7824c-2.43246-4.19705-12.01412-19.98682-21.29258-35.08839-21.2355-34.5628-31.51457-55.94693-34.937-72.68139-4.07527-19.92665-4.85006-48.81854-1.80488-67.30417 3.62781-22.022442 25.06106-86.63565 42.90396-129.339365 6.51736-15.598117 8.4567-15.887875 20.8444-3.114389 20.51758 21.1565375 37.73813 61.561499 44.81358 105.147066 2.95555 18.206535 2.85935 52.437898-.25171 89.568468-1.38212 16.49562-2.29064 33.40783-2.01892 37.58267l.49401 7.59061 13.72871-18.44772c25.74433-34.59355 69.08728-81.03407 81.73161-87.5727 11.87622-6.141438 10.44711 2.75931-6.01342 37.45253-5.22356 11.0095-9.49737 20.179-9.49737 20.37667s7.44984-2.26878 16.5552-5.48099l16.55521-5.84038h31.49527c24.78569.0 32.51783.51611 36.29529 2.42271 2.64 1.33249 4.76265 3.14952 4.71699 4.03785-.0457.88833-6.92996 5.38316-15.29843 9.98851-8.36849 4.60533-17.31261 9.53402-19.87583 10.95263-2.56323 1.4186-4.2907 2.94899-3.83883 3.40086.45187.45185 10.90741 3.65486 23.23454 7.11775 45.52284 12.78818 71.90058 27.18743 95.15333 51.94292 8.72669 9.29068 16.9 19.60656 16.9 21.33023.0.58241-3.81577 1.05538-8.47949 1.05108-6.6553-.006-11.95414-1.60894-24.63091-7.45042-21.59185-9.94957-47.77077-20.41348-48.7132-19.47106-.41836.41838 2.35308 3.55965 6.15876 6.98064 3.80566 3.42098 10.5764 9.65664 15.04606 13.85704l8.12667 7.63709-110.70734.0348-110.70733.0348-18.50403 7.44436c-10.1772 4.09442-21.93654 8.80028-26.13185 10.45748l-7.62784 3.01311zm18.71441-50.21208c5.79604-24.82386 7.2104-42.93561 7.26953-93.09083.0397-33.65637-.58729-51.66046-2.05295-58.952673-4.73522-23.559447-19.6022-62.56075-23.03817-60.4372-3.85959 2.385358-21.76817 51.205422-30.10378 82.065025-3.89137 14.406408-4.40728 19.179228-4.38378 40.555138.0294 26.70907 1.78412 35.92833 10.70868 56.26214 6.66935 15.1955 21.14452 39.2553 28.34784 47.11815 5.3417 5.8308 5.87894 6.05921 7.86455 3.3437 1.17216-1.60301 3.59679-9.19158 5.38808-16.86345z" id="path36"></path><path style="fill:#fff;stroke-width:1.61514" d="m225.62903 278.16428c-2.99175-5.08374-12.579-20.87217-21.30501-35.08542-31.65212-51.55615-37.03952-67.63715-37.06525-110.63722-.0136-22.69921.53814-28.76672 3.72176-40.929685 6.69216-25.567348 26.87563-83.4061905 40.52926-116.14287 6.50458-15.595717 8.4448-15.886699 20.83495-3.12469 20.32853 20.9386335 36.85737 59.362785 44.69588 103.903278 3.10013 17.615782 3.04377 53.827917-.14322 92.020257-1.3872 16.62383-2.29571 33.31864-2.01893 37.09959l.50323 6.87445 13.66748-18.41389c18.51739-24.94809 38.08992-47.38734 59.2581-67.93745 18.41861-17.88082 25.86886-23.13458 29.56625-20.84948 2.97953 1.84146-.76272 12.99748-12.88206 38.4027-5.19117 10.88202-8.7034 19.78549-7.80496 19.78549.89845.0 7.935-2.34514 15.63677-5.2114l14.00321-5.21141 32.30285-.0378c25.56094-.0299 33.30464.46783 37.10286 2.38491 2.64 1.33249 4.76265 3.14952 4.71699 4.03785-.0457.88833-6.92996 5.38316-15.29843 9.98851-8.36849 4.60533-17.31261 9.53402-19.87583 10.95263-2.56323 1.4186-4.2995 2.94018-3.85841 3.38128.44111.4411 9.87073 3.28772 20.95474 6.32582 36.63048 10.04035 59.69202 20.57599 78.99705 36.08976 15.52668 12.47746 35.35566 33.54417 35.35566 37.56258.0.82017-3.81577 1.48235-8.47949 1.4715-6.23187-.0145-11.90446-1.55925-21.40063-5.82782-22.92654-10.30556-45.11919-19.64345-49.26183-20.72768-2.8511-.7462.4718 3.22416 11.30599 13.50895l15.34385 14.56573-110.70734.0578-110.70733.0578-18.50403 7.44436c-10.1772 4.09443-21.93315 8.79894-26.1243 10.45452l-7.6203 3.01014zm19.71622-48.59396c5.7349-24.56201 7.12948-42.87236 7.15165-93.8984.0177-40.803828-.45837-51.434358-2.77286-61.915277-5.36527-24.296051-18.53258-56.856413-22.47444-55.575155-4.437 1.442199-27.13245 67.292307-32.75344 95.033032-3.26392 16.10812-2.41936 46.64595 1.71546 62.02838 5.2069 19.37075 25.11234 56.2798 37.35506 69.26444 4.07564 4.32264 4.54153 4.45505 6.45713 1.8353 1.13551-1.55289 3.53015-9.10045 5.32144-16.77232z" id="path37"></path><path style="fill:#fff;stroke-width:1.61514" d="m209.20699 251.20123c-37.55583-61.85066-41.92156-74.2105-41.94822-118.75959-.0136-22.74713.5371-28.7786 3.7605-41.186118 6.88563-26.504141 26.05728-81.3912962 40.47822-115.886437 6.59543-15.776365 8.73335-16.089713 21.03992-3.083761 20.41708 21.5773675 36.71682 59.617987 44.50321 103.862348 2.67928 15.224438 3.11659 56.436388.85441 80.521098-2.87208 30.57821-3.78667 53.72615-2.09798 53.09928.8996-.33395 7.06721-7.92292 13.7058-16.8644 17.04993-22.96446 38.24303-47.15197 58.80443-67.11302 18.41862-17.88083 25.86886-23.13458 29.56625-20.84948 2.92603 1.8084-.68457 12.71566-12.45115 37.61355-4.93766 10.44798-8.57564 19.39823-8.08445 19.88944.49122.4912 7.32708-1.43618 15.19083-4.28306 13.08418-4.73681 16.35576-5.24125 38.54483-5.94302 25.5095-.80679 45.03408 1.16585 48.79854 4.93032 2.91703 2.91703 2.21473 3.45636-19.32896 14.84321-9.54228 5.04354-17.74087 9.80317-18.2191 10.57695-.47821.7738 2.2414 2.19115 6.0436 3.14968 60.74588 15.3142 92.82006 32.99996 121.02167 66.73157 4.30906 5.154 7.83463 10.05999 7.83463 10.90221.0.8422-3.81577 1.52241-8.47949 1.51156-6.19641-.0144-11.95848-1.56985-21.40063-5.77702-32.62862-14.5384-46.04702-20.04185-48.75546-19.99666-1.94186.0324 1.97024 4.63785 11.60719 13.66428l14.53628 13.61537-110.47978.0644-110.47978.0644-20.34672 8.28357c-11.19069 4.55594-23.0366 9.23541-26.32423 10.3988l-5.97752 2.11525zm36.10025-21.63091c5.42438-23.50644 7.01224-44.30186 7.10806-93.09083.10205-51.95887-1.2125-63.2578-10.38669-89.276779-5.64816-16.018793-12.07719-29.436149-14.10456-29.436149-3.33357.0-22.10078 50.760285-31.12743 84.191378-4.83851 17.91991-5.25399 56.17148-.78724 72.47739 5.67382 20.71234 24.41849 55.56244 37.17254 69.11119 4.49054 4.77036 5.20089 5.04033 6.95395 2.64288 1.07387-1.46862 3.40099-8.94721 5.17137-16.61908z" id="path38"></path><path style="fill:#fff;stroke-width:1.61514" d="m209.20699 251.20123c-37.57833-61.88772-41.84301-73.87399-41.96706-117.95202-.0555-19.71154.64106-28.90764 2.93606-38.763408 5.29525-22.740176 27.26863-86.2175448 40.95399-118.309146 7.09086-16.627759 9.02348-16.979061 21.40743-3.891333 17.1845 18.1610877 31.1968 47.29833 40.5305 84.279285 4.45169 17.637985 5.18697 23.683997 5.88854 48.419622.52194 18.40275-.10274 39.86963-1.79033 61.52374-2.91755 37.43613-3.10989 43.48324-1.38063 43.40725.66624-.0293 5.57224-6.02461 10.9022-13.32293 16.36447-22.40786 39.8825-49.51662 60.00206-69.16317 19.81784-19.35191 27.40502-24.82336 31.18378-22.48797 2.92603 1.8084-.68457 12.71566-12.45115 37.61355-4.93766 10.44798-8.62627 19.34761-8.19693 19.77695.42934.42934 7.2652-1.54674 15.19083-4.39128 14.35961-5.15372 14.52367-5.17214 46.71306-5.24067 25.60777-.0545 33.29768.43335 37.10286 2.35393 2.64 1.33249 4.82044 3.12704 4.84543 3.98788.0412 1.41972-6.5548 5.41925-31.79109 19.2768l-8.54206 4.69055 6.46057 2.01657c3.55331 1.1091 15.54573 4.67292 26.64984 7.91961 23.24105 6.79532 49.99161 19.00051 62.92892 28.71189 16.56825 12.43695 40.44016 37.08574 40.44016 41.75625.0.80856-3.81577 1.46123-8.47949 1.45038-6.21211-.0144-11.93456-1.56519-21.40063-5.79939-29.70048-13.28513-47.75532-20.45292-48.82477-19.38345-.62662.62659 5.01493 6.88196 12.53674 13.9008l13.67604 12.76151-110.207.0777-110.20699.0777-20.6195 8.25851c-11.34073 4.5422-23.30939 9.22166-26.59702 10.39882l-5.97753 2.14029zm33.65096-12.564c6.99835-24.41061 7.97673-34.70222 8.71041-91.62504.502-38.9483.13156-59.339755-1.23232-67.835958-3.36436-20.95798-18.14374-61.40967-22.43656-61.40967-3.40425.0-22.56382 52.350279-31.03395 84.794958-4.91461 18.82534-5.32389 55.22043-.80826 71.87381 6.89482 25.42763 34.43758 72.68139 42.36374 72.68139 1.10326.0 3.09988-3.81578 4.43694-8.47949z" id="path39"></path><path style="fill:#fff;stroke-width:1.61514" d="m228.88105 283.98786c-21.45876-34.71428-42.95674-71.76478-48.80995-84.12104-8.98551-18.9686-11.79707-31.33217-13.14638-57.80994-1.73596-34.0655 3.12851-55.84603 27.21137-121.83807 16.82771-46.111433 21.86687-57.367075 25.68326-57.367075 7.49646.0 24.26119 20.796338 36.00898 44.6684745 7.90834 16.0701615 11.90143 27.6122885 18.07288 52.2400425 5.7432 22.918713 6.90762 61.581768 3.26361 108.362858-2.93635 37.69632-3.07399 41.86658-1.37933 41.79211.66624-.0293 5.57224-6.02461 10.9022-13.32293 16.36447-22.40786 39.8825-49.51662 60.00206-69.16317 19.81784-19.35191 27.40502-24.82336 31.18378-22.48797 2.91756 1.80315-.63369 12.62318-12.36726 37.68087-4.90973 10.48502-8.61052 19.37993-8.22398 19.76646.38654.38656 7.19211-1.65077 15.12351-4.52738l14.42069-5.23018h32.30285c25.50352.0 33.31317.50994 37.10286 2.42271 2.64 1.33249 4.76008 3.14952 4.71129 4.03785-.0883 1.60733-7.37834 6.11359-28.85306 17.83521l-11.26603 6.14937 6.42061 1.98297c3.53133 1.09062 15.50577 4.63932 26.60988 7.88601 24.17331 7.06791 49.76154 18.92364 64.40565 29.84095 15.20856 11.33809 38.96343 36.26639 38.96343 40.88811.0.94469-4.08 1.29535-10.09464.86764-7.79347-.5542-14.32874-2.66133-28.66876-9.24347-23.15113-10.62648-38.88287-16.67618-40.12775-15.43129-.52416.52416 5.20119 6.6957 12.723 13.71454l13.67604 12.76151-110.21112.0777-110.21112.0777-24.65324 9.86997c-13.55927 5.42848-25.50764 10.20977-26.55193 10.62509-1.04428.41531-2.94481-.93721-4.2234-3.00562zm12.81092-42.8213c1.36738-3.27258 4.0165-13.26627 5.88694-22.20819 3.10612-14.84926 3.40838-21.64633 3.48823-78.44102l.0874-62.182962-4.29914-15.35616C241.94302 45.431437 231.04714 19.107063 228.33 18.22092c-2.47235-.806316-11.51063 21.247162-22.14707 54.039058-11.66196 35.953512-12.97587 42.756462-13.02721 67.449802-.0506 24.33765 1.9567 34.99518 10.19089 54.10725 9.92444 23.03524 29.39346 53.29969 34.28753 53.29969.86443.0 2.69046-2.67757 4.05783-5.95016z" id="path40"></path><path style="fill:#fff;stroke-width:1.61514" d="m357.75394 424.70212c-29.25045-3.2514-45.90713-17.12433-55.01312-45.81904-2.23213-7.03386-5.90917-23.37271-8.1712-36.30854s-4.45863-24.07927-4.88132-24.7632c-1.55674-2.51887-9.78534-1.08498-31.16755 5.4312-24.17414 7.36701-50.1017 16.35405-85.78587 29.7352-24.32528 9.1217-27.66028 10.07132-24.59106 7.0021.99657-.99657 1.46879-2.70622 1.04937-3.79922-1.29442-3.37318 9.99251-11.72682 26.46665-19.58838 8.65397-4.12974 38.26571-16.94474 65.80388-28.4778l50.0694-20.96919 188.24231.89879c121.40704.57968 189.7755 1.48171 192.56057 2.54059 5.97157 2.2704 6.86199 6.8367 6.64146 34.0589-.15401 19.00909-.98664 26.89849-4.42134 41.89264-2.32738 10.16019-4.78586 19.00941-5.46326 19.66493-.67741.65552-3.37167-.52917-5.98725-2.63266l-4.75558-3.82452 5.36978-27.35562c5.73883-29.23567 5.90144-42.98485.56078-47.41723-2.07677-1.72358-30.59748-2.22067-148.89474-2.59511-80.47925-.25475-160.86219.10866-178.62875.80757l-32.30284 1.27073-.46803 4.03785c-.53807 4.64205 9.83863 62.87785 13.43518 75.40048 2.89595 10.08328 8.55951 17.18948 17.51993 21.98265 6.43671 3.44317 8.26677 3.56324 54.30787 3.56324 53.9595.0 58.78288-.73446 70.06974-10.66953 8.03791-7.07526 10.71931-12.96636 20.46285-44.95745 3.63655-11.93995 7.6677-23.75068 8.95811-26.24606l2.3462-4.53705 18.93644.0238c21.32096.0268 27.01215 1.73332 27.01215 8.0996.0 3.93886-.15865 3.98374-12.34075 3.49091-14.62233-.59154-22.69085 2.14248-26.12207 8.8515-1.28517 2.51289-5.04902 13.29065-8.36412 23.95058-15.9672 51.34376-29.00221 59.23905-96.9207 58.70469-13.32492-.10484-29.31482-.75615-35.53311-1.44735z" id="path41"></path><path style="fill:#fff;stroke-width:1.61514" d="m402.48347 1045.818c-2.93694-2.0266-13.90227-11.3518-24.36742-20.7226-37.57978-33.64993-58.07181-62.87743-85.9297-122.56048-5.68221-12.17362-10.90847-21.55665-11.61392-20.8512-1.34286 1.34288 1.12461 49.2453 3.7533 72.8652.84034 7.55078 1.53386 14.8059 1.54114 16.12253.0399 7.20787-25.31907-42.34035-38.01048-74.2677-11.58422-29.142-23.95591-78.6789-32.27012-129.21135-3.55763-21.62262-4.22283-24.42941-5.66265-23.89338-.70003.26062-12.68024 15.7369-26.62271 34.3918-13.94246 18.6549-25.73547 34.28145-26.20668 34.7256-1.96983 1.8567 1.28602-40.0272 5.14769-66.22084 4.89215-33.18335 13.65058-85.54627 19.42723-116.14717 2.24901-11.91373 3.89574-21.8547 3.65942-22.09103-.23633-.23633-9.50702 6.3842-20.60153 14.71228-18.9791 14.24661-20.20205 14.90829-20.68275 11.19034-.28101-2.17336 4.26169-23.03113 10.09489-46.35058 10.49846-41.96984 31.84034-138.80015 30.81764-139.82284-.29161-.29162-9.84247 5.6893-21.22413 13.29093-16.14623 10.7838-20.97611 13.31691-21.97794 11.52673-.83205-1.48679 3.7344-12.98896 12.9729-32.67675 11.73177-25.00109 14.95419-30.56783 18.19366-31.42958 9.75651-2.59539 72.97939-14.24725 73.9731-13.63309.61097.3776 5.45352 7.72992 10.76122 16.3385 18.42853 29.8893 46.64679 51.12032 74.18985 55.81942 17.3177 2.95455 53.01405 1.01504 72.13201-3.9192 15.84011-4.08824 40.18655-14.26641 40.18655-16.80021.0-.77064 3.45236-2.64692 7.67192-4.1695 16.18808-5.84131 38.00652-26.49102 47.19533-44.66726 1.58338-3.13206 5.58394-13.32619 8.89012-22.65363 8.99277-25.37047 7.09785-24.48669 36.74661-17.13843 67.29717 16.67916 104.77954 34.45776 115.37234 54.72326 4.66249 8.92001 2.24509 26.53546-5.20107 37.89973l-2.4005 3.66363-10.86293-8.96967c-5.97461-4.93332-15.86657-11.47605-21.98215-14.5394-10.57063-5.29494-11.95987-5.56972-28.15882-5.56972-13.92516.0-18.3007.61048-23.93915 3.34-7.8576 3.8038-16.73233 13.09379-16.73233 17.51528.0 6.02394 3.8028 7.08674 18.96003 5.29889 19.58289-2.30988 31.84778.32678 52.3488 11.25371 30.30393 16.15182 35.74323 50.19556 10.62246 66.48417-8.16321 5.29311-17.25407 6.65695-35.77518 5.36712-20.4443-1.42379-33.08183-4.79355-51.80911-13.81475-17.0689-8.22233-22.22802-12.59722-28.74818-24.37818-4.87857-8.81485-8.62627-9.71541-15.69132-3.77057-8.52556 7.17379-15.50838 28.22957-11.41352 34.41595 1.33616 2.01861 2.18896 1.80418 5.61125-1.41089l4.03561-3.79127 17.0384 9.95898c35.53906 20.77267 50.20896 24.91652 88.12044 24.89171 21.68151-.0142 29.46948-.67615 41.77416-3.55068 11.55495-2.6994 15.6526-3.09482 17.36277-1.67548 7.80954 6.48134-5.45307 23.88294-26.0955 34.23936-15.54474 7.79888-25.84954 9.95598-46.77014 9.79037-20.50822-.16233-32.78631-2.33502-72.01608-12.74372-38.39515-10.18725-45.0297-10.93994-85.46026-9.69539-19.98738.61525-46.15268 1.60747-58.1451 2.20491l-21.80442 1.08625 16.81692 17.37475c16.13273 16.66785 16.59273 17.33625 11.30599 16.42897-38.55791-6.61705-56.23502-14.53105-82.23017-36.81424-5.77413-4.94961-10.67573-8.47753-10.89243-7.83981-.21671.63771 2.11177 9.12752 5.17441 18.86625 3.06263 9.73872 5.39598 17.87922 5.18521 18.08997-.21076.21076-5.16903-3.20085-11.01836-7.58138-23.99942-17.97298-50.87912-51.3133-62.78794-77.87922-3.08573-6.88357-6.25733-12.51637-7.04801-12.51734-2.57733-.003.25811 51.33472 3.74023 67.71958 3.27039 15.38867 8.19579 24.94396 8.19579 15.89988.0-2.35875.58419-4.64969 1.2982-5.09096.71402-.44129 5.91806 7.6459 11.56454 17.97152 22.79368 41.68244 43.09309 67.60239 85.06758 108.62124 30.0072 29.32403 48.72337 45.0041 75.86378 63.55715 9.86311 6.74243 18.85557 13.37063 19.98326 14.7294 2.59563 3.12758 3.2057 3.12878-25.69067-.0518-31.98572-3.5208-32.75936-3.20145-17.07092 7.04648 14.99028 9.79185 65.54726 35.8452 80.331 41.39662 17.09691 6.42015 68.19689 15.18458 78.8953 13.5318 3.51235-.54262 3.18015-.9831-3.3187-4.40062-13.43333-7.06403-42.11686-26.33505-43.75572-29.3973-1.56417-2.92275-.52874-3.0489 26.28972-3.20355 15.35491-.0885 29.54933-.63368 31.54314-1.21133l3.62512-1.05037-4.92114-7.26818c-14.35339-21.19882-41.61423-70.2753-39.9569-71.93264.44776-.44777 5.82655 1.33176 11.95282 3.9545 8.48005 3.63042 11.25764 4.24609 11.63704 2.57944.27407-1.20404-2.06396-9.69482-5.19565-18.86841-8.14715-23.86531-15.4785-53.60811-16.453-66.74863l-.83846-11.306 6.34011 16.15142c21.1348 53.84095 35.95589 79.16085 63.81197 109.01435 16.79881 18.00345 16.79895 18.0033-10.37978 13.0389-9.9861-1.824-18.4495-3.02348-18.80757-2.66535-1.13898 1.13895 29.11997 30.87067 38.36158 37.69312 4.88581 3.6069 17.60505 11.33835 28.26499 17.181 10.65994 5.84273 18.65488 10.6452 17.76656 10.67213-.88833.027-.5791.70477.68721 1.50615 1.42055.8991-2.60029 1.13085-10.49842.6051-7.04044-.4686-15.34463-.16853-18.45378.6669l-5.65299 1.51882 9.69085 6.26325c12.57209 8.12543 40.74455 20.70203 55.90216 24.95565 13.86981 3.8922 55.93511 6.8904 86.23033 6.14603l19.38173-.47618-16.15148-5.46637c-25.70755-8.7006-49.26519-18.2562-48.41178-19.63703.43189-.69877-.12779-1.69792-1.24374-2.2203-2.52525-1.18215 168.04865.47033 221.41047 2.14493 42.3804 1.3299 42.17678 1.28055 42.22665 10.2387.0232 4.12192.51885 4.45207 6.88913 4.58625 15.25672.3213 44.4526 2.86552 51.2807 4.4688 3.9975.93855 19.3904 1.73812 34.2064 1.77682 22.2703.0585 27.3781.51015 29.4763 2.60843 3.1012 3.10117 3.1058 2.62245-.09 9.32302l-2.6276 5.51018-26.8487 1.0671c-106.9313 4.24987-251.40793 6.51345-418.25928 6.5532l-181.44209.0427-3.20577 3.95857c-3.08903 3.81443-3.09691 3.99893-.21644 5.06445 1.64411.60818 34.9489 2.3244 74.01063 3.81383 105.85054 4.03605 111.43863 4.45192 115.88072 8.62507 2.03601 1.91273 6.58503 4.1529 10.10891 4.97813l6.40708 1.50045-5.653.4584c-3.10915.25215.16151.8703 7.26814 1.37362 7.49448.53085-11.37367 1.37505-44.92613 2.01023-63.17235 1.1958-62.52707 1.09215-68.36896 10.98165l-2.88404 4.88227-24.28268.0165-24.28267.0165-2.82301 6.71685c-1.55265 3.69428-2.64287 6.87345-2.42271 7.06493.71213.61935 116.63886 3.92347 176.43025 5.0286 45.16648.83482 58.605 1.55467 60.27941 3.22912 1.18514 1.18515 1.44557 2.15483.57872 2.15483-.86684.0-3.23295 3.99742-5.25799 8.88322l-3.68193 8.8833H584.38418 442.54889l-.19108 6.05678c-.10511 3.3313-.65022 8.1128-1.21136 10.6258l-1.02027 4.569-13.24252-9.8667c-7.28337-5.4268-19.01383-15.29605-26.06769-21.93183-7.05384-6.63577-13.36249-12.06502-14.01919-12.06502-3.63929.0 4.52756 33.24505 14.63388 59.57055 3.9203 10.2119 6.96242 18.7166 6.76026 18.8995-.20217.1828-2.77052-1.3257-5.70745-3.3524z" id="path42"></path></svg><style>.header-logo svg{width:32px;height:32px}</style></div></a><button id="close-sidebar" class="btn btn-icon btn-soft">
<span class="material-icons size-20 menu-icon align-middle">menu</span>
</button>
<button id="flexsearch-button" class="ms-3 btn btn-soft collapsed" data-bs-toggle="collapse" data-bs-target="#FlexSearchCollapse" aria-expanded="false" aria-controls="FlexSearchCollapse">
<span class="material-icons size-20 menu-icon align-middle">search</span>
<span class="flexsearch-button-placeholder ms-1 me-2 d-none d-sm-block">Search</span><div class="d-none d-sm-block"><span class="flexsearch-button-keys"><kbd class="flexsearch-button-cmd-key"><svg width="44" height="15"><path d="M2.118 11.5A1.519 1.519.0 011 11.042 1.583 1.583.0 011 8.815a1.519 1.519.0 011.113-.458h.715V6.643h-.71A1.519 1.519.0 011 6.185 1.519 1.519.0 01.547 5.071 1.519 1.519.0 011 3.958 1.519 1.519.0 012.118 3.5a1.519 1.519.0 011.114.458A1.519 1.519.0 013.69 5.071v.715H5.4V5.071A1.564 1.564.0 016.976 3.5 1.564 1.564.0 018.547 5.071 1.564 1.564.0 016.976 6.643H6.261V8.357h.715a1.575 1.575.0 011.113 2.685 1.583 1.583.0 01-2.227.0A1.519 1.519.0 015.4 9.929V9.214H3.69v.715a1.519 1.519.0 01-.458 1.113A1.519 1.519.0 012.118 11.5zm0-.857a.714.714.0 00.715-.714V9.214H2.118a.715.715.0 100 1.429zm4.858.0a.715.715.0 100-1.429H6.261v.715a.714.714.0 00.715.714zM3.69 8.357H5.4V6.643H3.69zM2.118 5.786h.715V5.071a.714.714.0 00-.715-.714.715.715.0 00-.5 1.22A.686.686.0 002.118 5.786zm4.143.0h.715a.715.715.0 00.5-1.22.715.715.0 00-1.22.5z" fill="currentcolor"></path><path d="M12.4 11.475H11.344l3.879-7.95h1.056z" fill="currentcolor"></path><path d="M25.073 5.384l-.864.576a2.121 2.121.0 00-1.786-.923 2.207 2.207.0 00-2.266 2.326 2.206 2.206.0 002.266 2.325 2.1 2.1.0 001.782-.918l.84.617a3.108 3.108.0 01-2.622 1.293 3.217 3.217.0 01-3.349-3.317 3.217 3.217.0 013.349-3.317A3.046 3.046.0 0125.073 5.384z" fill="currentcolor"></path><path d="M30.993 5.142h-2.07v5.419H27.891V5.142h-2.07V4.164h5.172z" fill="currentcolor"></path><path d="M34.67 4.164c1.471.0 2.266.658 2.266 1.851.0 1.087-.832 1.809-2.134 1.855l2.107 2.691h-1.28L33.591 7.87H33.07v2.691H32.038v-6.4zm-1.6.969v1.8h1.572c.832.0 1.22-.3 1.22-.918s-.411-.882-1.22-.882z" fill="currentcolor"></path><path d="M42.883 10.561H38.31v-6.4h1.033V9.583h3.54z" fill="currentcolor"></path></svg>
</kbd><kbd class="flexsearch-button-key"><svg width="15" height="15"><path d="M5.926 12.279H4.41L9.073 2.721H10.59z" fill="currentcolor"></path></svg></kbd></span></div></button></div><div class="d-none d-lg-flex d-flex align-items-center m-1"><h5>Star us on GitHub !&nbsp;</h5><script async="" defer="" src="https://buttons.github.io/buttons.js"></script><span></span></div><div class="d-flex align-items-center"><ul class="list-unstyled mb-0"><li class="list-inline-item mb-0"><a href="https://github.com/mudler/LocalAI" alt="github" rel="noopener noreferrer" target="_blank"><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>GitHub</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"></path></svg></div></a></li><li class="list-inline-item mb-0"><a href="https://twitter.com/LocalAI_API" alt="twitter" rel="noopener noreferrer" target="_blank"><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24"><title>Twitter / X</title><path d="M.088.768l9.266 12.39L.029 23.231h2.1l8.163-8.819 6.6 8.819h7.142L14.242 10.145 22.921.768h-2.1L13.3 8.891 7.229.768zM3.174 2.314H6.455L20.942 21.685h-3.28z" fill="currentcolor"></path></svg></div></a></li><li class="list-inline-item mb-0"><a href="https://localai.io/index.xml" alt="rss" rel="noopener noreferrer" target="_blank"><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>RSS</title><path d="M4 11a9 9 0 019 9"></path><path d="M4 4a16 16 0 0116 16"></path><circle cx="5" cy="19" r="1"></circle></svg></div></a></li></ul><button id="mode" class="btn btn-icon btn-default ms-2" type="button" aria-label="Toggle user interface mode">
<span class="toggle-dark"><svg height="30" width="30" viewBox="0 0 48 48" fill="currentcolor"><title>Enable dark mode</title><path d="M24 42q-7.5.0-12.75-5.25T6 24t5.25-12.75T24 6q.4.0.85.025.45.025 1.15.075-1.8 1.6-2.8 3.95t-1 4.95q0 4.5 3.15 7.65Q28.5 25.8 33 25.8q2.6.0 4.95-.925T41.9 22.3q.05.6.075.975Q42 23.65 42 24q0 7.5-5.25 12.75T24 42zm0-3q5.45.0 9.5-3.375t5.05-7.925q-1.25.55-2.675.825Q34.45 28.8 33 28.8q-5.75.0-9.775-4.025T19.2 15q0-1.2.25-2.575t.9-3.125q-4.9 1.35-8.125 5.475Q9 18.9 9 24q0 6.25 4.375 10.625T24 39zm-.2-14.85z"></path></svg>
</span><span class="toggle-light"><svg height="30" width="30" viewBox="0 0 48 48" fill="currentcolor"><title>Enable light mode</title><path d="M24 31q2.9.0 4.95-2.05T31 24t-2.05-4.95T24 17t-4.95 2.05T17 24t2.05 4.95T24 31zm0 3q-4.15.0-7.075-2.925T14 24t2.925-7.075T24 14t7.075 2.925T34 24t-2.925 7.075T24 34zM3.5 25.5q-.65.0-1.075-.425Q2 24.65 2 24t.425-1.075Q2.85 22.5 3.5 22.5h5q.65.0 1.075.425Q10 23.35 10 24t-.425 1.075T8.5 25.5zm36 0q-.65.0-1.075-.425Q38 24.65 38 24t.425-1.075T39.5 22.5h5q.65.0 1.075.425Q46 23.35 46 24t-.425 1.075-1.075.425zM24 10q-.65.0-1.075-.425Q22.5 9.15 22.5 8.5v-5q0-.65.425-1.075Q23.35 2 24 2t1.075.425T25.5 3.5v5q0 .65-.425 1.075Q24.65 10 24 10zm0 36q-.65.0-1.075-.425T22.5 44.5v-5q0-.65.425-1.075Q23.35 38 24 38t1.075.425.425 1.075v5q0 .65-.425 1.075Q24.65 46 24 46zM12 14.1l-2.85-2.8q-.45-.45-.425-1.075.025-.625.425-1.075.45-.45 1.075-.45t1.075.45L14.1 12q.4.45.4 1.05.0.6-.4 1-.4.45-1.025.45T12 14.1zm24.7 24.75L33.9 36q-.4-.45-.4-1.075t.45-1.025q.4-.45 1-.45t1.05.45l2.85 2.8q.45.45.425 1.075-.025.625-.425 1.075-.45.45-1.075.45t-1.075-.45zM33.9 14.1q-.45-.45-.45-1.05.0-.6.45-1.05l2.8-2.85q.45-.45 1.075-.425.625.025 1.075.425.45.45.45 1.075t-.45 1.075L36 14.1q-.4.4-1.025.4t-1.075-.4zM9.15 38.85q-.45-.45-.45-1.075t.45-1.075L12 33.9q.45-.45 1.05-.45.6.0 1.05.45.45.45.45 1.05.0.6-.45 1.05l-2.8 2.85q-.45.45-1.075.425-.625-.025-1.075-.425zM24 24z"></path></svg></span></button></div></div><div class="collapse" id="FlexSearchCollapse"><div class="flexsearch-container"><div class="flexsearch-keymap"><li><kbd class="flexsearch-button-cmd-key"><svg width="15" height="15" aria-label="Arrow down" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 3.5v8m3-3-3 3-3-3"></path></g></svg></kbd>
<kbd class="flexsearch-button-cmd-key"><svg width="15" height="15" aria-label="Arrow up" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 11.5v-8m3 3-3-3-3 3"></path></g></svg></kbd>
<span class="flexsearch-key-label">to navigate</span></li><li><kbd class="flexsearch-button-cmd-key"><svg width="15" height="15" aria-label="Enter key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M12 3.53088v3c0 1-1 2-2 2H4m3 3-3-3 3-3"></path></g></svg></kbd>
<span class="flexsearch-key-label">to select</span></li><li><kbd class="flexsearch-button-cmd-key"><svg width="15" height="15" aria-label="Escape key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993.0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016s1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5s-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864.0 1.6425 1.031 1.5443 2.2492h-2.956"></path></g></svg></kbd>
<span class="flexsearch-key-label">to close</span></li></div><form class="flexsearch position-relative flex-grow-1 ms-2 me-2"><div class="d-flex flex-row"><input id="flexsearch" class="form-control" type="search" placeholder="Search" aria-label="Search" autocomplete="off">
<button id="hideFlexsearch" type="button" class="ms-2 btn btn-soft">
cancel</button></div><div id="suggestions" class="shadow rounded-1 d-none"></div></form></div></div></div><div class="container-fluid"><div class="layout-spacing"><div class="d-md-flex justify-content-between align-items-center"><nav aria-label="breadcrumb" class="d-inline-block pb-2 mt-1 mt-sm-0"><ul id="breadcrumbs" class="breadcrumb bg-transparent mb-0" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumb-item text-capitalize active" aria-current="page" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><a itemprop="item" href="/docs/"><i class="material-icons size-20 align-text-bottom" itemprop="name">Home</i>
</a><meta itemprop="position" content="1"></li><li class="breadcrumb-item text-capitalize" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><a itemprop="item" href="/features/"><span itemprop="name">Features</span>
</a><meta itemprop="position" content="2"></li><li class="breadcrumb-item text-capitalize active" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><span itemprop="name">ğŸ“ˆ Reranker</span>
<meta itemprop="position" content="3"></li></ul></nav></div><div class="row flex-xl-nowrap"><div class="docs-toc col-xl-3 d-xl-block"><toc><div class="fw-bold text-uppercase mb-2">On this page</div><nav id="toc"><ul><li><ul><li><a href="#usage" class="active">Usage</a></li></ul></li></ul></nav></toc></div><div class="docs-toc-mobile d-print-none d-xl-none"><button id="toc-dropdown-btn" class="btn-secondary dropdown-toggle" type="button" data-bs-toggle="dropdown" data-bs-offset="0,0" aria-expanded="false">Usage</button><nav id="toc-mobile"><ul class="dropdown-menu"><li><ul><li><a href="#usage" class="">Usage</a></li></ul></li></ul></nav></div><div class="docs-content col-12 col-xl-9 mt-0"><div class="mb-0 d-flex"><i class="material-icons title-icon me-2">article</i><h1 class="content-title mb-0">ğŸ“ˆ Reranker</h1></div><p class="lead mb-3"></p><div id="content" class="main-content" data-bs-spy="scroll" data-bs-root-margin="0px 0px -65%" data-bs-target="#toc-mobile"><div data-prismjs-copy="" data-prismjs-copy-success="" data-prismjs-copy-error=""><p>A <strong>reranking</strong> model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks.
Given a query and a set of documents, it will output similarity scores.</p><p>We can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.</p><p><img src="/ede67b25-fac4-4833-ae4f-78290e401e60_16032486548195551081.gif" alt="output" width="1980" height="1064" loading="lazy"></p><p>LocalAI supports reranker models, and you can use them by using the <code>rerankers</code> backend, which uses <a href="https://github.com/AnswerDotAI/rerankers" rel="external" target="_blank">rerankers<svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"></path></svg></a>.</p><h2 id="usage">Usage <a href="#usage" class="anchor" aria-hidden="true"><i class="material-icons align-middle">link</i></a></h2><p>You can test <code>rerankers</code> by using container images with python (this does <strong>NOT</strong> work with <code>core</code> images) and a model config file like this, or by installing <code>cross-encoder</code> from the gallery in the UI:</p><div class="prism-codeblock"><div class="code-toolbar"><pre id="4a0d2eb" class="language-yaml" tabindex="0"><code class="language-yaml"><span class="token key atrule">name</span><span class="token punctuation">:</span> jina<span class="token punctuation">-</span>reranker<span class="token punctuation">-</span>v1<span class="token punctuation">-</span>base<span class="token punctuation">-</span>en
<span class="token key atrule">backend</span><span class="token punctuation">:</span> rerankers
<span class="token key atrule">parameters</span><span class="token punctuation">:</span>
  <span class="token key atrule">model</span><span class="token punctuation">:</span> cross<span class="token punctuation">-</span>encoder

<span class="token comment"># optionally:</span>
<span class="token comment"># type: flashrank</span>
<span class="token comment"># diffusers:</span>
<span class="token comment">#  pipeline_type: en # to specify the english language</span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" aria-label="copy" data-copy-state="copy"><span></span></button></div></div></div></div><p>and test it with:</p><div class="prism-codeblock"><div class="code-toolbar"><pre id="42e5e60" class="language-bash" tabindex="0"><code class="language-bash"><span class="token function">curl</span> http://localhost:8080/v1/rerank <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">"Content-Type: application/json"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "jina-reranker-v1-base-en",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'</span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" aria-label="copy" data-copy-state="copy"><span></span></button></div></div></div></div></div><div class="gitinfo d-flex flex-wrap justify-content-between align-items-center opacity-85 pt-3"><div id="edit-this-page" class="mt-1"><a href="https://github.com/mudler/LocalAI/blob/master/docs/content/docs/features/reranker.md" alt="ğŸ“ˆ Reranker" rel="noopener noreferrer" target="_blank"><span class="me-1 align-text-bottom"><svg width="20" height="20" viewBox="0 0 32 32" fill="currentcolor"><path d="M16 .396c-8.839.0-16 7.167-16 16 0 7.073 4.584 13.068 10.937 15.183.803.151 1.093-.344 1.093-.772.0-.38-.009-1.385-.015-2.719-4.453.964-5.391-2.151-5.391-2.151-.729-1.844-1.781-2.339-1.781-2.339-1.448-.989.115-.968.115-.968 1.604.109 2.448 1.645 2.448 1.645 1.427 2.448 3.744 1.74 4.661 1.328.14-1.031.557-1.74 1.011-2.135-3.552-.401-7.287-1.776-7.287-7.907.0-1.751.62-3.177 1.645-4.297-.177-.401-.719-2.031.141-4.235.0.0 1.339-.427 4.4 1.641 1.281-.355 2.641-.532 4-.541 1.36.009 2.719.187 4 .541 3.043-2.068 4.381-1.641 4.381-1.641.859 2.204.317 3.833.161 4.235 1.015 1.12 1.635 2.547 1.635 4.297.0 6.145-3.74 7.5-7.296 7.891.556.479 1.077 1.464 1.077 2.959.0 2.14-.02 3.864-.02 4.385.0.416.28.916 1.104.755 6.4-2.093 10.979-8.093 10.979-15.156.0-8.833-7.161-16-16-16z"></path></svg>
</span>Edit this page</a></div><div id="last-modified" class="mt-1"><p class="mb-0 fw-semibold">Last updated <span id="relativetime" data-authdate="2024-05-25T16:11:59+0200" title="25 May 2024, 16:11 +0200">a year ago</span>. <span class="material-icons size-20 align-text-bottom opacity-75">history</span></p></div></div></div><div><hr class="doc-hr"><div id="doc-nav" class="d-print-none"><div class="row flex-xl-nowrap"><div class="col-sm-6 pt-2 doc-next"><a href="/features/text-generation/"><div class="card h-100 my-1"><div class="card-body py-2"><p class="card-title fs-5 fw-semibold lh-base mb-0"><i class="material-icons align-middle">navigate_before</i> ğŸ“– Text generation (GPT)</p><p class="card-text ms-2"></p></div></div></a></div><div class="col-sm-6 pt-2 doc-prev"><a class="ms-auto" href="/features/text-to-audio/"><div class="card h-100 my-1 text-end"><div class="card-body py-2"><p class="card-title fs-5 fw-semibold lh-base mb-0">ğŸ—£ Text to audio (TTS) <i class="material-icons align-middle">navigate_next</i></p><p class="card-text me-2"></p></div></div></a></div></div></div></div></div></div></div></div><footer class="shadow py-3 d-print-none"><div class="container-fluid"><div class="row align-items-center"><div class="col"><div class="text-sm-start text-center mx-md-2"><p class="mb-0">Â© 2023-2025 <a href="https://mudler.pm" target="_blank">Ettore Di Giacinto</a></p></div></div></div></div></footer></main></div></div><button onclick="topFunction()" id="back-to-top" aria-label="Back to Top Button" class="back-to-top fs-5" style="display: block;"><svg width="24" height="24"><path d="M12 10.224l-6.3 6.3-1.38-1.372L12 7.472l7.68 7.68-1.38 1.376z" style="fill:#fff"></path></svg></button>
<script>(()=>{var e=document.getElementById("mode");e!==null&&(window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{e.matches?(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")):(localStorage.setItem("theme","light"),document.documentElement.removeAttribute("data-dark-mode"))}),e.addEventListener("click",()=>{document.documentElement.toggleAttribute("data-dark-mode"),localStorage.setItem("theme",document.documentElement.hasAttribute("data-dark-mode")?"dark":"light")}),localStorage.getItem("theme")==="dark"?document.documentElement.setAttribute("data-dark-mode",""):document.documentElement.removeAttribute("data-dark-mode"))})()</script><script src="/docs/js/bootstrap.eac7ee3f6fa791c684e7a51fc2fd50b6f724271b9e39562e4fe8c3942412df3acbfc4045f043d03399dac70091207507.js" integrity="sha384-6sfuP2+nkcaE56Ufwv1QtvckJxueOVYuT+jDlCQS3zrL/EBF8EPQM5naxwCRIHUH" defer=""></script><script type="text/javascript" src="https://localai.io/docs/js/bundle.min.efb0bd8565c727b225e7161fba5e91da36d3ecc3384f1119f0b549fad499a11a98b1f42cdfadc24ef65861e960e23f80.js" integrity="sha384-77C9hWXHJ7Il5xYful6R2jbT7MM4TxEZ8LVJ+tSZoRqYsfQs363CTvZYYelg4j+A" crossorigin="anonymous" defer=""></script><script type="module">
    var suggestions = document.getElementById('suggestions');
    var search = document.getElementById('flexsearch');

    const flexsearchContainer = document.getElementById('FlexSearchCollapse');

    const hideFlexsearchBtn = document.getElementById('hideFlexsearch');

    const configObject = { toggle: false }
    const flexsearchContainerCollapse = new Collapse(flexsearchContainer, configObject) 

    if (search !== null) {
        document.addEventListener('keydown', inputFocus);
        flexsearchContainer.addEventListener('shown.bs.collapse', function () {
            search.focus();
        });
        
        var topHeader = document.getElementById("top-header");
        document.addEventListener('click', function(elem) {
            if (!flexsearchContainer.contains(elem.target) && !topHeader.contains(elem.target))
                flexsearchContainerCollapse.hide();
        });
    }

    hideFlexsearchBtn.addEventListener('click', () =>{
        flexsearchContainerCollapse.hide()
    })

    function inputFocus(e) {
        if (e.ctrlKey && e.key === '/') {
            e.preventDefault();
            flexsearchContainerCollapse.toggle();
        }
        if (e.key === 'Escape' ) {
            search.blur();
            
            flexsearchContainerCollapse.hide();
        }
    };

    document.addEventListener('click', function(event) {

    var isClickInsideElement = suggestions.contains(event.target);

    if (!isClickInsideElement) {
        suggestions.classList.add('d-none');
    }

    });

    


    document.addEventListener('keydown',suggestionFocus);

    function suggestionFocus(e) {
    const suggestionsHidden = suggestions.classList.contains('d-none');
    if (suggestionsHidden) return;

    const focusableSuggestions= [...suggestions.querySelectorAll('a')];
    if (focusableSuggestions.length === 0) return;

    const index = focusableSuggestions.indexOf(document.activeElement);

    if (e.key === "ArrowUp") {
        e.preventDefault();
        const nextIndex = index > 0 ? index - 1 : 0;
        focusableSuggestions[nextIndex].focus();
    }
    else if (e.key === "ArrowDown") {
        e.preventDefault();
        const nextIndex= index + 1 < focusableSuggestions.length ? index + 1 : index;
        focusableSuggestions[nextIndex].focus();
    }

    }

    


    (function(){

    var index = new FlexSearch.Document({
        
        tokenize: "forward",
        minlength:  0 ,
        cache:  100 ,
        optimize:  true ,
        document: {
        id: 'id',
        store: [
            "href", "title", "description"
        ],
        index: ["title", "description", "content"]
        }
    });


    


    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


    

    

    index.add(
            {
                id:  0 ,
                href: "\/docs\/overview\/",
                title: "Overview",
                description: "What is LocalAI?",
                content: "Welcome to LocalAI linkLocalAI is your complete AI stack for running AI models locally. Itâ€™s designed to be simple, efficient, and accessible, providing a drop-in replacement for OpenAIâ€™s API while keeping your data private and secure.\nWhy LocalAI? linkIn todayâ€™s AI landscape, privacy, control, and flexibility are paramount. LocalAI addresses these needs by:\nPrivacy First: Your data never leaves your machine Complete Control: Run models on your terms, with your hardware Open Source: MIT licensed and community-driven Flexible Deployment: From laptops to servers, with or without GPUs Extensible: Add new models and features as needed Core Components linkLocalAI is more than just a single tool - itâ€™s a complete ecosystem:\nLocalAI Core\nOpenAI-compatible API Multiple model support (LLMs, image, audio) No GPU required Fast inference with native bindings Github repository LocalAGI\nAutonomous AI agents No coding required WebUI and REST API support Extensible agent framework Github repository LocalRecall\nSemantic search Memory management Vector database Perfect for AI applications Github repository Getting Started linkThe fastest way to get started is with our one-line installer:\ncurl https://localai.io/install.sh | sh Or use Docker for a quick start:\ndocker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu For more detailed installation options and configurations, see our Getting Started guide.\nKey Features link Text Generation: Run various LLMs locally Image Generation: Create images with stable diffusion Audio Processing: Text-to-speech and speech-to-text Vision API: Image understanding and analysis Embeddings: Vector database support Functions: OpenAI-compatible function calling P2P: Distributed inference capabilities Community and Support linkLocalAI is a community-driven project. You can:\nJoin our Discord community Check out our GitHub repository Contribute to the project Share your use cases and examples Next Steps linkReady to dive in? Here are some recommended next steps:\nInstall LocalAI Explore available models Model compatibility Try out examples Join the community Check the LocalAI Github repository Check the LocalAGI Github repository License linkLocalAI is MIT licensed, created and maintained by Ettore Di Giacinto.\n"
            }
        );
    index.add(
            {
                id:  1 ,
                href: "\/docs\/getting-started\/",
                title: "Getting started",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  2 ,
                href: "\/basics\/getting_started\/",
                title: "Quickstart",
                description: "LocalAI is a free, open-source alternative to OpenAI (Anthropic, etc.), functioning as a drop-in replacement REST API for local inferencing. It allows you to run LLMs, generate images, and produce audio, all locally or on-premises with consumer-grade hardware, supporting multiple model families and architectures.\nğŸ’¡\nSecurity considerations\nIf you are exposing LocalAI remotely, make sure you protect the API endpoints adequately with a mechanism which allows to protect from the incoming traffic or alternatively, run LocalAI with API_KEY to gate the access with an API key. The API key guarantees a total access to the features (there is no role separation), and it is to be considered as likely as an admin role.\n",
                content: "LocalAI is a free, open-source alternative to OpenAI (Anthropic, etc.), functioning as a drop-in replacement REST API for local inferencing. It allows you to run LLMs, generate images, and produce audio, all locally or on-premises with consumer-grade hardware, supporting multiple model families and architectures.\nğŸ’¡\nSecurity considerations\nIf you are exposing LocalAI remotely, make sure you protect the API endpoints adequately with a mechanism which allows to protect from the incoming traffic or alternatively, run LocalAI with API_KEY to gate the access with an API key. The API key guarantees a total access to the features (there is no role separation), and it is to be considered as likely as an admin role.\nQuickstart linkUsing the Bash Installer link # Basic installation curl https://localai.io/install.sh | sh See "
            }
        );
    index.add(
            {
                id:  3 ,
                href: "\/backends\/",
                title: "Backends",
                description: "Learn how to use, manage, and develop backends in LocalAI",
                content: "Backends linkLocalAI supports a variety of backends that can be used to run different types of AI models. There are core Backends which are included, and there are containerized applications that provide the runtime environment for specific model types, such as LLMs, diffusion models, or text-to-speech models.\nManaging Backends in the UI linkThe LocalAI web interface provides an intuitive way to manage your backends:\nNavigate to the â€œBackendsâ€ section in the navigation menu Browse available backends from configured galleries Use the search bar to find specific backends by name, description, or type Filter backends by type using the quick filter buttons (LLM, Diffusion, TTS, Whisper) Install or delete backends with a single click Monitor installation progress in real-time Each backend card displays:\nBackend name and description Type of models it supports Installation status Action buttons (Install/Delete) Additional information via the info button Backend Galleries linkBackend galleries are repositories that contain backend definitions. They work similarly to model galleries but are specifically for backends.\nAdding a Backend Gallery linkYou can add backend galleries by specifying the Environment Variable LOCALAI_BACKEND_GALLERIES:\nexport LOCALAI_BACKEND_GALLERIES='[{\"name\":\"my-gallery\",\"url\":\"https://raw.githubusercontent.com/username/repo/main/backends\"}]' The URL needs to point to a valid yaml file, for example:\n- name: \"test-backend\" uri: \"quay.io/image/tests:localai-backend-test\" alias: \"foo-backend\" Where URI is the path to an OCI container image.\nBackend Gallery Structure linkA backend gallery is a collection of YAML files, each defining a backend. Hereâ€™s an example structure:\n# backends/llm-backend.yaml name: \"llm-backend\" description: \"A backend for running LLM models\" uri: \"quay.io/username/llm-backend:latest\" alias: \"llm\" tags: - \"llm\" - \"text-generation\" Pre-installing Backends linkYou can pre-install backends when starting LocalAI using the LOCALAI_EXTERNAL_BACKENDS environment variable:\nexport LOCALAI_EXTERNAL_BACKENDS=\"llm-backend,diffusion-backend\" local-ai run Creating a Backend linkTo create a new backend, you need to:\nCreate a container image that implements the LocalAI backend interface Define a backend YAML file Publish your backend to a container registry Backend Container Requirements linkYour backend container should:\nImplement the LocalAI backend interface (gRPC or HTTP) Handle model loading and inference Support the required model types Include necessary dependencies Have a top level run.sh file that will be used to run the backend Pushed to a registry so can be used in a gallery Getting started linkFor getting started, see the available backends in LocalAI here: https://github.com/mudler/LocalAI/tree/master/backend .\nFor Python based backends there is a template that can be used as starting point: https://github.com/mudler/LocalAI/tree/master/backend/python/common/template . For Golang based backends, you can see the bark-cpp backend as an example: https://github.com/mudler/LocalAI/tree/master/backend/go/bark-cpp For C++ based backends, you can see the llama-cpp backend as an example: https://github.com/mudler/LocalAI/tree/master/backend/cpp/llama-cpp Publishing Your Backend link Build your container image:\ndocker build -t quay.io/username/my-backend:latest . Push to a container registry:\ndocker push quay.io/username/my-backend:latest Add your backend to a gallery:\nCreate a YAML entry in your gallery repository Include the backend definition Make the gallery accessible via HTTP/HTTPS Backend Types linkLocalAI supports various types of backends:\nLLM Backends: For running language models Diffusion Backends: For image generation TTS Backends: For text-to-speech conversion Whisper Backends: For speech-to-text conversion "
            }
        );
    index.add(
            {
                id:  4 ,
                href: "\/docs\/getting-started\/models\/",
                title: "Install and Run Models",
                description: "To install models with LocalAI, you can:\nBrowse the Model Gallery from the Web Interface and install models with a couple of clicks. For more details, refer to the Gallery Documentation. Specify a model from the LocalAI gallery during startup, e.g., local-ai run . Use a URI to specify a model file (e.g., huggingface://..., oci://, or ollama://) when starting LocalAI, e.g., local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf. Specify a URL to a model configuration file when starting LocalAI, e.g., local-ai run https://gist.githubusercontent.com/.../phi-2.yaml. Manually install the models by copying the files into the models directory (--models). Run and Install Models via the Gallery linkTo run models available in the LocalAI gallery, you can use the WebUI or specify the model name when starting LocalAI. Models can be found in the gallery via the Web interface, the model gallery, or the CLI with: local-ai models list.\n",
                content: "To install models with LocalAI, you can:\nBrowse the Model Gallery from the Web Interface and install models with a couple of clicks. For more details, refer to the Gallery Documentation. Specify a model from the LocalAI gallery during startup, e.g., local-ai run . Use a URI to specify a model file (e.g., huggingface://..., oci://, or ollama://) when starting LocalAI, e.g., local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf. Specify a URL to a model configuration file when starting LocalAI, e.g., local-ai run https://gist.githubusercontent.com/.../phi-2.yaml. Manually install the models by copying the files into the models directory (--models). Run and Install Models via the Gallery linkTo run models available in the LocalAI gallery, you can use the WebUI or specify the model name when starting LocalAI. Models can be found in the gallery via the Web interface, the model gallery, or the CLI with: local-ai models list.\nTo install a model from the gallery, use the model name as the URI. For example, to run LocalAI with the Hermes model, execute:\nlocal-ai run hermes-2-theta-llama-3-8b To install only the model, use:\nlocal-ai models install hermes-2-theta-llama-3-8b Note: The galleries available in LocalAI can be customized to point to a different URL or a local directory. For more information on how to setup your own gallery, see the Gallery Documentation.\nRun Models via URI linkTo run models via URI, specify a URI to a model file or a configuration file when starting LocalAI. Valid syntax includes:\nfile://path/to/model huggingface://repository_id/model_file (e.g., huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf) From OCIs: oci://container_image:tag, ollama://model_id:tag From configuration files: https://gist.githubusercontent.com/.../phi-2.yaml Configuration files can be used to customize the model defaults and settings. For advanced configurations, refer to the "
            }
        );
    index.add(
            {
                id:  5 ,
                href: "\/basics\/try\/",
                title: "Try it out",
                description: "Once LocalAI is installed, you can start it (either by using docker, or the cli, or the systemd service).\nBy default the LocalAI WebUI should be accessible from http://localhost:8080. You can also use 3rd party projects to interact with LocalAI as you would use OpenAI (see also ",
                content: "Once LocalAI is installed, you can start it (either by using docker, or the cli, or the systemd service).\nBy default the LocalAI WebUI should be accessible from http://localhost:8080. You can also use 3rd party projects to interact with LocalAI as you would use OpenAI (see also "
            }
        );
    index.add(
            {
                id:  6 ,
                href: "\/docs\/getting-started\/customize-model\/",
                title: "Customizing the Model",
                description: "To customize the prompt template or the default settings of the model, a configuration file is utilized. This file must adhere to the LocalAI YAML configuration standards. For comprehensive syntax details, refer to the ",
                content: "To customize the prompt template or the default settings of the model, a configuration file is utilized. This file must adhere to the LocalAI YAML configuration standards. For comprehensive syntax details, refer to the "
            }
        );
    index.add(
            {
                id:  7 ,
                href: "\/basics\/build\/",
                title: "Build LocalAI from source",
                description: "Build linkLocalAI can be built as a container image or as a single, portable binary. Note that some model architectures might require Python libraries, which are not included in the binary.\nLocalAIâ€™s extensible architecture allows you to add your own backends, which can be written in any language, and as such the container images contains also the Python dependencies to run all the available backends (for example, in order to run backends like Diffusers that allows to generate images and videos from text).\n",
                content: "Build linkLocalAI can be built as a container image or as a single, portable binary. Note that some model architectures might require Python libraries, which are not included in the binary.\nLocalAIâ€™s extensible architecture allows you to add your own backends, which can be written in any language, and as such the container images contains also the Python dependencies to run all the available backends (for example, in order to run backends like Diffusers that allows to generate images and videos from text).\nThis section contains instructions on how to build LocalAI from source.\nBuild LocalAI locally linkRequirements linkIn order to build LocalAI locally, you need the following requirements:\nGolang \u003e= 1.21 GCC GRPC To install the dependencies follow the instructions below:\nApple Debian From source Install xcode from the App Store\nbrew install go protobuf protoc-gen-go protoc-gen-go-grpc wget apt install golang make protobuf-compiler-grpc After you have golang installed and working, you can install the required binaries for compiling the golang protobuf components via the following commands\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.34.2 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@1958fcbe2ca8bd93af633f11e97d44e567e945af make build Build linkTo build LocalAI with make:\ngit clone https://github.com/go-skynet/LocalAI cd LocalAI make build This should produce the binary local-ai\nContainer image linkRequirements:\nDocker or podman, or a container engine In order to build the LocalAI container image locally you can use docker, for example:\n# build the image docker build -t localai . docker run localai Example: Build on mac linkBuilding on Mac (M1, M2 or M3) works, but you may need to install some prerequisites using brew.\nThe below has been tested by one mac user and found to work. Note that this doesnâ€™t use Docker to run the server:\nInstall xcode from the Apps Store (needed for metalkit)\n# install build dependencies brew install abseil cmake go grpc protobuf wget protoc-gen-go protoc-gen-go-grpc # clone the repo git clone https://github.com/go-skynet/LocalAI.git cd LocalAI # build the binary make build # Download phi-2 to models/ wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf -O models/phi-2.Q2_K # Use a template from the examples cp -rf prompt-templates/ggml-gpt4all-j.tmpl models/phi-2.Q2_K.tmpl # Install the llama-cpp backend ./local-ai backends install llama-cpp # Run LocalAI ./local-ai --models-path=./models/ --debug=true # Now API is accessible at localhost:8080 curl http://localhost:8080/v1/models curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"phi-2.Q2_K\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}], \"temperature\": 0.9 }' Troubleshooting mac link If you encounter errors regarding a missing utility metal, install Xcode from the App Store.\nAfter the installation of Xcode, if you receive a xcrun error 'xcrun: error: unable to find utility \"metal\", not a developer tool or in PATH'. You might have installed the Xcode command line tools before installing Xcode, the former one is pointing to an incomplete SDK.\n# print /Library/Developer/CommandLineTools, if command line tools were installed in advance xcode-select --print-path # point to a complete SDK sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer If completions are slow, ensure that gpu-layers in your model yaml matches the number of layers from the model in use (or simply use a high number such as 256).\nIf you get a compile error: error: only virtual member functions can be marked 'final', reinstall all the necessary brew packages, clean the build, and try again.\n# reinstall build dependencies brew reinstall go grpc protobuf wget make clean make build Build backends linkLocalAI have several backends available for installation in the backend gallery. The backends can be also built by source. As backends might vary from language and dependencies that they require, the documentation will provide generic guidance for few of the backends, which can be applied with some slight modifications also to the others.\nManually linkTypically each backend include a Makefile which allow to package the backend.\nIn the LocalAI repository, for instance you can build bark-cpp by doing:\ngit clone https://github.com/go-skynet/LocalAI.git # Build the bark-cpp backend (requires cmake) make -C LocalAI/backend/go/bark-cpp build package # Build vllm backend (requires python) make -C LocalAI/backend/python/vllm With Docker linkBuilding with docker is simpler as abstracts away all the requirement, and focuses on building the final OCI images that are available in the gallery. This allows for instance also to build locally a backend and install it with LocalAI. You can refer to Backends for general guidance on how to install and develop backends.\nIn the LocalAI repository, you can build bark-cpp by doing:\ngit clone https://github.com/go-skynet/LocalAI.git # Build the bark-cpp backend (requires docker) make docker-build-bark-cpp Note that make is only by convenience, in reality it just runs a simple docker command as:\ndocker build --build-arg BUILD_TYPE=$(BUILD_TYPE) --build-arg BASE_IMAGE=$(BASE_IMAGE) -t local-ai-backend:bark-cpp -f LocalAI/backend/Dockerfile.golang --build-arg BACKEND=bark-cpp . Note:\nBUILD_TYPE can be either: cublas, hipblas, sycl_f16, sycl_f32, metal. BASE_IMAGE is tested on ubuntu:22.04 (and defaults to it) and quay.io/go-skynet/intel-oneapi-base:latest for intel/sycl "
            }
        );
    index.add(
            {
                id:  8 ,
                href: "\/basics\/container\/",
                title: "Run with container images",
                description: "LocalAI provides a variety of images to support different environments. These images are available on quay.io and Docker Hub.\n",
                content: "LocalAI provides a variety of images to support different environments. These images are available on quay.io and Docker Hub.\nAll-in-One images comes with a pre-configured set of models and backends, standard images instead do not have any model pre-configured and installed.\nFor GPU Acceleration support for Nvidia video graphic cards, use the Nvidia/CUDA images, if you donâ€™t have a GPU, use the CPU images. If you have AMD or Mac Silicon, see the build section.\nğŸ’¡\nAvailable Images Types:\nImages ending with -core are smaller images without predownload python dependencies. Use these images if you plan to use llama.cpp, stablediffusion-ncn or rwkv backends - if you are not sure which one to use, do not use these images.\nImages containing the aio tag are all-in-one images with all the features enabled, and come with an opinionated set of configuration.\nPrerequisites linkBefore you begin, ensure you have a container engine installed if you are not using the binaries. Suitable options include Docker or Podman. For installation instructions, refer to the following guides:\nInstall Docker Desktop (Mac, Windows, Linux) Install Podman (Linux) Install Docker engine (Servers) ğŸ’¡\nHardware Requirements: The hardware requirements for LocalAI vary based on the model size and quantization method used. For performance benchmarks with different backends, such as llama.cpp, visit this link. The rwkv backend is noted for its lower resource consumption.\nAll-in-one images linkAll-In-One images are images that come pre-configured with a set of models and backends to fully leverage almost all the LocalAI featureset. These images are available for both CPU and GPU environments. The AIO images are designed to be easy to use and require no configuration. Models configuration can be found here separated by size.\nIn the AIO images there are models configured with the names of OpenAI models, however, they are really backed by Open Source models. You can find the table below\nCategory Model name Real model (CPU) Real model (GPU) Text Generation gpt-4 phi-2 hermes-2-pro-mistral Multimodal Vision gpt-4-vision-preview bakllava llava-1.6-mistral Image Generation stablediffusion stablediffusion dreamshaper-8 Speech to Text whisper-1 whisper with whisper-base model \u003c= same Text to Speech tts-1 en-us-amy-low.onnx from rhasspy/piper \u003c= same Embeddings text-embedding-ada-002 all-MiniLM-L6-v2 in Q4 all-MiniLM-L6-v2 Usage linkSelect the image (CPU or GPU) and start the container with Docker:\n# CPU example docker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu # For Nvidia GPUs: # docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-11 # docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-12 LocalAI will automatically download all the required models, and the API will be available at localhost:8080.\nOr with a docker-compose file:\nversion: \"3.9\" services: api: image: localai/localai:latest-aio-cpu # For a specific version: # image: localai/localai:v3.4.0-aio-cpu # For Nvidia GPUs decomment one of the following (cuda11 or cuda12): # image: localai/localai:v3.4.0-aio-gpu-nvidia-cuda-11 # image: localai/localai:v3.4.0-aio-gpu-nvidia-cuda-12 # image: localai/localai:latest-aio-gpu-nvidia-cuda-11 # image: localai/localai:latest-aio-gpu-nvidia-cuda-12 healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/readyz\"] interval: 1m timeout: 20m retries: 5 ports: - 8080:8080 environment: - DEBUG=true # ... volumes: - ./models:/models:cached # decomment the following piece if running with Nvidia GPUs # deploy: # resources: # reservations: # devices: # - driver: nvidia # count: 1 # capabilities: [gpu] ğŸ’¡\nModels caching: The AIO image will download the needed models on the first run if not already present and store those in /models inside the container. The AIO models will be automatically updated with new versions of AIO images.\nYou can change the directory inside the container by specifying a MODELS_PATH environment variable (or --models-path).\nIf you want to use a named model or a local directory, you can mount it as a volume to /models:\ndocker run -p 8080:8080 --name local-ai -ti -v $PWD/models:/models localai/localai:latest-aio-cpu or associate a volume:\ndocker volume create localai-models docker run -p 8080:8080 --name local-ai -ti -v localai-models:/models localai/localai:latest-aio-cpu Available AIO images link Description Quay Docker Hub Latest images for CPU quay.io/go-skynet/local-ai:latest-aio-cpu localai/localai:latest-aio-cpu Versioned image (e.g. for CPU) quay.io/go-skynet/local-ai:v3.4.0-aio-cpu localai/localai:v3.4.0-aio-cpu Latest images for Nvidia GPU (CUDA11) quay.io/go-skynet/local-ai:latest-aio-gpu-nvidia-cuda-11 localai/localai:latest-aio-gpu-nvidia-cuda-11 Latest images for Nvidia GPU (CUDA12) quay.io/go-skynet/local-ai:latest-aio-gpu-nvidia-cuda-12 localai/localai:latest-aio-gpu-nvidia-cuda-12 Latest images for AMD GPU quay.io/go-skynet/local-ai:latest-aio-gpu-hipblas localai/localai:latest-aio-gpu-hipblas Latest images for Intel GPU quay.io/go-skynet/local-ai:latest-aio-gpu-intel localai/localai:latest-aio-gpu-intel Available environment variables linkThe AIO Images are inheriting the same environment variables as the base images and the environment of LocalAI (that you can inspect by calling --help). However, it supports additional environment variables available only from the container image\nVariable Default Description PROFILE Auto-detected The size of the model to use. Available: cpu, gpu-8g MODELS Auto-detected A list of models YAML Configuration file URI/URL (see also "
            }
        );
    index.add(
            {
                id:  9 ,
                href: "\/basics\/kubernetes\/",
                title: "Run with Kubernetes",
                description: "For installing LocalAI in Kubernetes, the deployment file from the examples can be used and customized as preferred:\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment.yaml For Nvidia GPUs:\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment-nvidia.yaml Alternatively, the helm chart can be used as well:\n",
                content: "For installing LocalAI in Kubernetes, the deployment file from the examples can be used and customized as preferred:\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment.yaml For Nvidia GPUs:\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment-nvidia.yaml Alternatively, the helm chart can be used as well:\n# Install the helm repository helm repo add go-skynet https://go-skynet.github.io/helm-charts/ # Update the repositories helm repo update # Get the values helm show values go-skynet/local-ai \u003e values.yaml # Edit the values if needed # vim values.yaml ... # Install the helm chart helm install local-ai go-skynet/local-ai -f values.yaml "
            }
        );
    index.add(
            {
                id:  10 ,
                href: "\/basics\/news\/",
                title: "News",
                description: "Release notes have been now moved completely over Github releases.\nYou can see the release notes here.\nOlder release notes link04-12-2023: v2.0.0 linkThis release brings a major overhaul in some backends.\n",
                content: "Release notes have been now moved completely over Github releases.\nYou can see the release notes here.\nOlder release notes link04-12-2023: v2.0.0 linkThis release brings a major overhaul in some backends.\nBreaking/important changes:\nBackend rename: llama-stable renamed to llama-ggml 1287 Prompt template changes: 1254 (extra space in roles) Apple metal bugfixes: 1365 New:\nAdded support for LLaVa and OpenAI Vision API support ( 1254 ) Python based backends are now using conda to track env dependencies ( 1144 ) Support for parallel requests ( 1290 ) Support for transformers-embeddings ( 1308 ) Watchdog for backends ( 1341 ). As https://github.com/ggerganov/llama.cpp/issues/3969 is hitting LocalAIâ€™s llama-cpp implementation, we have now a watchdog that can be used to make sure backends are not stalling. This is a generic mechanism that can be enabled for all the backends now. Whisper.cpp updates ( 1302 ) Petals backend ( 1350 ) Full LLM fine-tuning example to use with LocalAI: https://localai.io/advanced/fine-tuning/ Due to the python dependencies size of images grew in size. If you still want to use smaller images without python dependencies, you can use the corresponding images tags ending with -core.\nFull changelog: https://github.com/mudler/LocalAI/releases/tag/v2.0.0\n30-10-2023: v1.40.0 linkThis release is a preparation before v2 - the efforts now will be to refactor, polish and add new backends. Follow up on: https://github.com/mudler/LocalAI/issues/1126\nHot topics linkThis release now brings the llama-cpp backend which is a c++ backend tied to llama.cpp. It follows more closely and tracks recent versions of llama.cpp. It is not feature compatible with the current llama backend but plans are to sunset the current llama backend in favor of this one. This one will be probably be the latest release containing the older llama backend written in go and c++. The major improvement with this change is that there are less layers that could be expose to potential bugs - and as well it ease out maintenance as well.\nSupport for ROCm/HIPBLAS linkThis release bring support for AMD thanks to @65a . See more details in 1100 More CLI commands linkThanks to @jespino now the local-ai binary has more subcommands allowing to manage the gallery or try out directly inferencing, check it out!\nRelease notes\n25-09-2023: v1.30.0 linkThis is an exciting LocalAI release! Besides bug-fixes and enhancements this release brings the new backend to a whole new level by extending support to vllm and vall-e-x for audio generation!\nCheck out the documentation for vllm here and Vall-E-X here\nRelease notes\n26-08-2023: v1.25.0 linkHey everyone, Ettore here, Iâ€™m so happy to share this release out - while this summer is hot apparently doesnâ€™t stop LocalAI development :)\nThis release brings a lot of new features, bugfixes and updates! Also a big shout out to the community, this was a great release!\nAttention ğŸš¨ linkFrom this release the llama backend supports only gguf files (see 943 ). LocalAI however still supports ggml files. We ship a version of llama.cpp before that change in a separate backend, named llama-stable to allow still loading ggml files. If you were specifying the llama backend manually to load ggml files from this release you should use llama-stable instead, or do not specify a backend at all (LocalAI will automatically handle this).\nImage generation enhancements linkThe Diffusers backend got now various enhancements, including support to generate images from images, longer prompts, and support for more kernels schedulers. See the Diffusers documentation for more information.\nLora adapters linkNow itâ€™s possible to load lora adapters for llama.cpp. See 955 for more information.\nDevice management linkIt is now possible for single-devices with one GPU to specify --single-active-backend to allow only one backend active at the time 925 .\nCommunity spotlight linkResources management linkThanks to the continous community efforts (another cool contribution from dave-gray101 ) now itâ€™s possible to shutdown a backend programmatically via the API. There is an ongoing effort in the community to better handling of resources. See also the ğŸ”¥Roadmap.\nNew how-to section linkThanks to the community efforts now we have a new how-to website with various examples on how to use LocalAI. This is a great starting point for new users! We are currently working on improving it, a huge shout out to lunamidori5 from the community for the impressive efforts on this!\nğŸ’¡ More examples! link Open source autopilot? See the new addition by gruberdev in our examples on how to use Continue with LocalAI! Want to try LocalAI with Insomnia? Check out the new Insomnia example by dave-gray101 ! LocalAGI in discord! linkDid you know that we have now few cool bots in our Discord? come check them out! We also have an instance of LocalAGI ready to help you out!\nChangelog summary linkBreaking Changes ğŸ›  link feat: bump llama.cpp, add gguf support by mudler in 943 Exciting New Features ğŸ‰ link feat(Makefile): allow to restrict backend builds by mudler in 890 feat(diffusers): various enhancements by mudler in 895 feat: make initializer accept gRPC delay times by mudler in 900 feat(diffusers): add DPMSolverMultistepScheduler++, DPMSolverMultistepSchedulerSDE++, guidance_scale by mudler in 903 feat(diffusers): overcome prompt limit by mudler in 904 feat(diffusers): add img2img and clip_skip, support more kernels schedulers by mudler in 906 Usage Features by dave-gray101 in 863 feat(diffusers): be consistent with pipelines, support also depthimg2img by mudler in 926 feat: add â€“single-active-backend to allow only one backend active at the time by mudler in 925 feat: add llama-stable backend by mudler in 932 feat: allow to customize rwkv tokenizer by dave-gray101 in 937 feat: backend monitor shutdown endpoint, process based by dave-gray101 in 938 feat: Allow to load lora adapters for llama.cpp by mudler in 955 Join our Discord community! our vibrant community is growing fast, and we are always happy to help! https://discord.gg/uJAeKSAGDy\nThe full changelog is available here.\nğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ 12-08-2023: v1.24.0 ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ linkThis is release brings four(!) new additional backends to LocalAI: ğŸ¶ Bark, ğŸ¦™ AutoGPTQ, ğŸ§¨ Diffusers, ğŸ¦™ exllama and a lot of improvements!\nMajor improvements: link feat: add bark and AutoGPTQ by mudler in 871 feat: Add Diffusers by mudler in 874 feat: add API_KEY list support by neboman11 and bnusunny in 877 feat: Add exllama by mudler in 881 feat: pre-configure LocalAI galleries by mudler in 886 ğŸ¶ Bark linkBark is a text-prompted generative audio model - it combines GPT techniques to generate Audio from text. It is a great addition to LocalAI, and itâ€™s available in the container images by default.\nIt can also generate music, see the example: lion.webm\nğŸ¦™ AutoGPTQ linkAutoGPTQ is an easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\nIt is targeted mainly for GPU usage only. Check out the documentation for usage.\nğŸ¦™ Exllama linkExllama is a â€œA more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weightsâ€. It is a faster alternative to run LLaMA models on GPU.Check out the Exllama documentation for usage.\nğŸ§¨ Diffusers linkDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Currently it is experimental, and supports generation only of images so you might encounter some issues on models which werenâ€™t tested yet. Check out the Diffusers documentation for usage.\nğŸ”‘ API Keys linkThanks to the community contributions now itâ€™s possible to specify a list of API keys that can be used to gate API requests.\nAPI Keys can be specified with the API_KEY environment variable as a comma-separated list of keys.\nğŸ–¼ï¸ Galleries linkNow by default the model-gallery repositories are configured in the container images\nğŸ’¡ New project linkLocalAGI is a simple agent that uses LocalAI functions to have a full locally runnable assistant (with no API keys needed).\nSee it here in action planning a trip for San Francisco!\nThe full changelog is available here.\nğŸ”¥ğŸ”¥ 29-07-2023: v1.23.0 ğŸš€ linkThis release focuses mostly on bugfixing and updates, with just a couple of new features:\nfeat: add rope settings and negative prompt, drop grammar backend by mudler in 797 Added CPU information to entrypoint.sh by @finger42 in 794 feat: cancel stream generation if client disappears by @tmm1 in 792 Most notably, this release brings important fixes for CUDA (and not only):\nfix: add rope settings during model load, fix CUDA by mudler in 821 fix: select function calls if â€™nameâ€™ is set in the request by mudler in 827 fix: symlink libphonemize in the container by mudler in 831 notifications From this release OpenAI functions are available in the llama backend. The llama-grammar has been deprecated. See also OpenAI functions.\nThe full changelog is available here\nğŸ”¥ğŸ”¥ğŸ”¥ 23-07-2023: v1.22.0 ğŸš€ link feat: add llama-master backend by mudler in 752 [build] pass build type to cmake on libtransformers.a build by @TonDar0n in 741 feat: resolve JSONSchema refs (planners) by mudler in 774 feat: backends improvements by mudler in 778 feat(llama2): add template for chat messages by dave-gray101 in 782 notifications From this release to use the OpenAI functions you need to use the llama-grammar backend. It has been added a llama backend for tracking llama.cpp master and llama-grammar for the grammar functionalities that have not been merged yet upstream. See also OpenAI functions. Until the feature is merged we will have two llama backends.\nHuggingface embeddings linkIn this release is now possible to specify to LocalAI external gRPC backends that can be used for inferencing 778 . It is now possible to write internal backends in any language, and a huggingface-embeddings backend is now available in the container image to be used with https://github.com/UKPLab/sentence-transformers. See also Embeddings.\nLLaMa 2 has been released! linkThanks to the community effort now LocalAI supports templating for LLaMa2! more at: 782 until we update the model gallery with LLaMa2 models!\nOfficial langchain integration linkProgress has been made to support LocalAI with langchain. See: https://github.com/langchain-ai/langchain/pull/8134\nğŸ”¥ğŸ”¥ğŸ”¥ 17-07-2023: v1.21.0 ğŸš€ link [whisper] Partial support for verbose_json format in transcribe endpoint by @ldotlopez in 721 LocalAI functions by @mudler in 726 gRPC-based backends by @mudler in 743 falcon support (7b and 40b) with ggllm.cpp by @mudler in 743 LocalAI functions linkThis allows to run OpenAI functions as described in the OpenAI blog post and documentation: https://openai.com/blog/function-calling-and-other-api-updates.\nThis is a video of running the same example, locally with LocalAI: And here when it actually picks to reply to the user instead of using functions! Note: functions are supported only with llama.cpp-compatible models.\nA full example is available here: https://github.com/mudler/LocalAI-examples/tree/main/functions\ngRPC backends linkThis is an internal refactor which is not user-facing, however, it allows to ease out maintenance and addition of new backends to LocalAI!\nfalcon support linkNow Falcon 7b and 40b models compatible with https://github.com/cmp-nct/ggllm.cpp are supported as well.\nThe former, ggml-based backend has been renamed to falcon-ggml.\nDefault pre-compiled binaries linkFrom this release the default behavior of images has changed. Compilation is not triggered on start automatically, to recompile local-ai from scratch on start and switch back to the old behavior, you can set REBUILD=true in the environment variables. Rebuilding can be necessary if your CPU and/or architecture is old and the pre-compiled binaries are not compatible with your platform. See the build section for more information.\nFull release changelog\nğŸ”¥ğŸ”¥ğŸ”¥ 28-06-2023: v1.20.0 ğŸš€ linkExciting New Features ğŸ‰ link Add Text-to-Audio generation with go-piper by mudler in 649 See API endpoints in our documentation. Add gallery repository by mudler in 663 . See models for documentation. Container images link Standard (GPT + stablediffusion): quay.io/go-skynet/local-ai:v1.20.0 FFmpeg: quay.io/go-skynet/local-ai:v1.20.0-ffmpeg CUDA 11+FFmpeg: quay.io/go-skynet/local-ai:v1.20.0-gpu-nvidia-cuda11-ffmpeg CUDA 12+FFmpeg: quay.io/go-skynet/local-ai:v1.20.0-gpu-nvidia-cuda12-ffmpeg Updates linkUpdates to llama.cpp, go-transformers, gpt4all.cpp and rwkv.cpp.\nThe NUMA option was enabled by mudler in 684 , along with many new parameters (mmap,mmlock, ..). See "
            }
        );
    index.add(
            {
                id:  11 ,
                href: "\/features\/",
                title: "Features",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  12 ,
                href: "\/features\/gpu-acceleration\/",
                title: "âš¡ GPU acceleration",
                description: " warning Section under construction\nThis section contains instruction on how to use LocalAI with GPU acceleration.\nâš¡\nFor acceleration for AMD or Metal HW is still in development, for additional details see the build\nAutomatic Backend Detection linkWhen you install a model from the gallery (or a YAML file), LocalAI intelligently detects the required backend and your systemâ€™s capabilities, then downloads the correct version for you. Whether youâ€™re running on a standard CPU, an NVIDIA GPU, an AMD GPU, or an Intel GPU, LocalAI handles it automatically.\n",
                content: " warning Section under construction\nThis section contains instruction on how to use LocalAI with GPU acceleration.\nâš¡\nFor acceleration for AMD or Metal HW is still in development, for additional details see the build\nAutomatic Backend Detection linkWhen you install a model from the gallery (or a YAML file), LocalAI intelligently detects the required backend and your systemâ€™s capabilities, then downloads the correct version for you. Whether youâ€™re running on a standard CPU, an NVIDIA GPU, an AMD GPU, or an Intel GPU, LocalAI handles it automatically.\nFor advanced use cases or to override auto-detection, you can use the LOCALAI_FORCE_META_BACKEND_CAPABILITY environment variable. Here are the available options:\ndefault: Forces CPU-only backend. This is the fallback if no specific hardware is detected. nvidia: Forces backends compiled with CUDA support for NVIDIA GPUs. amd: Forces backends compiled with ROCm support for AMD GPUs. intel: Forces backends compiled with SYCL/oneAPI support for Intel GPUs. Model configuration linkDepending on the model architecture and backend used, there might be different ways to enable GPU acceleration. It is required to configure the model you intend to use with a YAML config file. For example, for llama.cpp workloads a configuration file might look like this (where gpu_layers is the number of layers to offload to the GPU):\nname: my-model-name # Default model parameters parameters: # Relative to the models path model: llama.cpp-model.ggmlv3.q5_K_M.bin context_size: 1024 threads: 1 f16: true # enable with GPU acceleration gpu_layers: 22 # GPU Layers (only used when built with cublas) For diffusers instead, it might look like this instead:\nname: stablediffusion parameters: model: toonyou_beta6.safetensors backend: diffusers step: 30 f16: true diffusers: pipeline_type: StableDiffusionPipeline cuda: true enable_parameters: \"negative_prompt,num_inference_steps,clip_skip\" scheduler_type: \"k_dpmpp_sde\" CUDA(NVIDIA) acceleration linkRequirements linkRequirement: nvidia-container-toolkit (installation instructions 1 2)\nIf using a system with SELinux, ensure you have the policies installed, such as those provided by nvidia\nTo check what CUDA version do you need, you can either run nvidia-smi or nvcc --version.\nAlternatively, you can also check nvidia-smi with docker:\ndocker run --runtime=nvidia --rm nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi To use CUDA, use the images with the cublas tag, for example.\nThe image list is on quay:\nCUDA 11 tags: master-gpu-nvidia-cuda-11, v1.40.0-gpu-nvidia-cuda-11, â€¦ CUDA 12 tags: master-gpu-nvidia-cuda-12, v1.40.0-gpu-nvidia-cuda-12, â€¦ In addition to the commands to run LocalAI normally, you need to specify --gpus all to docker, for example:\ndocker run --rm -ti --gpus all -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v1.40.0-gpu-nvidia-cuda12 If the GPU inferencing is working, you should be able to see something like:\n5:22PM DBG Loading model in memory from file: /models/open-llama-7b-q4_0.bin ggml_init_cublas: found 1 CUDA devices: Device 0: Tesla T4 llama.cpp: loading model from /models/open-llama-7b-q4_0.bin llama_model_load_internal: format = ggjt v3 (latest) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 1024 llama_model_load_internal: n_embd = 4096 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 32 llama_model_load_internal: n_layer = 32 llama_model_load_internal: n_rot = 128 llama_model_load_internal: ftype = 2 (mostly Q4_0) llama_model_load_internal: n_ff = 11008 llama_model_load_internal: n_parts = 1 llama_model_load_internal: model size = 7B llama_model_load_internal: ggml ctx size = 0.07 MB llama_model_load_internal: using CUDA for GPU acceleration llama_model_load_internal: mem required = 4321.77 MB (+ 1026.00 MB per state) llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer llama_model_load_internal: offloading 10 repeating layers to GPU llama_model_load_internal: offloaded 10/35 layers to GPU llama_model_load_internal: total VRAM used: 1598 MB ................................................................................................... llama_init_from_file: kv self size = 512.00 MB ROCM(AMD) acceleration linkThere are a limited number of tested configurations for ROCm systems however most newer deditated GPU consumer grade devices seem to be supported under the current ROCm6 implementation.\nDue to the nature of ROCm it is best to run all implementations in containers as this limits the number of packages required for installation on host system, compatibility and package versions for dependencies across all variations of OS must be tested independently if desired, please refer to the build documentation.\nRequirements link ROCm 6.x.x compatible GPU/accelerator OS: Ubuntu (22.04, 20.04), RHEL (9.3, 9.2, 8.9, 8.8), SLES (15.5, 15.4) Installed to host: amdgpu-dkms and rocm \u003e=6.0.0 as per ROCm documentation. Recommendations link Do not use on a system running Wayland. If running with Xorg do not use GPU assigned for compute for desktop rendering. Ensure at least 100GB of free space on disk hosting container runtime and storing images prior to installation. Limitations linkOngoing verification testing of ROCm compatability with integrated backends. Please note the following list of verified backends and devices.\nLocalAI hipblas images are built against the following targets: gfx900,gfx906,gfx908,gfx940,gfx941,gfx942,gfx90a,gfx1030,gfx1031,gfx1100,gfx1101\nIf your device is not one of these you must specify the corresponding GPU_TARGETS and specify REBUILD=true. Otherwise you donâ€™t need to specify these in the commands below.\nVerified linkThe devices in the following list have been tested with hipblas images running ROCm 6.0.0\nBackend Verified Devices llama.cpp yes Radeon VII (gfx906) diffusers yes Radeon VII (gfx906) piper yes Radeon VII (gfx906) whisper no none bark no none coqui no none transformers no none exllama no none exllama2 no none mamba no none sentencetransformers no none transformers-musicgen no none vall-e-x no none vllm no none You can help by expanding this list.\nSystem Prep link Check your GPU LLVM target is compatible with the version of ROCm. This can be found in the LLVM Docs. Check which ROCm version is compatible with your LLVM target and your chosen OS (pay special attention to supported kernel versions). See the following for compatability for (ROCm 6.0.0) or (ROCm 6.0.2) Install you chosen version of the dkms and rocm (it is recommended that the native package manager be used for this process for any OS as version changes are executed more easily via this method if updates are required). Take care to restart after installing amdgpu-dkms and before installing rocm, for details regarding this see the installation documentation for your chosen OS (6.0.2 or 6.0.0) Deploy. Yes itâ€™s that easy. Setup Example (Docker/containerd) linkThe following are examples of the ROCm specific configuration elements required.\n# docker-compose.yaml # For full functionality select a non-'core' image, version locking the image is recommended for debug purposes. image: quay.io/go-skynet/local-ai:master-aio-gpu-hipblas environment: - DEBUG=true # If your gpu is not already included in the current list of default targets the following build details are required. - REBUILD=true - BUILD_TYPE=hipblas - GPU_TARGETS=gfx906 # Example for Radeon VII devices: # AMD GPU only require the following devices be passed through to the container for offloading to occur. - /dev/dri - /dev/kfd The same can also be executed as a run for your container runtime\ndocker run \\ -e DEBUG=true \\ -e REBUILD=true \\ -e BUILD_TYPE=hipblas \\ -e GPU_TARGETS=gfx906 \\ --device /dev/dri \\ --device /dev/kfd \\ quay.io/go-skynet/local-ai:master-aio-gpu-hipblas Please ensure to add all other required environment variables, port forwardings, etc to your compose file or run command.\nThe rebuild process will take some time to complete when deploying these containers and it is recommended that you pull the image prior to deployment as depending on the version these images may be ~20GB in size.\nExample (k8s) (Advanced Deployment/WIP) linkFor k8s deployments there is an additional step required before deployment, this is the deployment of the ROCm/k8s-device-plugin. For any k8s environment the documentation provided by AMD from the ROCm project should be successful. It is recommended that if you use rke2 or OpenShift that you deploy the SUSE or RedHat provided version of this resource to ensure compatability. After this has been completed the helm chart from go-skynet can be configured and deployed mostly un-edited.\nThe following are details of the changes that should be made to ensure proper function. While these details may be configurable in the values.yaml development of this Helm chart is ongoing and is subject to change.\nThe following details indicate the final state of the localai deployment relevant to GPU function.\napiVersion: apps/v1 kind: Deployment metadata: name: {NAME}-local-ai ... spec: ... template: ... spec: containers: - env: - name: HIP_VISIBLE_DEVICES value: '0' # This variable indicates the devices available to container (0:device1 1:device2 2:device3) etc. # For multiple devices (say device 1 and 3) the value would be equivalent to HIP_VISIBLE_DEVICES=\"0,2\" # Please take note of this when an iGPU is present in host system as compatability is not assured. ... resources: limits: amd.com/gpu: '1' requests: amd.com/gpu: '1' This configuration has been tested on a â€˜customâ€™ cluster managed by SUSE Rancher that was deployed on top of Ubuntu 22.04.4, certification of other configuration is ongoing and compatability is not guaranteed.\nNotes link When installing the ROCM kernel driver on your system ensure that you are installing an equal or newer version that that which is currently implemented in LocalAI (6.0.0 at time of writing). AMD documentation indicates that this will ensure functionality however your mileage may vary depending on the GPU and distro you are using. If you encounter an Error 413 on attempting to upload an audio file or image for whisper or llava/bakllava on a k8s deployment, note that the ingress for your deployment may require the annotation nginx.ingress.kubernetes.io/proxy-body-size: \"25m\" to allow larger uploads. This may be included in future versions of the helm chart. Intel acceleration (sycl) linkRequirements linkIf building from source, you need to install Intel oneAPI Base Toolkit and have the Intel drivers available in the system.\nContainer images linkTo use SYCL, use the images with gpu-intel in the tag, for example v3.4.0-gpu-intel, â€¦\nThe image list is on quay.\nExample linkTo run LocalAI with Docker and sycl starting phi-2, you can use the following command as an example:\ndocker run -e DEBUG=true --privileged -ti -v $PWD/models:/models -p 8080:8080 -v /dev/dri:/dev/dri --rm quay.io/go-skynet/local-ai:master-gpu-intel phi-2 Notes linkIn addition to the commands to run LocalAI normally, you need to specify --device /dev/dri to docker, for example:\ndocker run --rm -ti --device /dev/dri -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v3.4.0-gpu-intel Note also that sycl does have a known issue to hang with mmap: true. You have to disable it in the model configuration if explicitly enabled.\nVulkan acceleration linkRequirements linkIf using nvidia, follow the steps in the CUDA section to configure your docker runtime to allow access to the GPU.\nContainer images linkTo use Vulkan, use the images with the vulkan tag, for example v3.4.0-gpu-vulkan.\nExample linkTo run LocalAI with Docker and Vulkan, you can use the following command as an example:\ndocker run -p 8080:8080 -e DEBUG=true -v $PWD/models:/models localai/localai:latest-gpu-vulkan Notes linkIn addition to the commands to run LocalAI normally, you need to specify additional flags to pass the GPU hardware to the container.\nThese flags are the same as the sections above, depending on the hardware, for nvidia, AMD or Intel.\nIf you have mixed hardware, you can pass flags for multiple GPUs, for example:\ndocker run -p 8080:8080 -e DEBUG=true -v $PWD/models:/models \\ --gpus=all \\ # nvidia passthrough --device /dev/dri --device /dev/kfd \\ # AMD/Intel passthrough localai/localai:latest-gpu-vulkan "
            }
        );
    index.add(
            {
                id:  13 ,
                href: "\/features\/text-generation\/",
                title: "ğŸ“– Text generation (GPT)",
                description: "LocalAI supports generating text with GPT with llama.cpp and other backends (such as rwkv.cpp as ) see also the Model compatibility for an up-to-date list of the supported model families.\nNote:\nYou can also specify the model name as part of the OpenAI token. If only one model is available, the API will use it for all the requests. API Reference linkChat completions linkhttps://platform.openai.com/docs/api-reference/chat\n",
                content: "LocalAI supports generating text with GPT with llama.cpp and other backends (such as rwkv.cpp as ) see also the Model compatibility for an up-to-date list of the supported model families.\nNote:\nYou can also specify the model name as part of the OpenAI token. If only one model is available, the API will use it for all the requests. API Reference linkChat completions linkhttps://platform.openai.com/docs/api-reference/chat\nFor example, to generate a chat completion, you can send a POST request to the /v1/chat/completions endpoint with the instruction as the request body:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-koala-7b-model-q4_0-r2.bin\", \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}], \"temperature\": 0.7 }' Available additional parameters: top_p, top_k, max_tokens\nEdit completions linkhttps://platform.openai.com/docs/api-reference/edits\nTo generate an edit completion you can send a POST request to the /v1/edits endpoint with the instruction as the request body:\ncurl http://localhost:8080/v1/edits -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-koala-7b-model-q4_0-r2.bin\", \"instruction\": \"rephrase\", \"input\": \"Black cat jumped out of the window\", \"temperature\": 0.7 }' Available additional parameters: top_p, top_k, max_tokens.\nCompletions linkhttps://platform.openai.com/docs/api-reference/completions\nTo generate a completion, you can send a POST request to the /v1/completions endpoint with the instruction as per the request body:\ncurl http://localhost:8080/v1/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-koala-7b-model-q4_0-r2.bin\", \"prompt\": \"A long time ago in a galaxy far, far away\", \"temperature\": 0.7 }' Available additional parameters: top_p, top_k, max_tokens\nList models linkYou can list all the models available with:\ncurl http://localhost:8080/v1/models Backends linkRWKV linkRWKV support is available through llama.cpp (see below)\nllama.cpp linkllama.cpp is a popular port of Facebookâ€™s LLaMA model in C/C++.\nnotifications The ggml file format has been deprecated. If you are using ggml models and you are configuring your model with a YAML file, specify, use a LocalAI version older than v2.25.0. For gguf models, use the llama backend. The go backend is deprecated as well but still available as go-llama.\nFeatures linkThe llama.cpp model supports the following features:\nğŸ“– Text generation (GPT) ğŸ§  Embeddings ğŸ”¥ OpenAI functions âœï¸ Constrained grammars Setup linkLocalAI supports llama.cpp models out of the box. You can use the llama.cpp model in the same way as any other model.\nManual setup linkIt is sufficient to copy the ggml or gguf model files in the models folder. You can refer to the model in the model parameter in the API calls.\n"
            }
        );
    index.add(
            {
                id:  14 ,
                href: "\/features\/reranker\/",
                title: "ğŸ“ˆ Reranker",
                description: "A reranking model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks. Given a query and a set of documents, it will output similarity scores.\nWe can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.\nLocalAI supports reranker models, and you can use them by using the rerankers backend, which uses rerankers.\n",
                content: "A reranking model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks. Given a query and a set of documents, it will output similarity scores.\nWe can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.\nLocalAI supports reranker models, and you can use them by using the rerankers backend, which uses rerankers.\nUsage linkYou can test rerankers by using container images with python (this does NOT work with core images) and a model config file like this, or by installing cross-encoder from the gallery in the UI:\nname: jina-reranker-v1-base-en backend: rerankers parameters: model: cross-encoder # optionally: # type: flashrank # diffusers: # pipeline_type: en # to specify the english language and test it with:\ncurl http://localhost:8080/v1/rerank \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"jina-reranker-v1-base-en\", \"query\": \"Organic skincare products for sensitive skin\", \"documents\": [ \"Eco-friendly kitchenware for modern homes\", \"Biodegradable cleaning supplies for eco-conscious consumers\", \"Organic cotton baby clothes for sensitive skin\", \"Natural organic skincare range for sensitive skin\", \"Tech gadgets for smart homes: 2024 edition\", \"Sustainable gardening tools and compost solutions\", \"Sensitive skin-friendly facial cleansers and toners\", \"Organic food wraps and storage solutions\", \"All-natural pet food for dogs with allergies\", \"Yoga mats made from recycled materials\" ], \"top_n\": 3 }' "
            }
        );
    index.add(
            {
                id:  15 ,
                href: "\/features\/text-to-audio\/",
                title: "ğŸ—£ Text to audio (TTS)",
                description: "API Compatibility linkThe LocalAI TTS API is compatible with the OpenAI TTS API and the Elevenlabs API.\n",
                content: "API Compatibility linkThe LocalAI TTS API is compatible with the OpenAI TTS API and the Elevenlabs API.\nLocalAI API linkThe /tts endpoint can also be used to generate speech from text.\nUsage linkInput: input, model\nFor example, to generate an audio file, you can send a POST request to the /tts endpoint with the instruction as the request body:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"input\": \"Hello world\", \"model\": \"tts\" }' Returns an audio/wav file.\nBackends linkğŸ¸ Coqui linkRequired: Donâ€™t use LocalAI images ending with the -core tag,. Python dependencies are required in order to use this backend.\nCoqui works without any configuration, to test it, you can run the following curl command:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"coqui\", \"model\": \"tts_models/en/ljspeech/glow-tts\", \"input\":\"Hello, this is a test!\" }' You can use the env variable COQUI_LANGUAGE to set the language used by the coqui backend.\nYou can also use config files to configure tts models (see section below on how to use config files).\nBark linkBark allows to generate audio from text prompts.\nThis is an extra backend - in the container is already available and there is nothing to do for the setup.\nModel setup linkThere is nothing to be done for the model setup. You can already start to use bark. The models will be downloaded the first time you use the backend.\nUsage linkUse the tts endpoint by specifying the bark backend:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"bark\", \"input\":\"Hello!\" }' | aplay To specify a voice from https://github.com/suno-ai/bark#-voice-presets ( https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c ), use the model parameter:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"bark\", \"input\":\"Hello!\", \"model\": \"v2/en_speaker_4\" }' | aplay Piper linkTo install the piper audio models manually:\nDownload Voices from https://github.com/rhasspy/piper/releases/tag/v0.0.2 Extract the .tar.tgz files (.onnx,.json) inside models Run the following command to test the model is working To use the tts endpoint, run the following command. You can specify a backend with the backend parameter. For example, to use the piper backend:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"model\":\"it-riccardo_fasol-x-low.onnx\", \"backend\": \"piper\", \"input\": \"Ciao, sono Ettore\" }' | aplay Note:\naplay is a Linux command. You can use other tools to play the audio file. The model name is the filename with the extension. The model name is case sensitive. LocalAI must be compiled with the GO_TAGS=tts flag. Transformers-musicgen linkLocalAI also has experimental support for transformers-musicgen for the generation of short musical compositions. Currently, this is implemented via the same requests used for text to speech:\ncurl --request POST \\ --url http://localhost:8080/tts \\ --header 'Content-Type: application/json' \\ --data '{ \"backend\": \"transformers-musicgen\", \"model\": \"facebook/musicgen-medium\", \"input\": \"Cello Rave\" }' | aplay Future versions of LocalAI will expose additional control over audio generation beyond the text prompt.\nVall-E-X linkVALL-E-X is an open source implementation of Microsoftâ€™s VALL-E X zero-shot TTS model.\nSetup linkThe backend will automatically download the required files in order to run the model.\nThis is an extra backend - in the container is already available and there is nothing to do for the setup. If you are building manually, you need to install Vall-E-X manually first.\nUsage linkUse the tts endpoint by specifying the vall-e-x backend:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"vall-e-x\", \"input\":\"Hello!\" }' | aplay Voice cloning linkIn order to use voice cloning capabilities you must create a YAML configuration file to setup a model:\nname: cloned-voice backend: vall-e-x parameters: model: \"cloned-voice\" tts: vall-e: # The path to the audio file to be cloned # relative to the models directory # Max 15s audio_path: \"audio-sample.wav\" Then you can specify the model name in the requests:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"model\": \"cloned-voice\", \"input\":\"Hello!\" }' | aplay Using config files linkYou can also use a config-file to specify TTS models and their parameters.\nIn the following example we define a custom config to load the xtts_v2 model, and specify a voice and language.\nname: xtts_v2 backend: coqui parameters: language: fr model: tts_models/multilingual/multi-dataset/xtts_v2 tts: voice: Ana Florence With this config, you can now use the following curl command to generate a text-to-speech audio file:\ncurl -L http://localhost:8080/tts \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"xtts_v2\", \"input\": \"Bonjour, je suis Ana Florence. Comment puis-je vous aider?\" }' | aplay Response format linkTo provide some compatibility with OpenAI API regarding response_format, ffmpeg must be installed (or a docker image including ffmpeg used) to leverage converting the generated wav file before the api provide its response.\nWarning regarding a change in behaviour. Before this addition, the parameter was ignored and a wav file was always returned, with potential codec errors later in the integration (like trying to decode a mp3 file from a wav, which is the default format used by OpenAI)\nSupported format thanks to ffmpeg are wav, mp3, aac, flac, opus, defaulting to wav if an unknown or no format is provided.\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"input\": \"Hello world\", \"model\": \"tts\", \"response_format\": \"mp3\" }' If a response_format is added in the query (other than wav) and ffmpeg is not available, the call will fail.\n"
            }
        );
    index.add(
            {
                id:  16 ,
                href: "\/features\/image-generation\/",
                title: "ğŸ¨ Image generation",
                description: " (Generated with AnimagineXL)\nLocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.\nUsage linkOpenAI docs: https://platform.openai.com/docs/api-reference/images/create\n",
                content: " (Generated with AnimagineXL)\nLocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.\nUsage linkOpenAI docs: https://platform.openai.com/docs/api-reference/images/create\nTo generate an image you can send a POST request to the /v1/images/generations endpoint with the instruction as the request body:\n# 512x512 is supported too curl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{ \"prompt\": \"A cute baby sea otter\", \"size\": \"256x256\" }' Available additional parameters: mode, step.\nNote: To set a negative prompt, you can split the prompt with |, for instance: a cute baby sea otter|malformed.\ncurl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{ \"prompt\": \"floating hair, portrait, ((loli)), ((one girl)), cute face, hidden hands, asymmetrical bangs, beautiful detailed eyes, eye shadow, hair ornament, ribbons, bowties, buttons, pleated skirt, (((masterpiece))), ((best quality)), colorful|((part of the head)), ((((mutated hands and fingers)))), deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, Octane renderer, lowres, bad anatomy, bad hands, text\", \"size\": \"256x256\" }' Backends linkstablediffusion-ggml linkThis backend is based on stable-diffusion.cpp. Every model supported by that backend is supported indeed with LocalAI.\nSetup linkThere are already several models in the gallery that are available to install and get up and running with this backend, you can for example run flux by searching it in the Model gallery (flux.1-dev-ggml) or start LocalAI with run:\nlocal-ai run flux.1-dev-ggml To use a custom model, you can follow these steps:\nCreate a model file stablediffusion.yaml in the models folder: name: stablediffusion backend: stablediffusion-ggml parameters: model: gguf_model.gguf step: 25 cfg_scale: 4.5 options: - \"clip_l_path:clip_l.safetensors\" - \"clip_g_path:clip_g.safetensors\" - \"t5xxl_path:t5xxl-Q5_0.gguf\" - \"sampler:euler\" Download the required assets to the models repository Start LocalAI Diffusers linkDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. LocalAI has a diffusers backend which allows image generation using the diffusers library.\n(Generated with AnimagineXL)\nModel setup linkThe models will be downloaded the first time you use the backend from huggingface automatically.\nCreate a model configuration file in the models directory, for instance to use Linaqruf/animagine-xl with CPU:\nname: animagine-xl parameters: model: Linaqruf/animagine-xl backend: diffusers # Force CPU usage - set to true for GPU f16: false diffusers: cuda: false # Enable for GPU usage (CUDA) scheduler_type: euler_a Dependencies linkThis is an extra backend - in the container is already available and there is nothing to do for the setup. Do not use core images (ending with -core). If you are building manually, see the build instructions.\nModel setup linkThe models will be downloaded the first time you use the backend from huggingface automatically.\nCreate a model configuration file in the models directory, for instance to use Linaqruf/animagine-xl with CPU:\nname: animagine-xl parameters: model: Linaqruf/animagine-xl backend: diffusers cuda: true f16: true diffusers: scheduler_type: euler_a Local models linkYou can also use local models, or modify some parameters like clip_skip, scheduler_type, for instance:\nname: stablediffusion parameters: model: toonyou_beta6.safetensors backend: diffusers step: 30 f16: true cuda: true diffusers: pipeline_type: StableDiffusionPipeline enable_parameters: \"negative_prompt,num_inference_steps,clip_skip\" scheduler_type: \"k_dpmpp_sde\" clip_skip: 11 cfg_scale: 8 Configuration parameters linkThe following parameters are available in the configuration file:\nParameter Description Default f16 Force the usage of float16 instead of float32 false step Number of steps to run the model for 30 cuda Enable CUDA acceleration false enable_parameters Parameters to enable for the model negative_prompt,num_inference_steps,clip_skip scheduler_type Scheduler type k_dpp_sde cfg_scale Configuration scale 8 clip_skip Clip skip None pipeline_type Pipeline type AutoPipelineForText2Image lora_adapters A list of lora adapters (file names relative to model directory) to apply None lora_scales A list of lora scales (floats) to apply None There are available several types of schedulers:\nScheduler Description ddim DDIM pndm PNDM heun Heun unipc UniPC euler Euler euler_a Euler a lms LMS k_lms LMS Karras dpm_2 DPM2 k_dpm_2 DPM2 Karras dpm_2_a DPM2 a k_dpm_2_a DPM2 a Karras dpmpp_2m DPM++ 2M k_dpmpp_2m DPM++ 2M Karras dpmpp_sde DPM++ SDE k_dpmpp_sde DPM++ SDE Karras dpmpp_2m_sde DPM++ 2M SDE k_dpmpp_2m_sde DPM++ 2M SDE Karras Pipelines types available:\nPipeline type Description StableDiffusionPipeline Stable diffusion pipeline StableDiffusionImg2ImgPipeline Stable diffusion image to image pipeline StableDiffusionDepth2ImgPipeline Stable diffusion depth to image pipeline DiffusionPipeline Diffusion pipeline StableDiffusionXLPipeline Stable diffusion XL pipeline StableVideoDiffusionPipeline Stable video diffusion pipeline AutoPipelineForText2Image Automatic detection pipeline for text to image VideoDiffusionPipeline Video diffusion pipeline StableDiffusion3Pipeline Stable diffusion 3 pipeline FluxPipeline Flux pipeline FluxTransformer2DModel Flux transformer 2D model SanaPipeline Sana pipeline Advanced: Additional parameters linkAdditional arbitrarly parameters can be specified in the option field in key/value separated by ::\nname: animagine-xl # ... options: - \"cfg_scale:6\" Note: There is no complete parameter list. Any parameter can be passed arbitrarly and is passed to the model directly as argument to the pipeline. Different pipelines/implementations support different parameters.\nThe example above, will result in the following python code when generating images:\npipe( prompt=\"A cute baby sea otter\", # Options passed via API size=\"256x256\", # Options passed via API cfg_scale=6 # Additional parameter passed via configuration file ) Usage linkText to Image linkUse the image generation endpoint with the model name from the configuration file:\ncurl http://localhost:8080/v1/images/generations \\ -H \"Content-Type: application/json\" \\ -d '{ \"prompt\": \"|\", \"model\": \"animagine-xl\", \"step\": 51, \"size\": \"1024x1024\" }' Image to Image linkhttps://huggingface.co/docs/diffusers/using-diffusers/img2img\nAn example model (GPU):\nname: stablediffusion-edit parameters: model: nitrosocke/Ghibli-Diffusion backend: diffusers step: 25 cuda: true f16: true diffusers: pipeline_type: StableDiffusionImg2ImgPipeline enable_parameters: \"negative_prompt,num_inference_steps,image\" IMAGE_PATH=/path/to/your/image (echo -n '{\"file\": \"'; base64 $IMAGE_PATH; echo '\", \"prompt\": \"a sky background\",\"size\": \"512x512\",\"model\":\"stablediffusion-edit\"}') | curl -H \"Content-Type: application/json\" -d @- http://localhost:8080/v1/images/generations Depth to Image linkhttps://huggingface.co/docs/diffusers/using-diffusers/depth2img\nname: stablediffusion-depth parameters: model: stabilityai/stable-diffusion-2-depth backend: diffusers step: 50 # Force CPU usage f16: true cuda: true diffusers: pipeline_type: StableDiffusionDepth2ImgPipeline enable_parameters: \"negative_prompt,num_inference_steps,image\" cfg_scale: 6 (echo -n '{\"file\": \"'; base64 ~/path/to/image.jpeg; echo '\", \"prompt\": \"a sky background\",\"size\": \"512x512\",\"model\":\"stablediffusion-depth\"}') | curl -H \"Content-Type: application/json\" -d @- http://localhost:8080/v1/images/generations img2vid link name: img2vid parameters: model: stabilityai/stable-video-diffusion-img2vid backend: diffusers step: 25 # Force CPU usage f16: true cuda: true diffusers: pipeline_type: StableVideoDiffusionPipeline (echo -n '{\"file\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png?download=true\",\"size\": \"512x512\",\"model\":\"img2vid\"}') | curl -H \"Content-Type: application/json\" -X POST -d @- http://localhost:8080/v1/images/generations txt2vid link name: txt2vid parameters: model: damo-vilab/text-to-video-ms-1.7b backend: diffusers step: 25 # Force CPU usage f16: true cuda: true diffusers: pipeline_type: VideoDiffusionPipeline cuda: true (echo -n '{\"prompt\": \"spiderman surfing\",\"size\": \"512x512\",\"model\":\"txt2vid\"}') | curl -H \"Content-Type: application/json\" -X POST -d @- http://localhost:8080/v1/images/generations "
            }
        );
    index.add(
            {
                id:  17 ,
                href: "\/features\/object-detection\/",
                title: "ğŸ” Object detection",
                description: "LocalAI supports object detection through various backends. This feature allows you to identify and locate objects within images with high accuracy and real-time performance. Currently, RF-DETR is available as an implementation.\n",
                content: "LocalAI supports object detection through various backends. This feature allows you to identify and locate objects within images with high accuracy and real-time performance. Currently, RF-DETR is available as an implementation.\nOverview linkObject detection in LocalAI is implemented through dedicated backends that can identify and locate objects within images. Each backend provides different capabilities and model architectures.\nKey Features:\nReal-time object detection High accuracy detection with bounding boxes Support for multiple hardware accelerators (CPU, NVIDIA GPU, Intel GPU, AMD GPU) Structured detection results with confidence scores Easy integration through the /v1/detection endpoint Usage linkDetection Endpoint linkLocalAI provides a dedicated /v1/detection endpoint for object detection tasks. This endpoint is specifically designed for object detection and returns structured detection results with bounding boxes and confidence scores.\nAPI Reference linkTo perform object detection, send a POST request to the /v1/detection endpoint:\ncurl -X POST http://localhost:8080/v1/detection \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"rfdetr-base\", \"image\": \"https://media.roboflow.com/dog.jpeg\" }' Request Format linkThe request body should contain:\nmodel: The name of the object detection model (e.g., â€œrfdetr-baseâ€) image: The image to analyze, which can be: A URL to an image A base64-encoded image Response Format linkThe API returns a JSON response with detected objects:\n{ \"detections\": [ { \"x\": 100.5, \"y\": 150.2, \"width\": 200.0, \"height\": 300.0, \"confidence\": 0.95, \"class_name\": \"dog\" }, { \"x\": 400.0, \"y\": 200.0, \"width\": 150.0, \"height\": 250.0, \"confidence\": 0.87, \"class_name\": \"person\" } ] } Each detection includes:\nx, y: Coordinates of the bounding box top-left corner width, height: Dimensions of the bounding box confidence: Detection confidence score (0.0 to 1.0) class_name: The detected object class Backends linkRF-DETR Backend linkThe RF-DETR backend is implemented as a Python-based gRPC service that integrates seamlessly with LocalAI. It provides object detection capabilities using the RF-DETR model architecture and supports multiple hardware configurations:\nCPU: Optimized for CPU inference NVIDIA GPU: CUDA acceleration for NVIDIA GPUs Intel GPU: Intel oneAPI optimization AMD GPU: ROCm acceleration for AMD GPUs NVIDIA Jetson: Optimized for ARM64 NVIDIA Jetson devices Setup link Using the Model Gallery (Recommended)\nThe easiest way to get started is using the model gallery. The rfdetr-base model is available in the official LocalAI gallery:\n# Install and run the rfdetr-base model local-ai run rfdetr-base You can also install it through the web interface by navigating to the Models section and searching for â€œrfdetr-baseâ€.\nManual Configuration\nCreate a model configuration file in your models directory:\nname: rfdetr backend: rfdetr parameters: model: rfdetr-base Available Models linkCurrently, the following model is available in the Model Gallery:\nrfdetr-base: Base model with balanced performance and accuracy You can browse and install this model through the LocalAI web interface or using the command line.\nExamples linkBasic Object Detection link # Detect objects in an image from URL curl -X POST http://localhost:8080/v1/detection \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"rfdetr-base\", \"image\": \"https://example.com/image.jpg\" }' Base64 Image Detection link # Convert image to base64 and send base64_image=$(base64 -w 0 image.jpg) curl -X POST http://localhost:8080/v1/detection \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"rfdetr-base\\\", \\\"image\\\": \\\"data:image/jpeg;base64,$base64_image\\\" }\" Troubleshooting linkCommon Issues link Model Loading Errors\nEnsure the model file is properly downloaded Check available disk space Verify model compatibility with your backend version Low Detection Accuracy\nEnsure good image quality and lighting Check if objects are clearly visible Consider using a larger model for better accuracy Slow Performance\nEnable GPU acceleration if available Use a smaller model for faster inference Optimize image resolution Debug Mode linkEnable debug logging for troubleshooting:\nlocal-ai run --debug rfdetr-base Object Detection Category linkLocalAI includes a dedicated object-detection category for models and backends that specialize in identifying and locating objects within images. This category currently includes:\nRF-DETR: Real-time transformer-based object detection Additional object detection models and backends will be added to this category in the future. You can filter models by the object-detection tag in the model gallery to find all available object detection models.\nRelated Features link ğŸ¨ Image generation: Generate images with AI ğŸ“– Text generation: Generate text with language models ğŸ” GPT Vision: Analyze images with language models ğŸš€ GPU acceleration: Optimize performance with GPU acceleration "
            }
        );
    index.add(
            {
                id:  18 ,
                href: "\/features\/embeddings\/",
                title: "ğŸ§  Embeddings",
                description: "LocalAI supports generating embeddings for text or list of tokens.\nFor the API documentation you can refer to the OpenAI docs: https://platform.openai.com/docs/api-reference/embeddings\nModel compatibility linkThe embedding endpoint is compatible with llama.cpp models, bert.cpp models and sentence-transformers models available in huggingface.\n",
                content: "LocalAI supports generating embeddings for text or list of tokens.\nFor the API documentation you can refer to the OpenAI docs: https://platform.openai.com/docs/api-reference/embeddings\nModel compatibility linkThe embedding endpoint is compatible with llama.cpp models, bert.cpp models and sentence-transformers models available in huggingface.\nManual Setup linkCreate a YAML config file in the models directory. Specify the backend and the model file.\nname: text-embedding-ada-002 # The model name used in the API parameters: model: backend: \"\" embeddings: true # .. other parameters Huggingface embeddings linkTo use sentence-transformers and models in huggingface you can use the sentencetransformers embedding backend.\nname: text-embedding-ada-002 backend: sentencetransformers embeddings: true parameters: model: all-MiniLM-L6-v2 The sentencetransformers backend uses Python sentence-transformers. For a list of all pre-trained models available see here: https://github.com/UKPLab/sentence-transformers#pre-trained-models\nnotifications The sentencetransformers backend is an optional backend of LocalAI and uses Python. If you are running LocalAI from the containers you are good to go and should be already configured for use.\nFor local execution, you also have to specify the extra backend in the EXTERNAL_GRPC_BACKENDS environment variable.\nExample: EXTERNAL_GRPC_BACKENDS=\"sentencetransformers:/path/to/LocalAI/backend/python/sentencetransformers/sentencetransformers.py\" The sentencetransformers backend does support only embeddings of text, and not of tokens. If you need to embed tokens you can use the bert backend or llama.cpp.\nNo models are required to be downloaded before using the sentencetransformers backend. The models will be downloaded automatically the first time the API is used.\nLlama.cpp embeddings linkEmbeddings with llama.cpp are supported with the llama-cpp backend, it needs to be enabled with embeddings set to true.\nname: my-awesome-model backend: llama-cpp embeddings: true parameters: model: ggml-file.bin # ... Then you can use the API to generate embeddings:\ncurl http://localhost:8080/embeddings -X POST -H \"Content-Type: application/json\" -d '{ \"input\": \"My text\", \"model\": \"my-awesome-model\" }' | jq \".\" ğŸ’¡ Examples link Example that uses LLamaIndex and LocalAI as embedding: here. "
            }
        );
    index.add(
            {
                id:  19 ,
                href: "\/features\/gpt-vision\/",
                title: "ğŸ¥½ GPT Vision",
                description: "LocalAI supports understanding images by using LLaVA, and implements the GPT Vision API from OpenAI.\n",
                content: "LocalAI supports understanding images by using LLaVA, and implements the GPT Vision API from OpenAI.\nUsage linkOpenAI docs: https://platform.openai.com/docs/guides/vision\nTo let LocalAI understand and reply with what sees in the image, use the /v1/chat/completions endpoint, for example with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"What is in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' Grammars and function tools can be used as well in conjunction with vision APIs:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"Is there some grass in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' Setup linkAll-in-One images have already shipped the llava model as gpt-4-vision-preview, so no setup is needed in this case.\nTo setup the LLaVa models, follow the full example in the configuration examples.\n"
            }
        );
    index.add(
            {
                id:  20 ,
                href: "\/features\/constrained_grammars\/",
                title: "âœï¸ Constrained Grammars",
                description: "Overview linkThe chat endpoint supports the grammar parameter, which allows users to specify a grammar in Backus-Naur Form (BNF). This feature enables the Large Language Model (LLM) to generate outputs adhering to a user-defined schema, such as JSON, YAML, or any other format that can be defined using BNF. For more details about BNF, see Backus-Naur Form on Wikipedia.\n",
                content: "Overview linkThe chat endpoint supports the grammar parameter, which allows users to specify a grammar in Backus-Naur Form (BNF). This feature enables the Large Language Model (LLM) to generate outputs adhering to a user-defined schema, such as JSON, YAML, or any other format that can be defined using BNF. For more details about BNF, see Backus-Naur Form on Wikipedia.\nnotifications Compatibility Notice: This feature is only supported by models that use the llama.cpp backend. For a complete list of compatible models, refer to the "
            }
        );
    index.add(
            {
                id:  21 ,
                href: "\/features\/distribute\/",
                title: "ğŸ†•ğŸ–§ Distributed Inference",
                description: "This functionality enables LocalAI to distribute inference requests across multiple worker nodes, improving efficiency and performance. Nodes are automatically discovered and connect via p2p by using a shared token which makes sure the communication is secure and private between the nodes of the network.\nLocalAI supports two modes of distributed inferencing via p2p:\nFederated Mode: Requests are shared between the cluster and routed to a single worker node in the network based on the load balancerâ€™s decision. Worker Mode (aka â€œmodel shardingâ€ or â€œsplitting weightsâ€): Requests are processed by all the workers which contributes to the final inference result (by sharing the model weights). A list of global instances shared by the community is available at explorer.localai.io.\n",
                content: "This functionality enables LocalAI to distribute inference requests across multiple worker nodes, improving efficiency and performance. Nodes are automatically discovered and connect via p2p by using a shared token which makes sure the communication is secure and private between the nodes of the network.\nLocalAI supports two modes of distributed inferencing via p2p:\nFederated Mode: Requests are shared between the cluster and routed to a single worker node in the network based on the load balancerâ€™s decision. Worker Mode (aka â€œmodel shardingâ€ or â€œsplitting weightsâ€): Requests are processed by all the workers which contributes to the final inference result (by sharing the model weights). A list of global instances shared by the community is available at explorer.localai.io.\nUsage linkStarting LocalAI with --p2p generates a shared token for connecting multiple instances: and thatâ€™s all you need to create AI clusters, eliminating the need for intricate network setups.\nSimply navigate to the â€œSwarmâ€ section in the WebUI and follow the on-screen instructions.\nFor fully shared instances, initiate LocalAI with â€“p2p â€“federated and adhere to the Swarm sectionâ€™s guidance. This feature, while still experimental, offers a tech preview quality experience.\nFederated mode linkFederated mode allows to launch multiple LocalAI instances and connect them together in a federated network. This mode is useful when you want to distribute the load of the inference across multiple nodes, but you want to have a single point of entry for the API. In the Swarm section of the WebUI, you can see the instructions to connect multiple instances together.\nTo start a LocalAI server in federated mode, run:\nlocal-ai run --p2p --federated This will generate a token that you can use to connect other LocalAI instances to the network or others can use to join the network. If you already have a token, you can specify it using the TOKEN environment variable.\nTo start a load balanced server that routes the requests to the network, run with the TOKEN:\nlocal-ai federated To see all the available options, run local-ai federated --help.\nThe instructions are displayed in the â€œSwarmâ€ section of the WebUI, guiding you through the process of connecting multiple instances.\nWorkers mode link notifications This feature is available exclusively with llama-cpp compatible models.\nThis feature was introduced in LocalAI pull request #2324 and is based on the upstream work in llama.cpp pull request #6829.\nTo connect multiple workers to a single LocalAI instance, start first a server in p2p mode:\nlocal-ai run --p2p And navigate the WebUI to the â€œSwarmâ€ section to see the instructions to connect multiple workers to the network.\nWithout P2P linkTo start workers for distributing the computational load, run:\nlocal-ai worker llama-cpp-rpc --llama-cpp-args=\"-H -p -m \" And you can specify the address of the workers when starting LocalAI with the LLAMACPP_GRPC_SERVERS environment variable:\nLLAMACPP_GRPC_SERVERS=\"address1:port,address2:port\" local-ai run The workload on the LocalAI server will then be distributed across the specified nodes.\nAlternatively, you can build the RPC workers/server following the llama.cpp README, which is compatible with LocalAI.\nManual example (worker) linkUse the WebUI to guide you in the process of starting new workers. This example shows the manual steps to highlight the process.\nStart the server with --p2p: ./local-ai run --p2p # Get the token in the Swarm section of the WebUI Copy the token from the WebUI or via API call (e.g., curl http://localhost:8000/p2p/token) and save it for later use.\nTo reuse the same token later, restart the server with --p2ptoken or P2P_TOKEN.\nStart the workers. Copy the local-ai binary to other hosts and run as many workers as needed using the token: TOKEN=XXX ./local-ai worker p2p-llama-cpp-rpc --llama-cpp-args=\"-m \" # 1:06AM INF loading environment variables from file envFile=.env # 1:06AM INF Setting logging to info # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:288\",\"message\":\"connmanager disabled\\n\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:295\",\"message\":\" go-libp2p resource manager protection enabled\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:409\",\"message\":\"max connections: 100\\n\"} # 1:06AM INF Starting llama-cpp-rpc-server on '127.0.0.1:34371' # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"node/node.go:118\",\"message\":\" Starting EdgeVPN network\"} # create_backend: using CPU backend # Starting RPC server on 127.0.0.1:34371, backend memory: 31913 MB # 2024/05/19 01:06:01 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). # See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details. # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.805+0200\",\"caller\":\"node/node.go:172\",\"message\":\" Node ID: 12D3KooWJ7WQAbCWKfJgjw2oMMGGss9diw3Sov5hVWi8t4DMgx92\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.806+0200\",\"caller\":\"node/node.go:173\",\"message\":\" Node Addresses: [/ip4/127.0.0.1/tcp/44931 /ip4/127.0.0.1/udp/33251/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip4/127.0.0.1/udp/35660/quic-v1 /ip4/192.168.68.110/tcp/44931 /ip4/192.168.68.110/udp/33251/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip4/192.168.68.110/udp/35660/quic-v1 /ip6/::1/tcp/41289 /ip6/::1/udp/33160/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip6/::1/udp/35701/quic-v1]\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.806+0200\",\"caller\":\"discovery/dht.go:104\",\"message\":\" Bootstrapping DHT\"} (Note: You can also supply the token via command-line arguments)\nThe server logs should indicate that new workers are being discovered.\nStart inference as usual on the server initiated in step 1. Environment Variables linkThere are options that can be tweaked or parameters that can be set using environment variables\nEnvironment Variable Description LOCALAI_P2P Set to â€œtrueâ€ to enable p2p LOCALAI_FEDERATED Set to â€œtrueâ€ to enable federated mode FEDERATED_SERVER Set to â€œtrueâ€ to enable federated server LOCALAI_P2P_DISABLE_DHT Set to â€œtrueâ€ to disable DHT and enable p2p layer to be local only (mDNS) LOCALAI_P2P_ENABLE_LIMITS Set to â€œtrueâ€ to enable connection limits and resources management (useful when running with poor connectivity or want to limit resources consumption) LOCALAI_P2P_LISTEN_MADDRS Set to comma separated list of multiaddresses to override default libp2p 0.0.0.0 multiaddresses LOCALAI_P2P_DHT_ANNOUNCE_MADDRS Set to comma separated list of multiaddresses to override announcing of listen multiaddresses (useful when external address:port is remapped) LOCALAI_P2P_BOOTSTRAP_PEERS_MADDRS Set to comma separated list of multiaddresses to specify custom DHT bootstrap nodes LOCALAI_P2P_TOKEN Set the token for the p2p network LOCALAI_P2P_LOGLEVEL Set the loglevel for the LocalAI p2p stack (default: info) LOCALAI_P2P_LIB_LOGLEVEL Set the loglevel for the underlying libp2p stack (default: fatal) Architecture linkLocalAI uses https://github.com/libp2p/go-libp2p under the hood, the same project powering IPFS. Differently from other frameworks, LocalAI uses peer2peer without a single master server, but rather it uses sub/gossip and ledger functionalities to achieve consensus across different peers.\nEdgeVPN is used as a library to establish the network and expose the ledger functionality under a shared token to ease out automatic discovery and have separated, private peer2peer networks.\nThe weights are split proportional to the memory when running into worker mode, when in federation mode each request is split to every node which have to load the model fully.\nDebugging linkTo debug, itâ€™s often useful to run in debug mode, for instance:\nLOCALAI_P2P_LOGLEVEL=debug LOCALAI_P2P_LIB_LOGLEVEL=debug LOCALAI_P2P_ENABLE_LIMITS=true LOCALAI_P2P_DISABLE_DHT=true LOCALAI_P2P_TOKEN=\"\" ./local-ai ... Notes link If running in p2p mode with container images, make sure you start the container with --net host or network_mode: host in the docker-compose file. Only a single model is supported currently. Ensure the server detects new workers before starting inference. Currently, additional workers cannot be added once inference has begun. For more details on the implementation, refer to LocalAI pull request #2343 "
            }
        );
    index.add(
            {
                id:  22 ,
                href: "\/features\/audio-to-text\/",
                title: "ğŸ”ˆ Audio to text",
                description: "Audio to text models are models that can generate text from an audio file.\nThe transcription endpoint allows to convert audio files to text. The endpoint is based on whisper.cpp, a C++ library for audio transcription. The endpoint input supports all the audio formats supported by ffmpeg.\n",
                content: "Audio to text models are models that can generate text from an audio file.\nThe transcription endpoint allows to convert audio files to text. The endpoint is based on whisper.cpp, a C++ library for audio transcription. The endpoint input supports all the audio formats supported by ffmpeg.\nUsage linkOnce LocalAI is started and whisper models are installed, you can use the /v1/audio/transcriptions API endpoint.\nFor instance, with cURL:\ncurl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@\" -F model=\"\" Example linkDownload one of the models from here in the models folder, and create a YAML file for your model:\nname: whisper-1 backend: whisper parameters: model: whisper-en The transcriptions endpoint then can be tested like so:\n## Get an example audio file wget --quiet --show-progress -O gb1.ogg https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg ## Send the example audio file to the transcriptions endpoint curl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@$PWD/gb1.ogg\" -F model=\"whisper-1\" ## Result {\"text\":\"My fellow Americans, this day has brought terrible news and great sadness to our country.At nine o'clock this morning, Mission Control in Houston lost contact with our Space ShuttleColumbia.A short time later, debris was seen falling from the skies above Texas.The Columbia's lost.There are no survivors.One board was a crew of seven.Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark, Captain DavidBrown, Commander William McCool, Dr. Kultna Shavla, and Elon Ramon, a colonel in the IsraeliAir Force.These men and women assumed great risk in the service to all humanity.In an age when spaceflight has come to seem almost routine, it is easy to overlook thedangers of travel by rocket and the difficulties of navigating the fierce outer atmosphere ofthe Earth.These astronauts knew the dangers, and they faced them willingly, knowing they had a highand noble purpose in life.Because of their courage and daring and idealism, we will miss them all the more.All Americans today are thinking as well of the families of these men and women who havebeen given this sudden shock and grief.You're not alone.Our entire nation agrees with you, and those you loved will always have the respect andgratitude of this country.The cause in which they died will continue.Mankind has led into the darkness beyond our world by the inspiration of discovery andthe longing to understand.Our journey into space will go on.In the skies today, we saw destruction and tragedy.As farther than we can see, there is comfort and hope.In the words of the prophet Isaiah, \\\"Lift your eyes and look to the heavens who createdall these, he who brings out the starry hosts one by one and calls them each by name.\\\"Because of his great power and mighty strength, not one of them is missing.The same creator who names the stars also knows the names of the seven souls we mourntoday.The crew of the shuttle Columbia did not return safely to Earth yet we can pray that all aresafely home.May God bless the grieving families and may God continue to bless America.[BLANK_AUDIO]\"} "
            }
        );
    index.add(
            {
                id:  23 ,
                href: "\/features\/openai-functions\/",
                title: "ğŸ”¥ OpenAI functions and tools",
                description: "LocalAI supports running OpenAI functions and tools API with llama.cpp compatible models.\nTo learn more about OpenAI functions, see also the OpenAI API blog post.\n",
                content: "LocalAI supports running OpenAI functions and tools API with llama.cpp compatible models.\nTo learn more about OpenAI functions, see also the OpenAI API blog post.\nLocalAI is also supporting JSON mode out of the box with llama.cpp-compatible models.\nğŸ’¡ Check out also LocalAGI for an example on how to use LocalAI functions.\nSetup linkOpenAI functions are available only with ggml or gguf models compatible with llama.cpp.\nYou donâ€™t need to do anything specific - just use ggml or gguf models.\nUsage example linkYou can configure a model manually with a YAML config file in the models directory, for example:\nname: gpt-3.5-turbo parameters: # Model file name model: ggml-openllama.bin top_p: 80 top_k: 0.9 temperature: 0.1 To use the functions with the OpenAI client in python:\nfrom openai import OpenAI # ... # Send the conversation and available functions to GPT messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Beijing now?\"}] tools = [ { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Return the temperature of the specified region specified by the user\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"User specified region\", }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"temperature unit\" }, }, \"required\": [\"location\"], }, }, } ] client = OpenAI( # This is the default and can be omitted api_key=\"test\", base_url=\"http://localhost:8080/v1/\" ) response =client.chat.completions.create( messages=messages, tools=tools, tool_choice =\"auto\", model=\"gpt-4\", ) #... For example, with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"gpt-4\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Beijing now?\"}], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Return the temperature of the specified region specified by the user\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"User specified region\" }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"temperature unit\" } }, \"required\": [\"location\"] } } } ], \"tool_choice\":\"auto\" }' Return dataï¼š\n{ \"created\": 1724210813, \"object\": \"chat.completion\", \"id\": \"16b57014-477c-4e6b-8d25-aad028a5625e\", \"model\": \"gpt-4\", \"choices\": [ { \"index\": 0, \"finish_reason\": \"tool_calls\", \"message\": { \"role\": \"assistant\", \"content\": \"\", \"tool_calls\": [ { \"index\": 0, \"id\": \"16b57014-477c-4e6b-8d25-aad028a5625e\", \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\":\\\"Beijing\\\",\\\"unit\\\":\\\"celsius\\\"}\" } } ] } } ], \"usage\": { \"prompt_tokens\": 221, \"completion_tokens\": 26, \"total_tokens\": 247 } } Advanced linkUse functions without grammars linkThe functions calls maps automatically to grammars which are currently supported only by llama.cpp, however, it is possible to turn off the use of grammars, and extract tool arguments from the LLM responses, by specifying in the YAML file no_grammar and a regex to map the response from the LLM:\nname: model_name parameters: # Model file name model: model/name function: # set to true to not use grammars no_grammar: true # set one or more regexes used to extract the function tool arguments from the LLM response response_regex: - \"(?P\\w+)\\s*\\((?P.*)\\)\" The response regex have to be a regex with named parameters to allow to scan the function name and the arguments. For instance, consider:\n(?P\\w+)\\s*\\((?P.*)\\) will catch\nfunction_name({ \"foo\": \"bar\"}) Parallel tools calls linkThis feature is experimental and has to be configured in the YAML of the model by enabling function.parallel_calls:\nname: gpt-3.5-turbo parameters: # Model file name model: ggml-openllama.bin top_p: 80 top_k: 0.9 temperature: 0.1 function: # set to true to allow the model to call multiple functions in parallel parallel_calls: true Use functions with grammar linkIt is possible to also specify the full function signature (for debugging, or to use with other clients).\nThe chat endpoint accepts the grammar_json_functions additional parameter which takes a JSON schema object.\nFor example, with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"gpt-4\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}], \"temperature\": 0.1, \"grammar_json_functions\": { \"oneOf\": [ { \"type\": \"object\", \"properties\": { \"function\": {\"const\": \"create_event\"}, \"arguments\": { \"type\": \"object\", \"properties\": { \"title\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"}, \"time\": {\"type\": \"string\"} } } } }, { \"type\": \"object\", \"properties\": { \"function\": {\"const\": \"search\"}, \"arguments\": { \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\"} } } } } ] } }' Grammars and function tools can be used as well in conjunction with vision APIs:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"Is there some grass in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' ğŸ’¡ Examples linkA full e2e example with docker-compose is available here.\n"
            }
        );
    index.add(
            {
                id:  24 ,
                href: "\/stores\/",
                title: "ğŸ’¾ Stores",
                description: "Stores are an experimental feature to help with querying data using similarity search. It is a low level API that consists of only get, set, delete and find.\nFor example if you have an embedding of some text and want to find text with similar embeddings. You can create embeddings for chunks of all your text then compare them against the embedding of the text you are searching on.\nAn embedding here meaning a vector of numbers that represent some information about the text. The embeddings are created from an A.I. model such as BERT or a more traditional method such as word frequency.\n",
                content: "Stores are an experimental feature to help with querying data using similarity search. It is a low level API that consists of only get, set, delete and find.\nFor example if you have an embedding of some text and want to find text with similar embeddings. You can create embeddings for chunks of all your text then compare them against the embedding of the text you are searching on.\nAn embedding here meaning a vector of numbers that represent some information about the text. The embeddings are created from an A.I. model such as BERT or a more traditional method such as word frequency.\nPreviously you would have to integrate with an external vector database or library directly. With the stores feature you can now do it through the LocalAI API.\nNote however that doing a similarity search on embeddings is just one way to do retrieval. A higher level API can take this into account, so this may not be the best place to start.\nAPI overview linkThere is an internal gRPC API and an external facing HTTP JSON API. Weâ€™ll just discuss the external HTTP API, however the HTTP API mirrors the gRPC API. Consult pkg/store/client for internal usage.\nEverything is in columnar format meaning that instead of getting an array of objects with a key and a value each. You instead get two separate arrays of keys and values.\nKeys are arrays of floating point numbers with a maximum width of 32bits. Values are strings (in gRPC they are bytes).\nThe key vectors must all be the same length and itâ€™s best for search performance if they are normalized. When addings keys it will be detected if they are not normalized and what length they are.\nAll endpoints accept a store field which specifies which store to operate on. Presently they are created on the fly and there is only one store backend so no configuration is required.\nSet linkTo set some keys you can do\ncurl -X POST http://localhost:8080/stores/set \\ -H \"Content-Type: application/json\" \\ -d '{\"keys\": [[0.1, 0.2], [0.3, 0.4]], \"values\": [\"foo\", \"bar\"]}' Setting the same keys again will update their values.\nOn success 200 OK is returned with no body.\nGet linkTo get some keys you can do\ncurl -X POST http://localhost:8080/stores/get \\ -H \"Content-Type: application/json\" \\ -d '{\"keys\": [[0.1, 0.2]]}' Both the keys and values are returned, e.g: {\"keys\":[[0.1,0.2]],\"values\":[\"foo\"]}\nThe order of the keys is not preserved! If a key does not exist then nothing is returned.\nDelete linkTo delete keys and values you can do\ncurl -X POST http://localhost:8080/stores/delete \\ -H \"Content-Type: application/json\" \\ -d '{\"keys\": [[0.1, 0.2]]}' If a key doesnâ€™t exist then it is ignored.\nOn success 200 OK is returned with no body.\nFind linkTo do a similarity search you can do\ncurl -X POST http://localhost:8080/stores/find -H \"Content-Type: application/json\" \\ -d '{\"topk\": 2, \"key\": [0.2, 0.1]}' topk limits the number of results returned. The result value is the same as get, except that it also includes an array of similarities. Where 1.0 is the maximum similarity. They are returned in the order of most similar to least.\n"
            }
        );
    index.add(
            {
                id:  25 ,
                href: "\/models\/",
                title: "ğŸ–¼ï¸ Model gallery",
                description: "The model gallery is a curated collection of models configurations for LocalAI that enables one-click install of models directly from the LocalAI Web interface.\nA list of the models available can also be browsed at the Public LocalAI Gallery.\n",
                content: "The model gallery is a curated collection of models configurations for LocalAI that enables one-click install of models directly from the LocalAI Web interface.\nA list of the models available can also be browsed at the Public LocalAI Gallery.\nLocalAI to ease out installations of models provide a way to preload models on start and downloading and installing them in runtime. You can install models manually by copying them over the models directory, or use the API or the Web interface to configure, download and verify the model assets for you.\nnotifications The models in this gallery are not directly maintained by LocalAI. If you find a model that is not working, please open an issue on the model gallery repository.\nnotifications GPT and text generation models might have a license which is not permissive for commercial use or might be questionable or without any license at all. Please check the model license before using it. The official gallery contains only open licensed models.\nUseful Links and resources link Open LLM Leaderboard - here you can find a list of the most performing models on the Open LLM benchmark. Keep in mind models compatible with LocalAI must be quantized in the gguf format. How it works linkNavigate the WebUI interface in the â€œModelsâ€ section from the navbar at the top. Here you can find a list of models that can be installed, and you can install them by clicking the â€œInstallâ€ button.\nAdd other galleries linkYou can add other galleries by setting the GALLERIES environment variable. The GALLERIES environment variable is a list of JSON objects, where each object has a name and a url field. The name field is the name of the gallery, and the url field is the URL of the galleryâ€™s index file, for example:\nGALLERIES=[{\"name\":\"\", \"url\":\""
            }
        );
    index.add(
            {
                id:  26 ,
                href: "\/docs\/integrations\/",
                title: "Integrations",
                description: "Community integrations linkList of projects that are using directly LocalAI behind the scenes can be found here.\nThe list below is a list of software that integrates with LocalAI.\n",
                content: "Community integrations linkList of projects that are using directly LocalAI behind the scenes can be found here.\nThe list below is a list of software that integrates with LocalAI.\nAnythingLLM Logseq GPT3 OpenAI plugin allows to set a base URL, and works with LocalAI. https://plugins.jetbrains.com/plugin/21056-codegpt allows for custom OpenAI compatible endpoints since 2.4.0 Wave Terminal has native support for LocalAI! https://github.com/longy2k/obsidian-bmo-chatbot https://github.com/FlowiseAI/Flowise https://github.com/k8sgpt-ai/k8sgpt https://github.com/kairos-io/kairos https://github.com/langchain4j/langchain4j https://github.com/henomis/lingoose https://github.com/trypromptly/LLMStack https://github.com/mattermost/openops https://github.com/charmbracelet/mods https://github.com/cedriking/spark Big AGI is a powerful web interface entirely running in the browser, supporting LocalAI Midori AI Subsystem Manager is a powerful docker subsystem for running all types of AI programs LLPhant is a PHP library for interacting with LLMs and Vector Databases GPTLocalhost (Word Add-in) - run LocalAI in Microsoft Word locally use LocalAI from Nextcloud with the integration plugin and AI assistant Feel free to open up a Pull request (by clicking at the â€œEdit pageâ€ below) to get a page for your project made or if you see a error on one of the pages!\n"
            }
        );
    index.add(
            {
                id:  27 ,
                href: "\/docs\/advanced\/",
                title: "Advanced",
                description: "Advanced usage",
                content: ""
            }
        );
    index.add(
            {
                id:  28 ,
                href: "\/advanced\/",
                title: "Advanced usage",
                description: "Advanced configuration with YAML files linkIn order to define default prompts, model parameters (such as custom default top_p or top_k), LocalAI can be configured to serve user-defined models with a set of default parameters and templates.\nIn order to configure a model, you can create multiple yaml files in the models path or either specify a single YAML configuration file. Consider the following models folder in the example/chatbot-ui:\nbase â¯ ls -liah examples/chatbot-ui/models 36487587 drwxr-xr-x 2 mudler mudler 4.0K May 3 12:27 . 36487586 drwxr-xr-x 3 mudler mudler 4.0K May 3 10:42 .. 36465214 -rw-r--r-- 1 mudler mudler 10 Apr 27 07:46 completion.tmpl 36464855 -rw-r--r-- 1 mudler mudler ?G Apr 27 00:08 luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin 36464537 -rw-r--r-- 1 mudler mudler 245 May 3 10:42 gpt-3.5-turbo.yaml 36467388 -rw-r--r-- 1 mudler mudler 180 Apr 27 07:46 chat.tmpl In the gpt-3.5-turbo.yaml file it is defined the gpt-3.5-turbo model which is an alias to use luna-ai-llama2 with pre-defined options.\n",
                content: "Advanced configuration with YAML files linkIn order to define default prompts, model parameters (such as custom default top_p or top_k), LocalAI can be configured to serve user-defined models with a set of default parameters and templates.\nIn order to configure a model, you can create multiple yaml files in the models path or either specify a single YAML configuration file. Consider the following models folder in the example/chatbot-ui:\nbase â¯ ls -liah examples/chatbot-ui/models 36487587 drwxr-xr-x 2 mudler mudler 4.0K May 3 12:27 . 36487586 drwxr-xr-x 3 mudler mudler 4.0K May 3 10:42 .. 36465214 -rw-r--r-- 1 mudler mudler 10 Apr 27 07:46 completion.tmpl 36464855 -rw-r--r-- 1 mudler mudler ?G Apr 27 00:08 luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin 36464537 -rw-r--r-- 1 mudler mudler 245 May 3 10:42 gpt-3.5-turbo.yaml 36467388 -rw-r--r-- 1 mudler mudler 180 Apr 27 07:46 chat.tmpl In the gpt-3.5-turbo.yaml file it is defined the gpt-3.5-turbo model which is an alias to use luna-ai-llama2 with pre-defined options.\nFor instance, consider the following that declares gpt-3.5-turbo backed by the luna-ai-llama2 model:\nname: gpt-3.5-turbo # Default model parameters parameters: # Relative to the models path model: luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin # temperature temperature: 0.3 # all the OpenAI request options here.. # Default context size context_size: 512 threads: 10 # Define a backend (optional). By default it will try to guess the backend the first time the model is interacted with. backend: llama-stable # available: llama, stablelm, gpt2, gptj rwkv # Enable prompt caching prompt_cache_path: \"alpaca-cache\" prompt_cache_all: true # stopwords (if supported by the backend) stopwords: - \"HUMAN:\" - \"### Response:\" # define chat roles roles: assistant: '### Response:' system: '### System Instruction:' user: '### Instruction:' template: # template file \".tmpl\" with the prompt template to use by default on the endpoint call. Note there is no extension in the files completion: completion chat: chat Specifying a config-file via CLI allows to declare models in a single file as a list, for instance:\n- name: list1 parameters: model: testmodel context_size: 512 threads: 10 stopwords: - \"HUMAN:\" - \"### Response:\" roles: user: \"HUMAN:\" system: \"GPT:\" template: completion: completion chat: chat - name: list2 parameters: model: testmodel context_size: 512 threads: 10 stopwords: - \"HUMAN:\" - \"### Response:\" roles: user: \"HUMAN:\" system: \"GPT:\" template: completion: completion chat: chat See also chatbot-ui as an example on how to use config files.\nIt is possible to specify a full URL or a short-hand URL to a YAML model configuration file and use it on start with local-ai, for example to use phi-2:\nlocal-ai github://mudler/LocalAI/examples/configurations/phi-2.yaml@master Full config model file reference link # Main configuration of the model, template, and system features. name: \"\" # Model name, used to identify the model in API calls. # Precision settings for the model, reducing precision can enhance performance on some hardware. f16: null # Whether to use 16-bit floating-point precision. embeddings: true # Enable embeddings for the model. # Concurrency settings for the application. threads: null # Number of threads to use for processing. # Roles define how different entities interact in a conversational model. # It can be used to map roles to specific parts of the conversation. roles: {} # Roles for entities like user, system, assistant, etc. # Backend to use for computation (like llama-cpp, diffusers, whisper). backend: \"\" # Backend for AI computations. # Templates for various types of model interactions. template: chat: \"\" # Template for chat interactions. Uses golang templates with Sprig functions. chat_message: \"\" # Template for individual chat messages. Uses golang templates with Sprig functions. completion: \"\" # Template for generating text completions. Uses golang templates with Sprig functions. edit: \"\" # Template for edit operations. Uses golang templates with Sprig functions. function: \"\" # Template for function calls. Uses golang templates with Sprig functions. use_tokenizer_template: false # Whether to use a specific tokenizer template. (vLLM) join_chat_messages_by_character: null # Character to join chat messages, if applicable. Defaults to newline. # Function-related settings to control behavior of specific function calls. function: disable_no_action: false # Whether to disable the no-action behavior. grammar: parallel_calls: false # Allow to return parallel tools disable_parallel_new_lines: false # Disable parallel processing for new lines in grammar checks. mixed_mode: false # Allow mixed-mode grammar enforcing no_mixed_free_string: false # Disallow free strings in mixed mode. disable: false # Completely disable grammar enforcing functionality. prefix: \"\" # Prefix to add before grammars rules. expect_strings_after_json: false # Expect string after JSON data. no_action_function_name: \"\" # Function name to call when no action is determined. no_action_description_name: \"\" # Description name for no-action functions. response_regex: [] # Regular expressions to match response from argument_regex: [] # Named regular to extract function arguments from the response. argument_regex_key_name: \"key\" # Name of the named regex capture to capture the key of the function arguments argument_regex_value_name: \"value\" # Name of the named regex capture to capture the value of the function arguments json_regex_match: [] # Regular expressions to match JSON data when in tool mode replace_function_results: [] # Placeholder to replace function call results with arbitrary strings or patterns. replace_llm_results: [] # Replace language model results with arbitrary strings or patterns. capture_llm_results: [] # Capture language model results as text result, among JSON, in function calls. For instance, if a model returns a block for \"thinking\" and a block for \"response\", this will allow you to capture the thinking block. function_name_key: \"name\" function_arguments_key: \"arguments\" # Feature gating flags to enable experimental or optional features. feature_flags: {} # System prompt to use by default. system_prompt: \"\" # Configuration for splitting tensors across GPUs. tensor_split: \"\" # Identifier for the main GPU used in multi-GPU setups. main_gpu: \"\" # Small value added to the denominator in RMS normalization to prevent division by zero. rms_norm_eps: 0 # Natural question generation model parameter. ngqa: 0 # Path where prompt cache is stored. prompt_cache_path: \"\" # Whether to cache all prompts. prompt_cache_all: false # Whether the prompt cache is read-only. prompt_cache_ro: false # Mirostat sampling settings. mirostat_eta: null mirostat_tau: null mirostat: null # GPU-specific layers configuration. gpu_layers: null # Memory mapping for efficient I/O operations. mmap: null # Memory locking to ensure data remains in RAM. mmlock: null # Mode to use minimal VRAM for GPU operations. low_vram: null # Words or phrases that halts processing. stopwords: [] # Strings to cut from responses to maintain context or relevance. cutstrings: [] # Strings to trim from responses for cleaner outputs. trimspace: [] trimsuffix: [] # Default context size for the model's understanding of the conversation or text. context_size: null # Non-uniform memory access settings, useful for systems with multiple CPUs. numa: false # Configuration for LoRA lora_adapter: \"\" lora_base: \"\" lora_scale: 0 # Disable matrix multiplication queuing in GPU operations. no_mulmatq: false # Model for generating draft responses. draft_model: \"\" n_draft: 0 # Quantization settings for the model, impacting memory and processing speed. quantization: \"\" # Utilization percentage of GPU memory to allocate for the model. (vLLM) gpu_memory_utilization: 0 # Whether to trust and execute remote code. trust_remote_code: false # Force eager execution of TensorFlow operations if applicable. (vLLM) enforce_eager: false # Space allocated for swapping data in and out of memory. (vLLM) swap_space: 0 # Maximum model length, possibly referring to the number of tokens or parameters. (vLLM) max_model_len: 0 # Size of the tensor parallelism in distributed computing environments. (vLLM) tensor_parallel_size: 0 # vision model to use for multimodal mmproj: \"\" # Disables offloading of key/value pairs in transformer models to save memory. no_kv_offloading: false # Scaling factor for the rope penalty. rope_scaling: \"\" # Type of configuration, often related to the type of task or model architecture. type: \"\" # YARN settings yarn_ext_factor: 0 yarn_attn_factor: 0 yarn_beta_fast: 0 yarn_beta_slow: 0 # configuration for diffusers model diffusers: cuda: false # Whether to use CUDA pipeline_type: \"\" # Type of pipeline to use. scheduler_type: \"\" # Type of scheduler for controlling operations. enable_parameters: \"\" # Parameters to enable in the diffuser. cfg_scale: 0 # Scale for CFG in the diffuser setup. img2img: false # Whether image-to-image transformation is supported. clip_skip: 0 # Number of steps to skip in CLIP operations. clip_model: \"\" # Model to use for CLIP operations. clip_subfolder: \"\" # Subfolder for storing CLIP-related data. control_net: \"\" # Control net to use # Step count, usually for image processing models step: 0 # Configuration for gRPC communication. grpc: attempts: 0 # Number of retry attempts for gRPC calls. attempts_sleep_time: 0 # Sleep time between retries. # Text-to-Speech (TTS) configuration. tts: voice: \"\" # Voice setting for TTS. vall-e: audio_path: \"\" # Path to audio files for Vall-E. # Whether to use CUDA for GPU-based operations. cuda: false # List of files to download as part of the setup or operations. download_files: [] Prompt templates linkThe API doesnâ€™t inject a default prompt for talking to the model. You have to use a prompt similar to whatâ€™s described in the standford-alpaca docs: https://github.com/tatsu-lab/stanford_alpaca#data-release.\nYou can use a default template for every model present in your model path, by creating a corresponding file with the `.tmpl` suffix next to your model. For instance, if the model is called `foo.bin`, you can create a sibling file, `foo.bin.tmpl` which will be used as a default prompt and can be used with alpaca: The below instruction describes a task. Write a response that appropriately completes the request. ### Instruction: {{.Input}} ### Response: See the prompt-templates directory in this repository for templates for some of the most popular models.\nFor the edit endpoint, an example template for alpaca-based models can be:\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: {{.Instruction}} ### Input: {{.Input}} ### Response: Install models using the API linkInstead of installing models manually, you can use the LocalAI API endpoints and a model definition to install programmatically via API models in runtime.\nA curated collection of model files is in the model-gallery. The files of the model gallery are different from the model files used to configure LocalAI models. The model gallery files contains information about the model setup, and the files necessary to run the model locally.\nTo install for example lunademo, you can send a POST call to the /models/apply endpoint with the model definition url (url) and the name of the model should have in LocalAI (name, optional):\ncurl --location 'http://localhost:8080/models/apply' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"id\": \"TheBloke/Luna-AI-Llama2-Uncensored-GGML/luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin\", \"name\": \"lunademo\" }' Preloading models during startup linkIn order to allow the API to start-up with all the needed model on the first-start, the model gallery files can be used during startup.\nPRELOAD_MODELS='[{\"url\": \"https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml\",\"name\": \"gpt4all-j\"}]' local-ai PRELOAD_MODELS (or --preload-models) takes a list in JSON with the same parameter of the API calls of the /models/apply endpoint.\nSimilarly it can be specified a path to a YAML configuration file containing a list of models with PRELOAD_MODELS_CONFIG ( or --preload-models-config ):\n- url: https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml name: gpt4all-j # ... Automatic prompt caching linkLocalAI can automatically cache prompts for faster loading of the prompt. This can be useful if your model need a prompt template with prefixed text in the prompt before the input.\nTo enable prompt caching, you can control the settings in the model config YAML file:\n# Enable prompt caching prompt_cache_path: \"cache\" prompt_cache_all: true prompt_cache_path is relative to the models folder. you can enter here a name for the file that will be automatically create during the first load if prompt_cache_all is set to true.\nConfiguring a specific backend for the model linkBy default LocalAI will try to autoload the model by trying all the backends. This might work for most of models, but some of the backends are NOT configured to autoload.\nThe available backends are listed in the model compatibility table.\nIn order to specify a backend for your models, create a model config file in your models directory specifying the backend:\nname: gpt-3.5-turbo # Default model parameters parameters: # Relative to the models path model: ... backend: llama-stable # ... Connect external backends linkLocalAI backends are internally implemented using gRPC services. This also allows LocalAI to connect to external gRPC services on start and extend LocalAI functionalities via third-party binaries.\nThe --external-grpc-backends parameter in the CLI can be used either to specify a local backend (a file) or a remote URL. The syntax is :. Once LocalAI is started with it, the new backend name will be available for all the API endpoints.\nSo for instance, to register a new backend which is a local file:\n./local-ai --debug --external-grpc-backends \"my-awesome-backend:/path/to/my/backend.py\" Or a remote URI:\n./local-ai --debug --external-grpc-backends \"my-awesome-backend:host:port\" For example, to start vllm manually after compiling LocalAI (also assuming running the command from the root of the repository):\n./local-ai --external-grpc-backends \"vllm:$PWD/backend/python/vllm/run.sh\" Note that first is is necessary to create the environment with:\nmake -C backend/python/vllm Environment variables linkWhen LocalAI runs in a container, there are additional environment variables available that modify the behavior of LocalAI on startup:\nEnvironment variable Default Description REBUILD false Rebuild LocalAI on startup BUILD_TYPE Build type. Available: cublas, openblas, clblas, intel (intel core), sycl_f16, sycl_f32 (intel backends) GO_TAGS Go tags. Available: stablediffusion HUGGINGFACEHUB_API_TOKEN Special token for interacting with HuggingFace Inference API, required only when using the langchain-huggingface backend EXTRA_BACKENDS A space separated list of backends to prepare. For example EXTRA_BACKENDS=\"backend/python/diffusers backend/python/transformers\" prepares the python environment on start DISABLE_AUTODETECT false Disable autodetect of CPU flagset on start LLAMACPP_GRPC_SERVERS A list of llama.cpp workers to distribute the workload. For example LLAMACPP_GRPC_SERVERS=\"address1:port,address2:port\" Here is how to configure these variables:\n# Option 1: command line docker run --env REBUILD=true localai # Option 2: set within an env file docker run --env-file .env localai CLI parameters linkYou can control LocalAI with command line arguments, to specify a binding address, or the number of threads. Any command line parameter can be specified via an environment variable.\nIn the help text below, BASEPATH is the location that local-ai is being executed from\nGlobal Flags link Parameter Default Description Environment Variable -h, â€“help Show context-sensitive help. â€“log-level info Set the level of logs to output [error,warn,info,debug] $LOCALAI_LOG_LEVEL Storage Flags link Parameter Default Description Environment Variable â€“models-path BASEPATH/models Path containing models used for inferencing $LOCALAI_MODELS_PATH â€“backend-assets-path /tmp/localai/backend_data Path used to extract libraries that are required by some of the backends in runtime $LOCALAI_BACKEND_ASSETS_PATH â€“generated-content-path /tmp/generated/content Location for assets generated by backends (e.g. stablediffusion) $LOCALAI_GENERATED_CONTENT_PATH â€“upload-path /tmp/localai/upload Path to store uploads from files api $LOCALAI_UPLOAD_PATH â€“config-path /tmp/localai/config $LOCALAI_CONFIG_PATH â€“localai-config-dir BASEPATH/configuration Directory for dynamic loading of certain configuration files (currently api_keys.json and external_backends.json) $LOCALAI_CONFIG_DIR â€“localai-config-dir-poll-interval Typically the config path picks up changes automatically, but if your system has broken fsnotify events, set this to a time duration to poll the LocalAI Config Dir (example: 1m) $LOCALAI_CONFIG_DIR_POLL_INTERVAL â€“models-config-file STRING YAML file containing a list of model backend configs $LOCALAI_MODELS_CONFIG_FILE Models Flags link Parameter Default Description Environment Variable â€“galleries STRING JSON list of galleries $LOCALAI_GALLERIES â€“autoload-galleries $LOCALAI_AUTOLOAD_GALLERIES â€“remote-library â€œhttps://raw.githubusercontent.com/mudler/LocalAI/master/embedded/model_library.yaml\" A LocalAI remote library URL $LOCALAI_REMOTE_LIBRARY â€“preload-models STRING A List of models to apply in JSON at start $LOCALAI_PRELOAD_MODELS â€“models MODELS,â€¦ A List of model configuration URLs to load $LOCALAI_MODELS â€“preload-models-config STRING A List of models to apply at startup. Path to a YAML config file $LOCALAI_PRELOAD_MODELS_CONFIG Performance Flags link Parameter Default Description Environment Variable â€“f16 Enable GPU acceleration $LOCALAI_F16 -t, â€“threads 4 Number of threads used for parallel computation. Usage of the number of physical cores in the system is suggested $LOCALAI_THREADS â€“context-size 512 Default context size for models $LOCALAI_CONTEXT_SIZE API Flags link Parameter Default Description Environment Variable â€“address â€œ:8080â€ Bind address for the API server $LOCALAI_ADDRESS â€“cors $LOCALAI_CORS â€“cors-allow-origins $LOCALAI_CORS_ALLOW_ORIGINS â€“upload-limit 15 Default upload-limit in MB $LOCALAI_UPLOAD_LIMIT â€“api-keys API-KEYS,â€¦ List of API Keys to enable API authentication. When this is set, all the requests must be authenticated with one of these API keys $LOCALAI_API_KEY â€“disable-welcome Disable welcome pages $LOCALAI_DISABLE_WELCOME â€“disable-webui false Disables the web user interface. When set to true, the server will only expose API endpoints without serving the web interface $LOCALAI_DISABLE_WEBUI â€“machine-tag If not empty - put that string to Machine-Tag header in each response. Useful to track response from different machines using multiple P2P federated nodes $LOCALAI_MACHINE_TAG Backend Flags link Parameter Default Description Environment Variable â€“parallel-requests Enable backends to handle multiple requests in parallel if they support it (e.g.: llama.cpp or vllm) $LOCALAI_PARALLEL_REQUESTS â€“single-active-backend Allow only one backend to be run at a time $LOCALAI_SINGLE_ACTIVE_BACKEND â€“preload-backend-only Do not launch the API services, only the preloaded models / backends are started (useful for multi-node setups) $LOCALAI_PRELOAD_BACKEND_ONLY â€“external-grpc-backends EXTERNAL-GRPC-BACKENDS,â€¦ A list of external grpc backends $LOCALAI_EXTERNAL_GRPC_BACKENDS â€“enable-watchdog-idle Enable watchdog for stopping backends that are idle longer than the watchdog-idle-timeout $LOCALAI_WATCHDOG_IDLE â€“watchdog-idle-timeout 15m Threshold beyond which an idle backend should be stopped $LOCALAI_WATCHDOG_IDLE_TIMEOUT, $WATCHDOG_IDLE_TIMEOUT â€“enable-watchdog-busy Enable watchdog for stopping backends that are busy longer than the watchdog-busy-timeout $LOCALAI_WATCHDOG_BUSY â€“watchdog-busy-timeout 5m Threshold beyond which a busy backend should be stopped $LOCALAI_WATCHDOG_BUSY_TIMEOUT .env files linkAny settings being provided by an Environment Variable can also be provided from within .env files. There are several locations that will be checked for relevant .env files. In order of precedence they are:\n.env within the current directory localai.env within the current directory localai.env within the home directory .config/localai.env within the home directory /etc/localai.env Environment variables within files earlier in the list will take precedence over environment variables defined in files later in the list.\nAn example .env file is:\nLOCALAI_THREADS=10 LOCALAI_MODELS_PATH=/mnt/storage/localai/models LOCALAI_F16=true Request headers linkYou can use â€˜Extra-Usageâ€™ request header key presence (â€˜Extra-Usage: trueâ€™) to receive inference timings in milliseconds extending default OpenAI response model in the usage field:\n... { \"id\": \"...\", \"created\": ..., \"model\": \"...\", \"choices\": [ { ... }, ... ], \"object\": \"...\", \"usage\": { \"prompt_tokens\": ..., \"completion_tokens\": ..., \"total_tokens\": ..., // Extra-Usage header key will include these two float fields: \"timing_prompt_processing: ..., \"timing_token_generation\": ..., }, } ... Extra backends linkLocalAI can be extended with extra backends. The backends are implemented as gRPC services and can be written in any language. See the backend section for more details on how to install and build new backends for LocalAI.\nIn runtime linkWhen using the -core container image it is possible to prepare the python backends you are interested into by using the EXTRA_BACKENDS variable, for instance:\ndocker run --env EXTRA_BACKENDS=\"backend/python/diffusers\" quay.io/go-skynet/local-ai:master Concurrent requests linkLocalAI supports parallel requests for the backends that supports it. For instance, vLLM and llama.cpp supports parallel requests, and thus LocalAI allows to run multiple requests in parallel.\nIn order to enable parallel requests, you have to pass --parallel-requests or set the PARALLEL_REQUEST to true as environment variable.\nA list of the environment variable that tweaks parallelism is the following:\n### Python backends GRPC max workers ### Default number of workers for GRPC Python backends. ### This actually controls wether a backend can process multiple requests or not. # PYTHON_GRPC_MAX_WORKERS=1 ### Define the number of parallel LLAMA.cpp workers (Defaults to 1) # LLAMACPP_PARALLEL=1 ### Enable to run parallel requests # LOCALAI_PARALLEL_REQUESTS=true Note that, for llama.cpp you need to set accordingly LLAMACPP_PARALLEL to the number of parallel processes your GPU/CPU can handle. For python-based backends (like vLLM) you can set PYTHON_GRPC_MAX_WORKERS to the number of parallel requests.\nDisable CPU flagset auto detection in llama.cpp linkLocalAI will automatically discover the CPU flagset available in your host and will use the most optimized version of the backends.\nIf you want to disable this behavior, you can set DISABLE_AUTODETECT to true in the environment variables.\n"
            }
        );
    index.add(
            {
                id:  29 ,
                href: "\/docs\/advanced\/fine-tuning\/",
                title: "Fine-tuning LLMs for text generation",
                description: " notifications Section under construction\nThis section covers how to fine-tune a language model for text generation and consume it in LocalAI.\nOpen in ColabOpen in Colab ",
                content: " notifications Section under construction\nThis section covers how to fine-tune a language model for text generation and consume it in LocalAI.\nOpen in ColabOpen in Colab Requirements linkFor this example you will need at least a 12GB VRAM of GPU and a Linux box.\nFine-tuning linkFine-tuning a language model is a process that requires a lot of computational power and time.\nCurrently LocalAI doesnâ€™t support the fine-tuning endpoint as LocalAI but there are are plans to support that. For the time being a guide is proposed here to give a simple starting point on how to fine-tune a model and use it with LocalAI (but also with llama.cpp).\nThere is an e2e example of fine-tuning a LLM model to use with LocalAI written by @mudler available here.\nThe steps involved are:\nPreparing a dataset Prepare the environment and install dependencies Fine-tune the model Merge the Lora base with the model Convert the model to gguf Use the model with LocalAI Dataset preparation linkWe are going to need a dataset or a set of datasets.\nAxolotl supports a variety of formats, in the notebook and in this example we are aiming for a very simple dataset and build that manually, so we are going to use the completion format which requires the full text to be used for fine-tuning.\nA dataset for an instructor model (like Alpaca) can look like the following:\n[ { \"text\": \"As an AI language model you are trained to reply to an instruction. Try to be as much polite as possible\\n\\n## Instruction\\n\\nWrite a poem about a tree.\\n\\n## Response\\n\\nTrees are beautiful, ...\", }, { \"text\": \"As an AI language model you are trained to reply to an instruction. Try to be as much polite as possible\\n\\n## Instruction\\n\\nWrite a poem about a tree.\\n\\n## Response\\n\\nTrees are beautiful, ...\", } ] Every block in the text is the whole text that is used to fine-tune. For example, for an instructor model it follows the following format (more or less):\n## Instruction ## Response The instruction format works such as when we are going to inference with the model, we are going to feed it only the first part up to the ## Instruction block, and the model is going to complete the text with the ## Response block.\nPrepare a dataset, and upload it to your Google Drive in case you are using the Google colab. Otherwise place it next the axolotl.yaml file as dataset.json.\nInstall dependencies link # Install axolotl and dependencies git clone https://github.com/OpenAccess-AI-Collective/axolotl \u0026\u0026 pushd axolotl \u0026\u0026 git checkout 797f3dd1de8fd8c0eafbd1c9fdb172abd9ff840a \u0026\u0026 popd #0.3.0 pip install packaging pushd axolotl \u0026\u0026 pip install -e '.[flash-attn,deepspeed]' \u0026\u0026 popd # https://github.com/oobabooga/text-generation-webui/issues/4238 pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.0/flash_attn-2.3.0+cu117torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl Configure accelerate:\naccelerate config default Fine-tuning linkWe will need to configure axolotl. In this example is provided a file to use axolotl.yaml that uses openllama-3b for fine-tuning. Copy the axolotl.yaml file and edit it to your needs. The dataset needs to be next to it as dataset.json. You can find the axolotl.yaml file here.\nIf you have a big dataset, you can pre-tokenize it to speedup the fine-tuning process:\n# Optional pre-tokenize (run only if big dataset) python -m axolotl.cli.preprocess axolotl.yaml Now we are ready to start the fine-tuning process:\n# Fine-tune accelerate launch -m axolotl.cli.train axolotl.yaml After we have finished the fine-tuning, we merge the Lora base with the model:\n# Merge lora python3 -m axolotl.cli.merge_lora axolotl.yaml --lora_model_dir=\"./qlora-out\" --load_in_8bit=False --load_in_4bit=False And we convert it to the gguf format that LocalAI can consume:\n# Convert to gguf git clone https://github.com/ggerganov/llama.cpp.git pushd llama.cpp \u0026\u0026 cmake -B build -DGGML_CUDA=ON \u0026\u0026 cmake --build build --config Release \u0026\u0026 popd # We need to convert the pytorch model into ggml for quantization # It crates 'ggml-model-f16.bin' in the 'merged' directory. pushd llama.cpp \u0026\u0026 python3 convert_hf_to_gguf.py ../qlora-out/merged \u0026\u0026 popd # Start off by making a basic q4_0 4-bit quantization. # It's important to have 'ggml' in the name of the quant for some # software to recognize it's file format. pushd llama.cpp/build/bin \u0026\u0026 ./llama-quantize ../../../qlora-out/merged/Merged-33B-F16.gguf \\ ../../../custom-model-q4_0.gguf q4_0 Now you should have ended up with a custom-model-q4_0.gguf file that you can copy in the LocalAI models directory and use it with LocalAI.\n"
            }
        );
    index.add(
            {
                id:  30 ,
                href: "\/docs\/reference\/",
                title: "References",
                description: "Reference",
                content: ""
            }
        );
    index.add(
            {
                id:  31 ,
                href: "\/faq\/",
                title: "FAQ",
                description: "Frequently asked questions linkHere are answers to some of the most common questions.\nHow do I get models? linkMost gguf-based models should work, but newer models may require additions to the API. If a model doesnâ€™t work, please feel free to open up issues. However, be cautious about downloading models from the internet and directly onto your machine, as there may be security vulnerabilities in lama.cpp or ggml that could be maliciously exploited. Some models can be found on Hugging Face: https://huggingface.co/models?search=gguf, or models from gpt4all are compatible too: https://github.com/nomic-ai/gpt4all.\n",
                content: "Frequently asked questions linkHere are answers to some of the most common questions.\nHow do I get models? linkMost gguf-based models should work, but newer models may require additions to the API. If a model doesnâ€™t work, please feel free to open up issues. However, be cautious about downloading models from the internet and directly onto your machine, as there may be security vulnerabilities in lama.cpp or ggml that could be maliciously exploited. Some models can be found on Hugging Face: https://huggingface.co/models?search=gguf, or models from gpt4all are compatible too: https://github.com/nomic-ai/gpt4all.\nBenchmarking LocalAI and llama.cpp shows different results! linkLocalAI applies a set of defaults when loading models with the llama.cpp backend, one of these is mirostat sampling - while it achieves better results, it slows down the inference. You can disable this by setting mirostat: 0 in the model config file. See also the advanced section (/advanced/) for more information and this issue.\nWhatâ€™s the difference with Serge, or XXX? linkLocalAI is a multi-model solution that doesnâ€™t focus on a specific model type (e.g., llama.cpp or alpaca.cpp), and it handles all of these internally for faster inference, easy to set up locally and deploy to Kubernetes.\nEverything is slow, how is it possible? linkThere are few situation why this could occur. Some tips are:\nDonâ€™t use HDD to store your models. Prefer SSD over HDD. In case you are stuck with HDD, disable mmap in the model config file so it loads everything in memory. Watch out CPU overbooking. Ideally the --threads should match the number of physical cores. For instance if your CPU has 4 cores, you would ideally allocate \u003c= 4 threads to a model. Run LocalAI with DEBUG=true. This gives more information, including stats on the token inference speed. Check that you are actually getting an output: run a simple curl request with \"stream\": true to see how fast the model is responding. Can I use it with a Discord bot, or XXX? linkYes! If the client uses OpenAI and supports setting a different base URL to send requests to, you can use the LocalAI endpoint. This allows to use this with every application that was supposed to work with OpenAI, but without changing the application!\nCan this leverage GPUs? linkThere is GPU support, see /features/gpu-acceleration/.\nWhere is the webUI? linkThere is the availability of localai-webui and chatbot-ui in the examples section and can be setup as per the instructions. However as LocalAI is an API you can already plug it into existing projects that provides are UI interfaces to OpenAIâ€™s APIs. There are several already on Github, and should be compatible with LocalAI already (as it mimics the OpenAI API)\nDoes it work with AutoGPT? linkYes, see the examples!\nHow can I troubleshoot when something is wrong? linkEnable the debug mode by setting DEBUG=true in the environment variables. This will give you more information on whatâ€™s going on. You can also specify --debug in the command line.\nIâ€™m getting â€˜invalid pitchâ€™ error when running with CUDA, whatâ€™s wrong? linkThis typically happens when your prompt exceeds the context size. Try to reduce the prompt size, or increase the context size.\nIâ€™m getting a â€˜SIGILLâ€™ error, whatâ€™s wrong? linkYour CPU probably does not have support for certain instructions that are compiled by default in the pre-built binaries. If you are running in a container, try setting REBUILD=true and disable the CPU instructions that are not compatible with your CPU. For instance: CMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF\" make build\n"
            }
        );
    index.add(
            {
                id:  32 ,
                href: "\/docs\/advanced\/installer\/",
                title: "Installer options",
                description: "An installation script is available for quick and hassle-free installations, streamlining the setup process for new users.\nCan be used with the following command:\ncurl https://localai.io/install.sh | sh Installation can be configured with Environment variables, for example:\ncurl https://localai.io/install.sh | VAR=value sh List of the Environment Variables:\nEnvironment Variable Description DOCKER_INSTALL Set to â€œtrueâ€ to enable the installation of Docker images. USE_AIO Set to â€œtrueâ€ to use the all-in-one LocalAI Docker image. USE_VULKAN Set to â€œtrueâ€ to use Vulkan GPU support. API_KEY Specify an API key for accessing LocalAI, if required. PORT Specifies the port on which LocalAI will run (default is 8080). THREADS Number of processor threads the application should use. Defaults to the number of logical cores minus one. VERSION Specifies the version of LocalAI to install. Defaults to the latest available version. MODELS_PATH Directory path where LocalAI models are stored (default is /usr/share/local-ai/models). P2P_TOKEN Token to use for the federation or for starting workers see documentation WORKER Set to â€œtrueâ€ to make the instance a worker (p2p token is required see documentation) FEDERATED Set to â€œtrueâ€ to share the instance with the federation (p2p token is required see documentation) FEDERATED_SERVER Set to â€œtrueâ€ to run the instance as a federation server which forwards requests to the federation (p2p token is required see documentation) Image Selection linkThe installer will automatically detect your GPU and select the appropriate image. By default, it uses the standard images without extra Python dependencies. You can customize the image selection using the following environment variables:\n",
                content: "An installation script is available for quick and hassle-free installations, streamlining the setup process for new users.\nCan be used with the following command:\ncurl https://localai.io/install.sh | sh Installation can be configured with Environment variables, for example:\ncurl https://localai.io/install.sh | VAR=value sh List of the Environment Variables:\nEnvironment Variable Description DOCKER_INSTALL Set to â€œtrueâ€ to enable the installation of Docker images. USE_AIO Set to â€œtrueâ€ to use the all-in-one LocalAI Docker image. USE_VULKAN Set to â€œtrueâ€ to use Vulkan GPU support. API_KEY Specify an API key for accessing LocalAI, if required. PORT Specifies the port on which LocalAI will run (default is 8080). THREADS Number of processor threads the application should use. Defaults to the number of logical cores minus one. VERSION Specifies the version of LocalAI to install. Defaults to the latest available version. MODELS_PATH Directory path where LocalAI models are stored (default is /usr/share/local-ai/models). P2P_TOKEN Token to use for the federation or for starting workers see documentation WORKER Set to â€œtrueâ€ to make the instance a worker (p2p token is required see documentation) FEDERATED Set to â€œtrueâ€ to share the instance with the federation (p2p token is required see documentation) FEDERATED_SERVER Set to â€œtrueâ€ to run the instance as a federation server which forwards requests to the federation (p2p token is required see documentation) Image Selection linkThe installer will automatically detect your GPU and select the appropriate image. By default, it uses the standard images without extra Python dependencies. You can customize the image selection using the following environment variables:\nUSE_AIO=true: Use all-in-one images that include all dependencies USE_VULKAN=true: Use Vulkan GPU support instead of vendor-specific GPU support Uninstallation linkTo uninstall, run:\ncurl https://localai.io/install.sh | sh -s -- --uninstall We are looking into improving the installer, and as this is a first iteration any feedback is welcome! Open up an issue if something doesnâ€™t work for you!\n"
            }
        );
    index.add(
            {
                id:  33 ,
                href: "\/model-compatibility\/",
                title: "Model compatibility table",
                description: "Besides llama based models, LocalAI is compatible also with other architectures. The table below lists all the backends, compatible models families and the associated repository.\nnotifications LocalAI will attempt to automatically load models which are not explicitly configured for a specific backend. You can specify the backend to use by configuring a model with a YAML file. See ",
                content: "Besides llama based models, LocalAI is compatible also with other architectures. The table below lists all the backends, compatible models families and the associated repository.\nnotifications LocalAI will attempt to automatically load models which are not explicitly configured for a specific backend. You can specify the backend to use by configuring a model with a YAML file. See "
            }
        );
    index.add(
            {
                id:  34 ,
                href: "\/docs\/reference\/architecture\/",
                title: "Architecture",
                description: "LocalAI is an API written in Go that serves as an OpenAI shim, enabling software already developed with OpenAI SDKs to seamlessly integrate with LocalAI. It can be effortlessly implemented as a substitute, even on consumer-grade hardware. This capability is achieved by employing various C++ backends, including ggml, to perform inference on LLMs using both CPU and, if desired, GPU. Internally LocalAI backends are just gRPC server, indeed you can specify and build your own gRPC server and extend LocalAI in runtime as well. It is possible to specify external gRPC server and/or binaries that LocalAI will manage internally.\n",
                content: "LocalAI is an API written in Go that serves as an OpenAI shim, enabling software already developed with OpenAI SDKs to seamlessly integrate with LocalAI. It can be effortlessly implemented as a substitute, even on consumer-grade hardware. This capability is achieved by employing various C++ backends, including ggml, to perform inference on LLMs using both CPU and, if desired, GPU. Internally LocalAI backends are just gRPC server, indeed you can specify and build your own gRPC server and extend LocalAI in runtime as well. It is possible to specify external gRPC server and/or binaries that LocalAI will manage internally.\nLocalAI uses a mixture of backends written in various languages (C++, Golang, Python, â€¦). You can check the model compatibility table to learn about all the components of LocalAI.\nBackstory linkAs much as typical open source projects starts, I, mudler, was fiddling around with llama.cpp over my long nights and wanted to have a way to call it from go, as I am a Golang developer and use it extensively. So Iâ€™ve created LocalAI (or what was initially known as llama-cli) and added an API to it.\nBut guess what? The more I dived into this rabbit hole, the more I realized that I had stumbled upon something big. With all the fantastic C++ projects floating around the community, it dawned on me that I could piece them together to create a full-fledged OpenAI replacement. So, ta-da! LocalAI was born, and it quickly overshadowed its humble origins.\nNow, why did I choose to go with C++ bindings, you ask? Well, I wanted to keep LocalAI snappy and lightweight, allowing it to run like a champ on any system and avoid any Golang penalties of the GC, and, most importantly built on shoulders of giants like llama.cpp. Go is good at backends and API and is easy to maintain. And hey, donâ€™t forget that Iâ€™m all about sharing the love. Thatâ€™s why I made LocalAI MIT licensed, so everyone can hop on board and benefit from it.\nAs if that wasnâ€™t exciting enough, as the project gained traction, mkellerman and Aisuko jumped in to lend a hand. mkellerman helped set up some killer examples, while Aisuko is becoming our community maestro. The community now is growing even more with new contributors and users, and I couldnâ€™t be happier about it!\nOh, and letâ€™s not forget the real MVP hereâ€”llama.cpp. Without this extraordinary piece of software, LocalAI wouldnâ€™t even exist. So, a big shoutout to the community for making this magic happen!\n"
            }
        );
    index.add(
            {
                id:  35 ,
                href: "\/docs\/reference\/binaries\/",
                title: "LocalAI binaries",
                description: "LocalAI binaries are available for both Linux and MacOS platforms and can be executed directly from your command line. These binaries are continuously updated and hosted on our GitHub Releases page. This method also supports Windows users via the Windows Subsystem for Linux (WSL).\n",
                content: "LocalAI binaries are available for both Linux and MacOS platforms and can be executed directly from your command line. These binaries are continuously updated and hosted on our GitHub Releases page. This method also supports Windows users via the Windows Subsystem for Linux (WSL).\nUse the following one-liner command in your terminal to download and run LocalAI on Linux or MacOS:\ncurl -Lo local-ai \"https://github.com/mudler/LocalAI/releases/download/v3.4.0/local-ai-$(uname -s)-$(uname -m)\" \u0026\u0026 chmod +x local-ai \u0026\u0026 ./local-ai Otherwise, here are the links to the binaries:\nOS Link Linux (amd64) Download Linux (arm64) Download MacOS (arm64) Download âš¡\nBinaries do have limited support compared to container images:\nPython-based backends are not shipped with binaries (e.g. bark, diffusers or transformers)\nMacOS binaries and Linux-arm64 do not ship TTS nor stablediffusion-cpp backends\nLinux binaries do not ship stablediffusion-cpp backend\n"
            }
        );
    index.add(
            {
                id:  36 ,
                href: "\/docs\/reference\/nvidia-l4t\/",
                title: "Running on Nvidia ARM64",
                description: "LocalAI can be run on Nvidia ARM64 devices, such as the Jetson Nano, Jetson Xavier NX, and Jetson AGX Xavier. The following instructions will guide you through building the LocalAI container for Nvidia ARM64 devices.\nPrerequisites link Docker engine installed (https://docs.docker.com/engine/install/ubuntu/) Nvidia container toolkit installed (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-ap) Build the container linkBuild the LocalAI container for Nvidia ARM64 devices using the following command:\n",
                content: "LocalAI can be run on Nvidia ARM64 devices, such as the Jetson Nano, Jetson Xavier NX, and Jetson AGX Xavier. The following instructions will guide you through building the LocalAI container for Nvidia ARM64 devices.\nPrerequisites link Docker engine installed (https://docs.docker.com/engine/install/ubuntu/) Nvidia container toolkit installed (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-ap) Build the container linkBuild the LocalAI container for Nvidia ARM64 devices using the following command:\ngit clone https://github.com/mudler/LocalAI cd LocalAI docker build --build-arg SKIP_DRIVERS=true --build-arg BUILD_TYPE=cublas --build-arg BASE_IMAGE=nvcr.io/nvidia/l4t-jetpack:r36.4.0 --build-arg IMAGE_TYPE=core -t quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core . Otherwise images are available on quay.io and dockerhub:\ndocker pull quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core Usage linkRun the LocalAI container on Nvidia ARM64 devices using the following command, where /data/models is the directory containing the models:\ndocker run -e DEBUG=true -p 8080:8080 -v /data/models:/models -ti --restart=always --name local-ai --runtime nvidia --gpus all quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core Note: /data/models is the directory containing the models. You can replace it with the directory containing your models.\n"
            }
        );
    index.add(
            {
                id:  37 ,
                href: "\/docs\/",
                title: "Docs",
                description: "",
                content: ""
            }
        );
    search.addEventListener('input', show_results, true);

    function show_results(){
        const maxResult =  5 ;
        const minlength =  0 ;
        var searchQuery = sanitizeHTML(this.value);
        var results = index.search(searchQuery, {limit: maxResult, enrich: true});

        
        const flatResults = new Map(); 
        for (const result of results.flatMap(r => r.result)) {
        if (flatResults.has(result.doc.href)) continue;
        flatResults.set(result.doc.href, result.doc);
        }

        suggestions.innerHTML = "";
        suggestions.classList.remove('d-none');

        
        if (searchQuery.length < minlength) {
            const minCharMessage = document.createElement('div')
            minCharMessage.innerHTML = `Please type at least <strong>${minlength}</strong> characters`
            minCharMessage.classList.add("suggestion__no-results");
            suggestions.appendChild(minCharMessage);
            return;
        } else {
            
            if (flatResults.size === 0 && searchQuery) {
                const noResultsMessage = document.createElement('div')
                noResultsMessage.innerHTML = "No results for" + ` "<strong>${searchQuery}</strong>"`
                noResultsMessage.classList.add("suggestion__no-results");
                suggestions.appendChild(noResultsMessage);
                return;
            }
        }

        
        for(const [href, doc] of flatResults) {
            const entry = document.createElement('div');
            suggestions.appendChild(entry);

            const a = document.createElement('a');
            a.href = href;
            entry.appendChild(a);

            const title = document.createElement('span');
            title.textContent = doc.title;
            title.classList.add("suggestion__title");
            a.appendChild(title);

            const description = document.createElement('span');
            description.textContent = doc.description;
            description.classList.add("suggestion__description");
            a.appendChild(description);

            suggestions.appendChild(entry);

            if(suggestions.childElementCount == maxResult) break;
        }
    }
    }());
</script></body></html>