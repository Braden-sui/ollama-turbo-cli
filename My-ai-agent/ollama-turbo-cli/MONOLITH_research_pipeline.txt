================================================================================
Research Pipeline Monolith

Generated: 2025-09-08T17:54:17.193212

Project root: C:\Users\brade\Desktop\ai\My-ai-agent\ollama-turbo-cli

================================================================================


================================================================================
FILE: src/web/pipeline.py
================================================================================

from __future__ import annotations
import os
import json
import hashlib
import time
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from .robots import RobotsPolicy
from .config import WebConfig
# Ensure .env.local/.env are loaded even when this module is imported outside the CLI
# (e.g., tests or custom orchestrators). This mirrors src/config.py with a safe fallback.
try:
    from dotenv import load_dotenv, find_dotenv  # type: ignore
    try:
        path_local = find_dotenv('.env.local', usecwd=True)
        if path_local:
            load_dotenv(path_local, override=True)
    except Exception:
        pass
    try:
        path_default = find_dotenv('.env', usecwd=True)
        if path_default:
            load_dotenv(path_default, override=False)
    except Exception:
        pass
except Exception:
    # Fallback: minimal manual loader for .env.local and .env
    import os as _os
    def _find_upwards(_filename: str) -> str:
        try:
            _cwd = _os.getcwd()
        except Exception:
            return ""
        _cur = _cwd
        while True:
            _cand = _os.path.join(_cur, _filename)
            if _os.path.isfile(_cand):
                return _cand
            _parent = _os.path.dirname(_cur)
            if not _parent or _parent == _cur:
                break
            _cur = _parent
        return ""
    def _load_env_file(_path: str, *, _override: bool) -> None:
        if not _path:
            return
        try:
            with open(_path, 'r', encoding='utf-8') as _f:
                for _line in _f:
                    _s = _line.strip()
                    if not _s or _s.startswith('#') or '=' not in _s:
                        continue
                    _k, _v = _s.split('=', 1)
                    _key = _k.strip(); _val = _v.strip().strip('"').strip("'")
                    if _override or (_key not in _os.environ):
                        _os.environ[_key] = _val
        except Exception:
            pass
    try:
        _p_local = _find_upwards('.env.local')
        if _p_local:
            _load_env_file(_p_local, _override=True)
    except Exception:
        pass
    try:
        _p_env = _find_upwards('.env')
        if _p_env:
            _load_env_file(_p_env, _override=False)
    except Exception:
        pass
from . import progress as _progress
from .search import search, SearchResult
from .fetch import fetch_url, _httpx_client
from .extract import extract_content
from .rerank import chunk_text, rerank_chunks
from .archive import save_page_now, get_memento

_DEFAULT_CFG: Optional[WebConfig] = None

def set_default_config(cfg: WebConfig) -> None:
    """Inject a centralized default WebConfig used when callers omit cfg.

    This allows higher-level clients (e.g., OllamaTurboClient) to set a single
    source of truth for all web pipeline modules without threading cfg through
    every call site. Tests that monkeypatch functions may continue to omit cfg.
    """
    global _DEFAULT_CFG
    _DEFAULT_CFG = cfg


def _query_cache_key(query: str, opts: Dict[str, Any]) -> str:
    h = hashlib.sha256()
    h.update(query.encode())
    h.update(json.dumps(opts, sort_keys=True).encode())
    return h.hexdigest()


def run_research(query: str, *, cfg: Optional[WebConfig] = None, site_include: Optional[str] = None, site_exclude: Optional[str] = None, freshness_days: Optional[int] = None, top_k: int = 5, force_refresh: bool = False) -> Dict[str, Any]:
    cfg = cfg or _DEFAULT_CFG or WebConfig()
    os.makedirs(cfg.cache_root, exist_ok=True)
    opts = {
        'site_include': site_include,
        'site_exclude': site_exclude,
        'freshness_days': freshness_days,
        'top_k': top_k,
    }
    key = _query_cache_key(query, opts)
    cache_path = os.path.join(cfg.cache_root, f"query_{key}.json")
    now = time.time()
    # Load persistent dedupe index (hash -> url)
    index_path = os.path.join(cfg.cache_root, "content_hash_index.json")
    hash_to_url: Dict[str, str] = {}
    # When forcing refresh, ignore any previous dedupe index to avoid false positives in tests/runs
    if not force_refresh and os.path.isfile(index_path):
        try:
            data = json.loads(open(index_path, 'r', encoding='utf-8').read())
            if isinstance(data, dict):
                # Only keep string->string pairs
                for k, v in list(data.items()):
                    if isinstance(k, str) and isinstance(v, str):
                        hash_to_url[k] = v
        except Exception:
            pass
    if not force_refresh and os.path.isfile(cache_path):
        try:
            data = json.loads(open(cache_path, 'r', encoding='utf-8').read())
            if now - float(data.get('ts', 0)) <= cfg.cache_ttl_seconds:
                return data['result']
        except Exception:
            pass

    # Plan -> Search
    try:
        _progress.emit_current({"stage": "search", "status": "start", "query": query})
    except Exception:
        pass
    results = search(query, cfg=cfg, site=site_include, freshness_days=freshness_days)
    try:
        _progress.emit_current({"stage": "search", "status": "done", "count": len(results)})
    except Exception:
        pass

    simplified_used = False
    emergency_used = False
    # Debug counters (exposed only if cfg.debug_metrics)
    base_count = len(results)
    simplified_count = 0
    variant_count = 0
    emergency_count = 0
    if not results:
        # Heuristic: simplify long/narrow queries and retry once
        try:
            import re as _re
            toks = _re.findall(r"[A-Za-z0-9]+", query)
            stop = {
                'the','a','an','of','in','on','for','to','and','or','with','about','from','by','at','as',
                'is','are','was','were','be','being','been','this','that','these','those','it','its','into',
                '2023','2024','2025'
            }
            key_toks = [t for t in toks if t.lower() not in stop]
            short_q = " ".join(key_toks[:6]) or query[:80]
        except Exception:
            short_q = query[:80]
        if short_q and short_q.strip().lower() != query.strip().lower():
            try:
                _progress.emit_current({"stage": "search", "status": "retry", "query": short_q})
            except Exception:
                pass
            results = search(short_q, cfg=cfg, site=site_include, freshness_days=freshness_days)
            simplified_used = True
            simplified_count = len(results)

    if not results:
        # Final variant fallback: try "<ProperNoun> political makeup 2024" style seed
        try:
            import re as _re
            toks = _re.findall(r"[A-Za-z][A-Za-z0-9-]*", query)
            proper = None
            for t in toks:
                if t[0].isupper():
                    proper = t
                    break
            core = proper or (toks[0] if toks else "")
            if core:
                variant_q = f"{core} political makeup 2024"
                try:
                    _progress.emit_current({"stage": "search", "status": "variant", "query": variant_q})
                except Exception:
                    pass
                results = search(variant_q, cfg=cfg, site=site_include, freshness_days=freshness_days)
                variant_count = len(results)
        except Exception:
            pass

    if not results and cfg.emergency_bootstrap:
        # Emergency: call providers directly here to bootstrap candidates
        try:
            def _dedup_add(acc: list[SearchResult], seen: set[str], title: str, url: str, snippet: str, source: str):
                if url and url not in seen and url.startswith('http'):
                    seen.add(url)
                    acc.append(SearchResult(title=title or '', url=url, snippet=snippet or '', source=source, published=None))
            # choose a concise query for emergency path
            em_q = query
            try:
                import re as _re
                toks = _re.findall(r"[A-Za-z0-9]+", query)
                stop = {'the','a','an','of','in','on','for','to','and','or','with','about','from','by','at','as','is','are','was','were','be','being','been','this','that','these','those','it','its','into','2023','2024','2025'}
                em_q2 = " ".join([t for t in toks if t.lower() not in stop][:6])
                if em_q2:
                    em_q = em_q2
            except Exception:
                pass
            tmp: list[SearchResult] = []
            seen_urls: set[str] = set()
            with _httpx_client(cfg) as c:
                # Brave
                if getattr(cfg, 'brave_key', None):
                    try:
                        headers = {"X-Subscription-Token": cfg.brave_key, "Accept": "application/json", "User-Agent": cfg.user_agent}
                        r = c.get("https://api.search.brave.com/res/v1/web/search", params={"q": em_q, "count": 10}, headers=headers, timeout=cfg.timeout_read)
                        if r.status_code == 200:
                            data = r.json()
                            for d in ((data.get('web') or {}).get('results') or []):
                                _dedup_add(tmp, seen_urls, d.get('title',''), d.get('url',''), d.get('description',''), 'brave')
                            # generic crawl fallback
                            if not tmp:
                                def _walk(v):
                                    if isinstance(v, dict):
                                        u = v.get('url') or v.get('link') or v.get('href')
                                        t = v.get('title') or v.get('name') or ''
                                        s = v.get('description') or v.get('snippet') or ''
                                        if isinstance(u, str) and u.startswith('http'):
                                            _dedup_add(tmp, seen_urls, t, u, s, 'brave')
                                        for vv in v.values():
                                            _walk(vv)
                                    elif isinstance(v, list):
                                        for it in v:
                                            _walk(it)
                                _walk(data)
                    except Exception:
                        pass
                # Tavily
                if not tmp and getattr(cfg, 'tavily_key', None):
                    try:
                        headers = {"Authorization": f"Bearer {cfg.tavily_key}", "Content-Type":"application/json"}
                        r = c.post("https://api.tavily.com/search", json={"query": em_q, "search_depth":"basic", "max_results": 10}, headers=headers, timeout=cfg.timeout_read)
                        if r.status_code == 200:
                            data = r.json()
                            for d in data.get('results', []) or []:
                                _dedup_add(tmp, seen_urls, d.get('title',''), d.get('url',''), d.get('content',''), 'tavily')
                    except Exception:
                        pass
                # Exa
                if not tmp and getattr(cfg, 'exa_key', None):
                    try:
                        headers = {"x-api-key": cfg.exa_key, "Content-Type":"application/json"}
                        r = c.post("https://api.exa.ai/search", json={"query": em_q, "numResults": 10}, headers=headers, timeout=cfg.timeout_read)
                        if r.status_code == 200:
                            data = r.json()
                            for d in data.get('results', []) or []:
                                _dedup_add(tmp, seen_urls, d.get('title',''), d.get('url',''), d.get('snippet',''), 'exa')
                    except Exception:
                        pass
            if tmp:
                results = tmp
                emergency_used = True
                emergency_count = len(tmp)
        except Exception:
            pass

    # Wikipedia-guided expansion: use Wikipedia only for discovery.
    # For any Wikipedia results, fetch and extract external reference links,
    # and add those links as new candidates (skipping Wikipedia itself).
    wiki_refs_added = 0
    try:
        def _is_wiki(u: str) -> bool:
            try:
                h = (urlparse(u).hostname or '').lower().strip('.')
                return bool(h) and (h == 'wikipedia.org' or h.endswith('.wikipedia.org'))
            except Exception:
                return False

        expanded: List[SearchResult] = []
        seen_set: set[str] = set()
        # Keep non-wiki results as-is
        for sr in results:
            if not _is_wiki(sr.url):
                if sr.url and sr.url not in seen_set:
                    expanded.append(sr)
                    seen_set.add(sr.url)
        # Expand refs from wiki pages (bounded)
        for sr in results:
            if not _is_wiki(sr.url):
                continue
            try:
                f = fetch_url(sr.url, cfg=cfg, force_refresh=force_refresh, use_browser_if_needed=False)
                if not f.ok:
                    continue
                meta = {
                    'url': f.url,
                    'final_url': f.final_url,
                    'status': f.status,
                    'content_type': f.content_type,
                    'body_path': f.body_path,
                    'headers': f.headers,
                }
                ex = extract_content(meta, cfg=cfg)
                if not ex.ok:
                    continue
                md = ex.markdown or ''
                links = re.findall(r'https?://[^\s\)\]\>\"\']+', md)
                # Filter external links (non-wikipedia) and dedupe
                filtered: List[str] = []
                for lk in links:
                    if _is_wiki(lk):
                        continue
                    if lk not in seen_set:
                        filtered.append(lk)
                        seen_set.add(lk)
                # Add up to 8 references per wiki page to bound fanout
                for lk in filtered[:8]:
                    try:
                        host = (urlparse(lk).hostname or '')
                    except Exception:
                        host = ''
                    title = host or lk
                    expanded.append(SearchResult(title=title, url=lk, snippet='', source='wiki_ref', published=None))
                    wiki_refs_added += 1
            except Exception:
                continue
        # Replace results with expanded list if we added anything
        if wiki_refs_added > 0:
            results = expanded
    except Exception:
        pass

    citations: List[Dict[str, Any]] = []
    items: List[Dict[str, Any]] = []
    # Dedupe across URLs and content bodies (current-run only)
    seen_urls: set[str] = set()
    seen_hashes: set[str] = set()
    dedupe_lock = threading.Lock()

    dedup_skips = 0
    excluded_skips = 0

    def _host_in_excluded(u: str) -> bool:
        try:
            host = (urlparse(u).hostname or '').lower().strip('.')
            if not host:
                return False
            for dom in (cfg.exclude_citation_domains or []):
                d = str(dom or '').lower().strip('.')
                if not d:
                    continue
                if host == d or host.endswith('.' + d):
                    return True
            return False
        except Exception:
            return False
    def _build_citation(sr) -> Optional[Dict[str, Any]]:
        if site_exclude and site_exclude in (sr.url or ''):
            return None
        try:
            _progress.emit_current({"stage": "fetch", "status": "start", "url": sr.url})
        except Exception:
            pass
        # Respect exclusion list: allow discovery (search) but skip quoting as a citation
        if _host_in_excluded(sr.url):
            items.append({'url': sr.url, 'ok': False, 'reason': 'excluded-domain'})
            nonlocal excluded_skips
            with dedupe_lock:
                excluded_skips += 1
            return None
        f = fetch_url(sr.url, cfg=cfg, force_refresh=force_refresh, use_browser_if_needed=True)
        if not f.ok:
            items.append({'url': sr.url, 'ok': False, 'reason': f.reason or f"HTTP {f.status}"})
            try:
                _progress.emit_current({"stage": "fetch", "status": "error", "url": sr.url, "reason": f.reason or f"HTTP {f.status}"})
            except Exception:
                pass
            return None
        meta = {
            'url': f.url,
            'final_url': f.final_url,
            'status': f.status,
            'content_type': f.content_type,
            'body_path': f.body_path,
            'headers': f.headers,
        }
        try:
            _progress.emit_current({"stage": "extract", "status": "start", "url": f.final_url})
        except Exception:
            pass
        ex = extract_content(meta, cfg=cfg)
        if not ex.ok:
            items.append({'url': sr.url, 'ok': False, 'reason': 'extract-failed'})
            try:
                _progress.emit_current({"stage": "extract", "status": "error", "url": f.final_url})
            except Exception:
                pass
            return None
        # Dedupe by URL and content hash (computed on markdown) â€” within this run only.
        # We intentionally avoid cross-run dedupe via the persistent index because it can
        # eliminate all citations on subsequent runs and produce an empty citations list.
        body_hash = hashlib.sha256((ex.markdown or '').encode('utf-8', 'ignore')).hexdigest()
        with dedupe_lock:
            if f.final_url in seen_urls or body_hash in seen_hashes:
                nonlocal dedup_skips
                dedup_skips += 1
                return None
            seen_urls.add(f.final_url)
            seen_hashes.add(body_hash)
            # Record persistent mapping for future runs
            hash_to_url[body_hash] = f.final_url

        try:
            _progress.emit_current({"stage": "rerank", "status": "start", "url": f.final_url})
        except Exception:
            pass
        chunks = chunk_text(ex.markdown)
        ranked = rerank_chunks(query, chunks, cfg=cfg, top_k=3)
        # Archive on first success (with optional Memento pre-check)
        archive = {'archive_url': '', 'timestamp': ''}
        if cfg.archive_enabled:
            try:
                if cfg.archive_check_memento_first:
                    m = get_memento(f.final_url, cfg=cfg)
                    if m.get('archive_url'):
                        archive = m
                    else:
                        archive = save_page_now(f.final_url, cfg=cfg)
                else:
                    archive = save_page_now(f.final_url, cfg=cfg)
            except Exception:
                archive = {'archive_url': '', 'timestamp': ''}
        # Build citation entry deterministically
        page_starts = []
        try:
            if ex.kind == 'pdf':
                page_starts = list(ex.meta.get('page_start_lines') or [])
        except Exception:
            page_starts = []

        def _line_to_page(line_no: int) -> Optional[int]:
            if not page_starts:
                return None
            # Find last page start <= line_no
            lo, hi = 0, len(page_starts) - 1
            ans = 0
            while lo <= hi:
                mid = (lo + hi) // 2
                if page_starts[mid] <= line_no:
                    ans = mid
                    lo = mid + 1
                else:
                    hi = mid - 1
            return ans + 1

        cit = {
            'canonical_url': f.final_url,
            'archive_url': archive.get('archive_url', ''),
            'title': ex.title or sr.title,
            'date': ex.date or sr.published,
            'risk': ex.risk,
            'risk_reasons': ex.risk_reasons,
            'browser_used': f.browser_used,
            'kind': ex.kind,
            'lines': [],
        }
        for r in ranked:
            lines = []
            for hl in r.get('highlights', [])[:2]:
                if isinstance(hl, dict):
                    entry = {'line': int(hl.get('line', 0)), 'quote': hl.get('text', '')}
                else:
                    # backward compatibility if highlight was a string
                    entry = {'line': int(r.get('start_line', 0)), 'quote': str(hl)}
                pg = _line_to_page(entry['line'])
                if pg:
                    entry['page'] = pg
                lines.append(entry)
            if lines:
                cit['lines'].extend(lines)
        return cit

    candidates = results[: top_k * 2]
    max_workers = max(1, min(8, cfg.per_host_concurrency))
    with ThreadPoolExecutor(max_workers=max_workers) as exr:
        futs = [exr.submit(_build_citation, sr) for sr in candidates]
        for fut in as_completed(futs):
            try:
                cit = fut.result()
                if cit:
                    citations.append(cit)
                    try:
                        _progress.emit_current({"stage": "citation", "status": "added", "url": cit.get('canonical_url', '')})
                    except Exception:
                        pass
            except Exception:
                continue

    # Deterministic order: by score if available then by url hash
    citations.sort(key=lambda c: (c.get('risk', ''), hashlib.sha256(c.get('canonical_url','').encode()).hexdigest()))

    # Fallback: if no citations were produced, retry once with force_refresh=True to bypass
    # potentially stale caches or transient extraction failures.
    if not citations and not force_refresh:
        try:
            _progress.emit_current({"stage": "fallback", "status": "refresh"})
        except Exception:
            pass
        return run_research(
            query,
            cfg=cfg,
            site_include=site_include,
            site_exclude=site_exclude,
            freshness_days=freshness_days,
            top_k=top_k,
            force_refresh=True,
        )

    answer = {
        'query': query,
        'top_k': top_k,
        'citations': citations[:top_k],
        'policy': {
            'respect_robots': cfg.respect_robots,
            'allow_browser': cfg.allow_browser,
            'cache_ttl_seconds': cfg.cache_ttl_seconds,
            'simplified_query_used': simplified_used,
            'emergency_search_used': emergency_used,
        },
    }

    if cfg.debug_metrics:
        try:
            answer['debug'] = {
                'search': {
                    'initial_count': base_count,
                    'simplified_count': simplified_count,
                    'variant_count': variant_count,
                    'emergency_count': emergency_count,
                },
                'fetch': {
                    'attempted': len(candidates),
                    'ok': len(citations[:top_k]),
                    'failed': len([it for it in items if not it.get('ok')]),
                    'dedupe_skips': dedup_skips,
                    'excluded': excluded_skips,
                    'wiki_refs_added': wiki_refs_added,
                },
            }
        except Exception:
            pass

    # Persist updated dedupe index (best-effort)
    try:
        open(index_path, 'w', encoding='utf-8').write(json.dumps(hash_to_url, ensure_ascii=False))
    except Exception:
        pass
    try:
        open(cache_path, 'w', encoding='utf-8').write(json.dumps({'ts': now, 'result': answer}, ensure_ascii=False))
    except Exception:
        pass

    try:
        _progress.emit_current({"stage": "done", "status": "ok", "citations": len(citations[:top_k])})
    except Exception:
        pass
    return answer



================================================================================
FILE: src/web/search.py
================================================================================

from __future__ import annotations
import os
import re
import time
import json
import hashlib
from dataclasses import dataclass
from typing import List, Dict, Optional, Any
from urllib.parse import quote_plus, urlparse, urljoin, parse_qs, unquote
import xml.etree.ElementTree as ET
import gzip
import httpx

from .config import WebConfig
from .fetch import _httpx_client


@dataclass
class SearchQuery:
    query: str
    site: Optional[str] = None
    freshness_days: Optional[int] = None


@dataclass
class SearchResult:
    title: str
    url: str
    snippet: str
    source: str
    published: Optional[str]


def _norm(sr: Dict[str, Any], source: str) -> SearchResult:
    title = sr.get('title') or sr.get('name') or sr.get('title_full') or ''
    url = sr.get('url') or sr.get('link') or sr.get('href') or ''
    snippet = sr.get('snippet') or sr.get('description') or sr.get('summary') or ''
    published = sr.get('date') or sr.get('published') or sr.get('published_at')
    return SearchResult(title=title, url=url, snippet=snippet, source=source, published=published)


def _hash_url(u: str) -> str:
    return hashlib.sha256(u.encode()).hexdigest()[:16]


def _title_trigrams(t: str) -> set:
    toks = re.findall(r"\w+", t.lower())
    grams = set()
    for i in range(len(toks)-2):
        grams.add(" ".join(toks[i:i+3]))
    return grams


def _dedupe(items: List[SearchResult]) -> List[SearchResult]:
    seen: set = set()
    out: List[SearchResult] = []
    title_keys: list[set] = []
    for it in items:
        key = _hash_url(it.url)
        tkey = _title_trigrams(it.title)
        if key in seen:
            continue
        # basic title trigram de-dupe
        dup = False
        for k in title_keys:
            if len(k & tkey) >= 2:  # two common trigrams => duplicate-ish
                dup = True
                break
        if not dup:
            seen.add(key)
            out.append(it)
            title_keys.append(tkey)
    return out


def _norm_host(site: str) -> str:
    """Normalize a site string to host (netloc)."""
    try:
        p = urlparse(site if '://' in site else f"https://{site}")
        host = p.netloc or p.path
        return host.strip('/')
    except Exception:
        return site


def _discover_sitemaps_for_site(site: str, cfg: WebConfig) -> List[str]:
    host = _norm_host(site)
    base = f"https://{host}"
    sitemaps: List[str] = []
    try:
        headers = {"User-Agent": cfg.user_agent, "Accept": "text/plain, */*"}
        with httpx.Client(timeout=cfg.sitemap_timeout_s, headers=headers, follow_redirects=cfg.follow_redirects) as c:
            # robots.txt discovery
            try:
                r = c.get(urljoin(base, "/robots.txt"))
                if r.status_code == 200 and isinstance(r.text, str):
                    for line in r.text.splitlines():
                        if line.lower().startswith("sitemap:"):
                            url = line.split(":", 1)[1].strip()
                            if url:
                                sitemaps.append(url)
            except Exception:
                pass
            # common default
            try:
                r2 = c.get(urljoin(base, "/sitemap.xml"))
                if r2.status_code == 200:
                    sitemaps.append(str(r2.request.url))
            except Exception:
                pass
    except Exception:
        return []
    # de-dup preserve order
    seen: set[str] = set()
    out: List[str] = []
    for u in sitemaps:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out


def _parse_sitemap_urls(sitemap_url: str, cfg: WebConfig, *, limit: int, include_subs: bool, visited: set[str]) -> List[str]:
    """Parse a sitemap or sitemap index and return up to limit URLs.
    Avoid cycles via visited set.
    """
    if sitemap_url in visited or limit <= 0:
        return []
    visited.add(sitemap_url)
    urls: List[str] = []
    try:
        headers = {"User-Agent": cfg.user_agent, "Accept": "application/xml,text/xml,application/rss+xml,application/gzip"}
        with httpx.Client(timeout=cfg.sitemap_timeout_s, headers=headers, follow_redirects=cfg.follow_redirects) as c:
            r = c.get(sitemap_url)
            if r.status_code != 200:
                return []
            content: bytes
            try:
                if str(sitemap_url).endswith(".gz"):
                    content = gzip.decompress(r.content)
                else:
                    # httpx auto-decodes Content-Encoding; ensure bytes
                    content = r.content
            except Exception:
                content = r.content
            # Parse XML
            try:
                root = ET.fromstring(content)
            except Exception:
                return []
            tag = root.tag.lower()
            # Strip namespaces for robust matching
            def _strip_ns(t: str) -> str:
                return t.split('}')[-1] if '}' in t else t
            tag = _strip_ns(tag)
            if tag == 'urlset':
                # Iterate over all descendants and pick <loc> regardless of namespace
                for uel in root.iter():
                    if _strip_ns(uel.tag) == 'loc' and uel.text:
                        urls.append(uel.text.strip())
                        if len(urls) >= limit:
                            break
            elif tag == 'sitemapindex' and include_subs:
                for sm in root.iter():
                    if _strip_ns(sm.tag) == 'loc' and sm.text:
                        sub = sm.text.strip()
                        if sub and sub not in visited and len(urls) < limit:
                            sub_urls = _parse_sitemap_urls(sub, cfg, limit=limit - len(urls), include_subs=include_subs, visited=visited)
                            urls.extend(sub_urls)
                            if len(urls) >= limit:
                                break
    except Exception:
        return urls
    return urls[:limit]


def _sitemap_search(q: SearchQuery, cfg: WebConfig) -> List[SearchResult]:
    if not cfg.sitemap_enabled or not q.site:
        return []
    try:
        sitemaps = _discover_sitemaps_for_site(q.site, cfg)
        if not sitemaps:
            return []
        visited: set[str] = set()
        limit = max(1, int(cfg.sitemap_max_urls))
        include_subs = bool(cfg.sitemap_include_subs)
        agg: List[str] = []
        for sm in sitemaps:
            if len(agg) >= limit:
                break
            urls = _parse_sitemap_urls(sm, cfg, limit=limit - len(agg), include_subs=include_subs, visited=visited)
            agg.extend(urls)
            if len(agg) >= limit:
                break
        # Map to SearchResult
        items: List[SearchResult] = []
        src = 'sitemap'
        for u in agg:
            try:
                title = u
                # Derive a simple title from path
                pu = urlparse(u)
                if pu.path and pu.path.strip('/'):
                    title = pu.path.strip('/').split('/')[-1].replace('-', ' ').replace('_', ' ')
                items.append(SearchResult(title=title or u, url=u, snippet='', source=src, published=None))
            except Exception:
                continue
        return items
    except Exception:
        return []


def _search_duckduckgo_fallback(q: SearchQuery, cfg: WebConfig) -> List[SearchResult]:
    """Keyless fallback using DuckDuckGo Instant Answer API with HTML fallback.

    This avoids external API keys and keeps network policy centralized via WebConfig.
    """
    try:
        qtext = f"site:{q.site} {q.query}" if q.site else q.query
        headers_json = {"Accept": "application/json", "User-Agent": cfg.user_agent}
        params = {"q": qtext, "format": "json", "no_html": "1", "no_redirect": "1", "t": "ollama-turbo-cli"}
        results: List[SearchResult] = []
        with _httpx_client(cfg) as c:
            try:
                r = c.get("https://api.duckduckgo.com/", params=params, headers=headers_json, timeout=cfg.timeout_read)
                if r.status_code == 200:
                    data = r.json()
                    # Prefer Abstract if present
                    abstract = (data.get("AbstractText") or data.get("Abstract") or "").strip()
                    abstract_url = (data.get("AbstractURL") or "").strip()
                    if abstract and abstract_url:
                        results.append(_norm({"title": data.get("Heading") or "Instant Answer", "url": abstract_url, "snippet": abstract}, "duckduckgo"))
                    # RelatedTopics (flatten minimal subset)
                    def _flatten(items):
                        out = []
                        for it in items or []:
                            if isinstance(it, dict) and it.get("FirstURL"):
                                out.append({
                                    "title": (it.get("Text") or "").split(" - ")[0][:120],
                                    "url": it.get("FirstURL"),
                                    "snippet": it.get("Text") or "",
                                })
                            elif isinstance(it, dict) and it.get("Topics"):
                                out.extend(_flatten(it.get("Topics")))
                        return out
                    for it in _flatten(data.get("RelatedTopics")):
                        results.append(_norm(it, "duckduckgo"))
            except Exception:
                results = []
            if results:
                # Deduplicate by URL
                seen = set()
                uniq: List[SearchResult] = []
                for r0 in results:
                    if r0.url and r0.url not in seen:
                        seen.add(r0.url)
                        uniq.append(r0)
                return uniq[:10]
            # HTML fallback
            headers_html = {
                "Accept": "text/html",
                "User-Agent": cfg.user_agent,
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://duckduckgo.com/",
            }
            collected: List[SearchResult] = []
            for base in ("https://duckduckgo.com/lite/", "https://html.duckduckgo.com/html/"):
                try:
                    # Add locale and web intent to reduce gating and region variance
                    params_html = {"q": qtext, "kl": "wt-wt", "ia": "web", "t": "ollama-turbo-cli"}
                    r2 = c.get(base, params=params_html, headers=headers_html, timeout=cfg.timeout_read)
                except Exception:
                    continue
                if r2.status_code != 200:
                    continue
                html = r2.text or ""
                links = re.findall(r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', html, flags=re.I)
                for href, text in links:
                    try:
                        absolute = href if href.lower().startswith("http") else urljoin(base, href)
                        parsed = urlparse(absolute)
                        netloc = parsed.netloc or ""
                    except Exception:
                        continue
                    resolved_url = None
                    if netloc.endswith("duckduckgo.com") and parsed.path.startswith("/l/"):
                        # decode external URL from uddg param
                        try:
                            qs = parse_qs(parsed.query or "")
                            uddg = (qs.get("uddg") or [None])[0]
                            if uddg:
                                resolved_url = unquote(uddg)
                        except Exception:
                            pass
                    elif netloc.endswith("duckduckgo.com") or netloc.endswith("duck.com"):
                        continue
                    else:
                        resolved_url = absolute if absolute.lower().startswith("http") else None
                    if not resolved_url:
                        continue
                    title_text = re.sub(r"<[^>]+>", "", text)[:120].strip() or "(no title)"
                    snippet = re.sub(r"<[^>]+>", "", text)[:160].strip()
                    sr = SearchResult(title=title_text, url=resolved_url, snippet=snippet, source="duckduckgo", published=None)
                    if sr.url not in [c.url for c in collected]:
                        collected.append(sr)
                    if len(collected) >= 10:
                        break
                if collected:
                    break
            return collected[:10]
    except Exception:
        return []


def _search_brave(q: SearchQuery, cfg: WebConfig) -> List[SearchResult]:
    key = cfg.brave_key
    if not key:
        return []
    try:
        params = {"q": q.query, "count": 10}
        if q.site:
            params["q"] = f"site:{q.site} {q.query}"
        headers = {"X-Subscription-Token": key, "Accept": "application/json", "User-Agent": cfg.user_agent}
        with _httpx_client(cfg) as c:
            r = c.get("https://api.search.brave.com/res/v1/web/search", params=params, headers=headers, timeout=cfg.timeout_read)
            try:
                data = r.json()
            except Exception:
                return []
            items: List[SearchResult] = []
            # Normal, typed mapping
            try:
                for d in (data.get('web', {}) or {}).get('results', []) or []:
                    items.append(_norm({'title': d.get('title'), 'url': d.get('url'), 'snippet': d.get('description'), 'published': d.get('page_age')}, 'brave'))
            except Exception:
                items = []
            # Fallback: generic crawl for url/title pairs anywhere in the payload
            if not items:
                try:
                    def _walk(v, out: List[SearchResult]):
                        if isinstance(v, dict):
                            u = v.get('url') or v.get('link') or v.get('href')
                            t = v.get('title') or v.get('name') or v.get('heading') or ''
                            s = v.get('description') or v.get('snippet') or ''
                            if isinstance(u, str) and u.startswith('http'):
                                out.append(_norm({'title': t, 'url': u, 'snippet': s}, 'brave'))
                            for vv in v.values():
                                _walk(vv, out)
                        elif isinstance(v, list):
                            for it in v:
                                _walk(it, out)
                    tmp: List[SearchResult] = []
                    _walk(data, tmp)
                    # Deduplicate by URL preserving order
                    seen: set[str] = set()
                    items = []
                    for it in tmp:
                        if it.url and it.url not in seen:
                            seen.add(it.url)
                            items.append(it)
                except Exception:
                    pass
            return items
    except Exception:
        return []


def _search_tavily(q: SearchQuery, cfg: WebConfig) -> List[SearchResult]:
    key = cfg.tavily_key
    if not key:
        return []
    try:
        payload = {"query": q.query, "search_depth": "basic", "max_results": 10}
        if q.site:
            payload["query"] = f"site:{q.site} {q.query}"
        headers = {"Content-Type":"application/json", "Authorization": f"Bearer {key}"}
        with _httpx_client(cfg) as c:
            r = c.post("https://api.tavily.com/search", json=payload, headers=headers, timeout=cfg.timeout_read)
            data = r.json()
            items = []
            for d in data.get('results', []):
                items.append(_norm({'title': d.get('title'), 'url': d.get('url'), 'snippet': d.get('content')}, 'tavily'))
            return items
    except Exception:
        return []


def _search_exa(q: SearchQuery, cfg: WebConfig) -> List[SearchResult]:
    key = cfg.exa_key
    if not key:
        return []
    try:
        payload = {"query": q.query, "numResults": 10}
        if q.site:
            payload["query"] = f"site:{q.site} {q.query}"
        headers = {"Content-Type":"application/json", "x-api-key": key}
        with _httpx_client(cfg) as c:
            r = c.post("https://api.exa.ai/search", json=payload, headers=headers, timeout=cfg.timeout_read)
            data = r.json()
            items = []
            for d in data.get('results', []):
                items.append(_norm({'title': d.get('title'), 'url': d.get('url'), 'snippet': d.get('snippet')}, 'exa'))
            return items
    except Exception:
        return []


def _search_google_pse(q: SearchQuery, cfg: WebConfig) -> List[SearchResult]:
    key = cfg.google_pse_key; cx = cfg.google_pse_cx
    if not key or not cx:
        return []
    try:
        qtext = f"site:{q.site} {q.query}" if q.site else q.query
        with _httpx_client(cfg) as c:
            r = c.get("https://www.googleapis.com/customsearch/v1", params={"key": key, "cx": cx, "q": qtext}, timeout=cfg.timeout_read)
            data = r.json()
            items = []
            for d in data.get('items', []) or []:
                items.append(_norm({'title': d.get('title'), 'url': d.get('link'), 'snippet': d.get('snippet')}, 'google_pse'))
            return items
    except Exception:
        return []


def search(query: str, *, cfg: Optional[WebConfig] = None, site: Optional[str] = None, freshness_days: Optional[int] = None) -> List[SearchResult]:
    cfg = cfg or WebConfig()
    q = SearchQuery(query=query, site=site, freshness_days=freshness_days)
    candidates: List[SearchResult] = []
    # Provider rotation: Brave -> Tavily/Exa -> Google PSE
    candidates.extend(_search_brave(q, cfg))
    if not candidates:
        candidates.extend(_search_tavily(q, cfg))
        if not candidates:
            candidates.extend(_search_exa(q, cfg))
    if not candidates:
        candidates.extend(_search_google_pse(q, cfg))
    # Keyless fallback to DuckDuckGo if no API-backed engines succeeded
    if not candidates:
        candidates.extend(_search_duckduckgo_fallback(q, cfg))
    # Heuristic fallback: simplify long/narrow queries and retry
    if not candidates:
        try:
            toks = re.findall(r"[A-Za-z0-9]+", query)
            stop = {
                'the','a','an','of','in','on','for','to','and','or','with','about','from','by','at','as',
                'is','are','was','were','be','being','been','this','that','these','those','it','its','into',
                '2023','2024','2025'
            }
            key_toks = [t for t in toks if t.lower() not in stop]
            short_q = " ".join(key_toks[:6]) or query[:80]
        except Exception:
            short_q = query[:80]
        if short_q and short_q.strip().lower() != query.strip().lower():
            q2 = SearchQuery(query=short_q, site=site, freshness_days=freshness_days)
            # Retry full rotation
            items2: List[SearchResult] = []
            items2.extend(_search_brave(q2, cfg))
            if not items2:
                items2.extend(_search_tavily(q2, cfg))
                if not items2:
                    items2.extend(_search_exa(q2, cfg))
            if not items2:
                items2.extend(_search_google_pse(q2, cfg))
            if not items2:
                items2.extend(_search_duckduckgo_fallback(q2, cfg))
            candidates.extend(items2)
    # Optional sitemap ingestion (augment results for site-restricted queries)
    if cfg.sitemap_enabled and q.site:
        try:
            candidates.extend(_sitemap_search(q, cfg))
        except Exception:
            pass
    # Deduplicate
    out = _dedupe([it for it in candidates if it.url])
    return out[:15]



================================================================================
FILE: src/web/fetch.py
================================================================================

from __future__ import annotations
import os
import re
import io
import time
import json
import math
import hashlib
import random
import atexit
from collections import OrderedDict
from email.utils import parsedate_to_datetime
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple
import threading
import ipaddress
import fnmatch
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

from .config import WebConfig
from .robots import RobotsPolicy


# Note: Remote DNS semantics
# - When a proxy is enabled and selected for a domain host, we deliberately skip local DNS
#   to avoid leaking queries (remote DNS via proxy). IP-literals are always resolved locally
#   and checked against private/link-local/multicast/reserved ranges to enforce SSRF policy.
# - With HTTP(S) proxies, any peer IP obtained post-connect is the proxy's IP, not the origin.
#   Do not treat it as origin verification.
@dataclass
class FetchResult:
    ok: bool
    status: int
    url: str
    final_url: str
    headers: Dict[str, str]
    content_type: str
    bytes: int
    body_path: Optional[str]
    meta_path: Optional[str]
    cached: bool
    browser_used: bool
    reason: Optional[str] = None


def _canonicalize_url(url: str) -> str:
    try:
        p = urlparse(url)
        # Drop fragment, normalize scheme/host
        scheme = p.scheme.lower() if p.scheme else 'https'
        netloc = p.netloc.lower()
        # Remove default ports
        if (scheme == 'http' and netloc.endswith(':80')) or (scheme == 'https' and netloc.endswith(':443')):
            netloc = netloc.rsplit(':', 1)[0]
        # Normalize path
        path = p.path or '/'
        # Filter tracking query params and sort
        try:
            params = []
            for k, v in parse_qsl(p.query, keep_blank_values=True):
                kl = k.lower()
                if kl.startswith('utm_') or kl in {'gclid', 'fbclid', 'ref', 'mc_cid', 'mc_eid', 'igshid'}:
                    continue
                params.append((k, v))
            query = urlencode(sorted(params)) if params else ''
        except Exception:
            query = p.query
        return urlunparse((scheme, netloc, path, p.params, query, ''))
    except Exception:
        return url


def _host_allowed(cfg: WebConfig, host: str) -> bool:
    # Single source of truth: use cfg.sandbox_allow only (no env fallback here)
    allow = cfg.sandbox_allow or ''
    if not allow:
        return True  # if no allowlist defined, default allow
    for pat in (p.strip() for p in allow.split(',') if p.strip()):
        if fnmatch.fnmatch(host or '', pat):
            return True
    return False


def _check_ip_blocks(host: str) -> None:
    import socket, ipaddress
    # Strip brackets for IPv6 literals like "[::1]" to keep getaddrinfo happy
    raw = (host or '').strip('[]')
    infos = socket.getaddrinfo(raw, None)
    for info in infos:
        ip_any = info[4][0]
        ip_str = str(ip_any)
        ip_obj = ipaddress.ip_address(ip_str)
        if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_link_local or ip_obj.is_multicast or ip_obj.is_reserved:
            raise ValueError(f"Blocked private/loopback IP: {ip_str}")
        if ip_str.startswith('169.254.169.254'):
            raise ValueError(f"Blocked metadata IP: {ip_str}")


def _cache_paths(cfg: WebConfig, url: str) -> Tuple[str, str]:
    os.makedirs(cfg.cache_root, exist_ok=True)
    key = hashlib.sha256(_canonicalize_url(url).encode()).hexdigest()
    return os.path.join(cfg.cache_root, f"{key}.bin"), os.path.join(cfg.cache_root, f"{key}.json")


def _idna_ascii(host: str) -> str:
    try:
        return host.encode('idna').decode('ascii') if host else host
    except Exception:
        return host


def _is_ip_literal(host: str) -> bool:
    try:
        ip = (host or '').strip('[]')
        ipaddress.ip_address(ip)
        return True
    except Exception:
        return False


def _bypass_proxy(host: str, no_proxy: str) -> bool:
    try:
        h = host or ''
        patterns = [p.strip() for p in (no_proxy or '').split(',') if p.strip()]
        for pat in patterns:
            if pat.startswith('.'):
                if h.endswith(pat) or h == pat.lstrip('.'):
                    return True
            else:
                if fnmatch.fnmatch(h, pat):
                    return True
    except Exception:
        pass
    return False


def _cfg_proxy_for_scheme(cfg: WebConfig, scheme: str) -> Optional[str]:
    try:
        if scheme.lower() == 'https':
            return cfg.https_proxy or cfg.all_proxy or None
        if scheme.lower() == 'http':
            return cfg.http_proxy or cfg.all_proxy or None
        return cfg.all_proxy or None
    except Exception:
        return None


def _httpx_client(cfg: WebConfig):
    import httpx
    # httpx requires either a default timeout or all four: connect, read, write, pool
    timeout = httpx.Timeout(
        connect=cfg.timeout_connect,
        read=cfg.timeout_read,
        write=cfg.timeout_write,
        pool=cfg.timeout_connect,
    )
    limits = httpx.Limits(max_connections=cfg.max_connections, max_keepalive_connections=cfg.max_keepalive)
    # Build proxies from centralized cfg (do not read env here)
    proxies = None
    def _norm_proxy(u: Optional[str]) -> Optional[str]:
        try:
            if not u:
                return None
            s = str(u).strip()
            if s.lower() in {"none", "null", "false", "0"}:
                return None
            # Basic URL shape check; accept http(s) and socks schemes
            if not (s.startswith("http://") or s.startswith("https://") or s.startswith("socks")):
                return None
            return s
        except Exception:
            return None
    try:
        if cfg.sandbox_allow_proxies:
            http_p = _norm_proxy(cfg.http_proxy) or _norm_proxy(cfg.all_proxy)
            https_p = _norm_proxy(cfg.https_proxy) or _norm_proxy(cfg.all_proxy)
            if http_p or https_p:
                proxies = {}
                if http_p:
                    proxies["http"] = http_p
                if https_p:
                    proxies["https"] = https_p
    except Exception:
        proxies = None
    # Always disable trust_env so only cfg drives behavior
    client_kwargs = {
        "timeout": timeout,
        "limits": limits,
        "headers": {"User-Agent": cfg.user_agent},
        "follow_redirects": cfg.follow_redirects,
        "trust_env": False,
    }
    # Prefer HTTP/1.1 for wider compatibility; set http2 only if supported
    try:
        client_kwargs["http2"] = False
    except Exception:
        pass
    # Include proxies only when allowed and supported
    if proxies:
        client_kwargs["proxies"] = proxies
    try:
        return httpx.Client(**client_kwargs)
    except TypeError:
        # Fallback: remove proxies if this httpx version doesn't accept the kwarg
        client_kwargs.pop("proxies", None)
        try:
            return httpx.Client(**client_kwargs)
        except TypeError:
            # Final fallback: remove http2 flag if unsupported
            client_kwargs.pop("http2", None)
            return httpx.Client(**client_kwargs)


# Small LRU pool of httpx.Clients keyed by origin
_CLIENT_POOL: "OrderedDict[str, Any]" = OrderedDict()
_CLIENT_POOL_LOCK = threading.Lock()


def _origin_of(url: str) -> str:
    p = urlparse(url)
    return f"{p.scheme.lower()}://{p.netloc.lower()}"


def _get_client_for_origin(cfg: WebConfig, origin: str):
    try:
        with _CLIENT_POOL_LOCK:
            if origin in _CLIENT_POOL:
                client = _CLIENT_POOL.pop(origin)
                _CLIENT_POOL[origin] = client
                return client
        # create new
        client = _httpx_client(cfg)
        with _CLIENT_POOL_LOCK:
            _CLIENT_POOL[origin] = client
            # Evict LRU if over capacity
            while len(_CLIENT_POOL) > max(1, cfg.client_pool_size):
                _, old = _CLIENT_POOL.popitem(last=False)
                try:
                    old.close()
                except Exception:
                    pass
        return client
    except Exception:
        # Fallback: create a one-off client
        return _httpx_client(cfg)


def _close_all_clients():
    try:
        with _CLIENT_POOL_LOCK:
            for client in list(_CLIENT_POOL.values()):
                try:
                    client.close()
                except Exception:
                    pass
            _CLIENT_POOL.clear()
    except Exception:
        pass


atexit.register(_close_all_clients)


def _should_escalate_to_browser(status: int, ctype: str, body: bytes) -> bool:
    if status in (403, 401):
        return True
    if ('text/html' in ctype or 'application/xhtml+xml' in ctype) and len(body) < 1024:
        # Likely JS app shell
        return True
    # Heuristic: if HTML but contains minimal text and lots of JS
    txt = body.decode(errors='ignore')[:4096]
    if '<script' in txt.lower() and len(re.sub(r"<[^>]+>", ' ', txt).strip()) < 200:
        return True
    return False


_last_fetch: Dict[str, float] = {}
_lf_lock = threading.Lock()


class _TokenBucket:
    def __init__(self, capacity: int, rate: float) -> None:
        self.capacity = max(1, capacity)
        self.rate = max(0.01, rate)
        self.tokens = float(self.capacity)
        self.last = time.time()
        self.lock = threading.Lock()

    def acquire_wait(self, need: float = 1.0) -> float:
        with self.lock:
            now = time.time()
            # Refill
            elapsed = now - self.last
            if elapsed > 0:
                self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)
                self.last = now
            if self.tokens >= need:
                self.tokens -= need
                return 0.0
            # compute wait time for needed tokens
            deficit = need - self.tokens
            wait = deficit / self.rate
            # Don't modify tokens yet; just report wait
            self.last = now
            return max(0.0, wait)


_TB_MAP: Dict[str, _TokenBucket] = {}
_TB_LOCK = threading.Lock()


def _tb_for_host(host: str, cfg: WebConfig) -> _TokenBucket:
    with _TB_LOCK:
        tb = _TB_MAP.get(host)
        if tb is None:
            tb = _TokenBucket(cfg.rate_tokens_per_host, cfg.rate_refill_per_sec)
            _TB_MAP[host] = tb
        return tb


def _parse_retry_after(val: str) -> float:
    try:
        # seconds
        secs = int(val.strip())
        return float(max(0, secs))
    except Exception:
        pass
    try:
        dt = parsedate_to_datetime(val)
        if dt is not None:
            return max(0.0, (dt.timestamp() - time.time()))
    except Exception:
        pass
    return 0.0

def fetch_url(url: str, *, cfg: Optional[WebConfig] = None, robots: Optional[RobotsPolicy] = None, force_refresh: bool = False, use_browser_if_needed: bool = True) -> FetchResult:
    cfg = cfg or WebConfig()
    robots = robots or RobotsPolicy(cfg)
    url = _canonicalize_url(url)

    parsed = urlparse(url)
    if parsed.scheme == 'http' and not cfg.sandbox_allow_http:
        return FetchResult(False, 0, url, url, {}, '', 0, None, None, False, False, reason='HTTP blocked by policy')
    # IDNA-normalize host for allowlist matching
    if not _host_allowed(cfg, _idna_ascii(parsed.hostname or '')):
        return FetchResult(False, 0, url, url, {}, '', 0, None, None, False, False, reason=f'Host not in allowlist: {parsed.hostname}')
    # Apply remote DNS semantics: if proxies are enabled and would be used for this host (and the host is not an IP literal),
    # skip local DNS resolution to avoid leaks. Always resolve and check IP literals, or when not using a proxy.
    try:
        host = parsed.hostname or ''
        a_host = _idna_ascii(host)
        use_proxy = False
        try:
            if cfg.sandbox_allow_proxies:
                proxy_url = _cfg_proxy_for_scheme(cfg, parsed.scheme) or _cfg_proxy_for_scheme(cfg, 'http')
                if proxy_url:
                    use_proxy = not _bypass_proxy(a_host, cfg.no_proxy or '')
        except Exception:
            use_proxy = False
        if _is_ip_literal(a_host):
            _check_ip_blocks(a_host)
        else:
            if not use_proxy:
                _check_ip_blocks(a_host)
    except Exception as e:
        return FetchResult(False, 0, url, url, {}, '', 0, None, None, False, False, reason=str(e))

    if cfg.respect_robots:
        rec = robots.check(url)
        if not rec.allow:
            delay = rec.crawl_delay or 0
            if delay:
                time.sleep(min(delay, cfg.max_crawl_delay_s))
            return FetchResult(False, 0, url, url, {}, '', 0, None, None, False, False, reason='Blocked by robots.txt')
        # Enforce crawl-delay for allowed hosts as well
        host = parsed.hostname or ''
        if rec.crawl_delay:
            with _lf_lock:
                last = _last_fetch.get(host, 0.0)
                now = time.time()
                wait = rec.crawl_delay - (now - last)
            if wait and wait > 0:
                time.sleep(min(wait, cfg.max_crawl_delay_s))
            with _lf_lock:
                _last_fetch[host] = time.time()

    # Per-host token bucket
    host = parsed.hostname or ''
    try:
        tb_wait = _tb_for_host(host, cfg).acquire_wait(1.0)
        if tb_wait > 0:
            time.sleep(min(tb_wait, 5.0))
    except Exception:
        pass

    body_path, meta_path = _cache_paths(cfg, url)
    now = time.time()
    cached = False
    if not force_refresh and os.path.isfile(meta_path) and os.path.isfile(body_path):
        try:
            meta = json.loads(open(meta_path, 'r', encoding='utf-8').read())
            if now - float(meta.get('ts', 0)) <= cfg.cache_ttl_seconds:
                raw = open(body_path, 'rb').read()
                return FetchResult(True, int(meta.get('status', 200)), url, meta.get('final_url', url), dict(meta.get('headers', {})), meta.get('content_type', ''), len(raw), body_path, meta_path, True, bool(meta.get('browser_used', False)))
        except Exception:
            pass

    # Prepare conditional headers if any (for revalidation)
    prev_meta: Optional[Dict[str, Any]] = None
    prev_etag = None
    prev_lm = None
    try:
        if os.path.isfile(meta_path):
            prev_meta = json.loads(open(meta_path, 'r', encoding='utf-8').read())
            hdrs = dict(prev_meta.get('headers', {}))
            prev_etag = hdrs.get('etag')
            prev_lm = hdrs.get('last-modified')
    except Exception:
        prev_meta = None
        prev_etag = None
        prev_lm = None

    # Build common headers
    accept = cfg.accept_header_override or "text/html,application/xhtml+xml,application/pdf;q=0.9,text/plain;q=0.8,*/*;q=0.1"
    common_headers = {"Accept": accept}

    # HTTP path
    try:
        import httpx  # type: ignore
        client = _get_client_for_origin(cfg, _origin_of(url))
        # Optional HEAD gating
        if cfg.head_gating_enabled:
            try:
                hresp = client.head(url, headers=common_headers)
                h_ct = hresp.headers.get('content-type', '')
                h_cl = hresp.headers.get('content-length')
                if hresp.status_code in (429, 503) and cfg.respect_retry_after:
                    ra = hresp.headers.get('retry-after')
                    if ra:
                        delay = _parse_retry_after(ra)
                        if delay > 0:
                            time.sleep(min(delay, cfg.retry_backoff_max))
                if h_cl is not None:
                    try:
                        if int(h_cl) > cfg.max_download_bytes:
                            return FetchResult(False, int(hresp.status_code or 0), url, str(hresp.request.url), dict(hresp.headers), h_ct, 0, None, None, False, False, reason='content too large')
                    except Exception:
                        pass
                if h_ct:
                    allowed = any(x in h_ct for x in [
                        'text/html', 'application/xhtml+xml', 'application/pdf', 'text/plain'
                    ])
                    if not allowed:
                        return FetchResult(False, int(hresp.status_code or 0), url, str(hresp.request.url), dict(hresp.headers), h_ct, 0, None, None, False, False, reason='unsupported content-type')
            except Exception:
                pass

        # Retry with backoff and jitter
        attempts = max(1, cfg.retry_attempts)
        last_exc: Optional[Exception] = None
        data = b''
        headers: Dict[str, str] = {}
        status = 0
        content_type = ''
        resp_final_url = url
        for i in range(attempts):
            try:
                req_headers = dict(common_headers)
                if prev_etag:
                    req_headers['If-None-Match'] = str(prev_etag)
                if prev_lm:
                    req_headers['If-Modified-Since'] = str(prev_lm)
                with client.stream("GET", url, headers=req_headers) as resp:
                    status = resp.status_code
                    # Handle 304 revalidation
                    if status == 304 and prev_meta and os.path.isfile(body_path):
                        try:
                            raw = open(body_path, 'rb').read()
                        except Exception:
                            raw = b''
                        pm_headers = dict(prev_meta.get('headers', {}))
                        return FetchResult(True, int(prev_meta.get('status', 200)), url, prev_meta.get('final_url', url), pm_headers, prev_meta.get('content_type', ''), len(raw), body_path, meta_path, True, bool(prev_meta.get('browser_used', False)))
                    if status >= 500 or status in (429, 503):
                        ra = resp.headers.get('retry-after')
                        if cfg.respect_retry_after and ra:
                            delay = _parse_retry_after(ra)
                            if delay > 0:
                                time.sleep(min(delay, cfg.retry_backoff_max))
                        raise RuntimeError(f"server error {status}")
                    # Read with cap
                    buf = bytearray()
                    for chunk in resp.iter_bytes():
                        if chunk:
                            buf += chunk
                            if len(buf) >= cfg.max_download_bytes:
                                break
                    data = bytes(buf)
                    headers = {k: v for k, v in resp.headers.items()}
                    content_type = headers.get('content-type', '')
                    resp_final_url = str(resp.request.url)
                break
            except Exception as e:
                last_exc = e
                backoff = min(cfg.retry_backoff_max, cfg.retry_backoff_base * (2 ** i))
                time.sleep(backoff + random.random() * 0.2)
        else:
            raise last_exc or RuntimeError("request failed")

        final_url = resp_final_url
        # Revalidate final URL host/scheme and SSRF/IP policy similar to sandbox path
        try:
            f_parsed = urlparse(final_url)
            if f_parsed.scheme == 'http' and not cfg.sandbox_allow_http:
                return FetchResult(False, int(status or 0), url, final_url, headers, content_type, len(data or b''), None, None, False, False, reason='HTTP blocked by policy')
            # Use IDNA-normalized host for allowlist check
            if not _host_allowed(cfg, _idna_ascii(f_parsed.hostname or '')):
                return FetchResult(False, int(status or 0), url, final_url, headers, content_type, len(data or b''), None, None, False, False, reason=f'Final host not in allowlist: {f_parsed.hostname}')
            # Apply remote DNS semantics for final host
            f_host = _idna_ascii(f_parsed.hostname or '')
            use_proxy_f = False
            try:
                if cfg.sandbox_allow_proxies:
                    purl = _cfg_proxy_for_scheme(cfg, f_parsed.scheme) or _cfg_proxy_for_scheme(cfg, 'http')
                    if purl:
                        use_proxy_f = not _bypass_proxy(f_host, cfg.no_proxy or '')
            except Exception:
                use_proxy_f = False
            if _is_ip_literal(f_host):
                _check_ip_blocks(f_host)
            else:
                if not use_proxy_f:
                    _check_ip_blocks(f_host)
        except Exception as reval_err:
            return FetchResult(False, int(status or 0), url, final_url, headers, content_type, len(data or b''), None, None, False, False, reason=str(reval_err))
        raw = data
        browser_needed = use_browser_if_needed and _should_escalate_to_browser(status, content_type, raw)
    except Exception as e:
        status = 0
        final_url = url
        headers = {}
        content_type = ''
        raw = b''
        browser_needed = use_browser_if_needed  # escalate on network errors

    browser_used = False
    if browser_needed and cfg.allow_browser:
        bres = fetch_with_browser(url, cfg=cfg)
        if bres and bres.ok:
            status = bres.status
            final_url = bres.final_url
            headers = bres.headers
            content_type = bres.content_type
            raw = open(bres.body_path, 'rb').read() if bres.body_path and os.path.isfile(bres.body_path) else raw
            browser_used = True
        else:
            # keep HTTP result if any
            pass

    # Save to cache
    try:
        meta = {
            'ts': now,
            'status': status,
            'final_url': final_url,
            'headers': headers,
            'content_type': content_type,
            'browser_used': browser_used,
        }
        open(body_path, 'wb').write(raw)
        open(meta_path, 'w', encoding='utf-8').write(json.dumps(meta))
        cached = False
    except Exception:
        body_path = None
        meta_path = None
        cached = False

    return FetchResult(True if status and status < 400 else False, int(status or 0), url, final_url, headers, content_type, len(raw), body_path, meta_path, cached, browser_used)


def fetch_with_browser(url: str, *, cfg: Optional[WebConfig] = None) -> Optional[FetchResult]:
    cfg = cfg or WebConfig()
    try:
        from playwright.sync_api import sync_playwright  # type: ignore
    except Exception:
        return None

    parsed = urlparse(url)
    if parsed.scheme == 'http' and not cfg.sandbox_allow_http:
        return FetchResult(False, 0, url, url, {}, '', 0, None, None, False, True, reason='HTTP blocked by policy')

    body_path, meta_path = _cache_paths(cfg, url + "#browser")
    html = ""
    headers: Dict[str, str] = {}
    final_url = url
    status = 200

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent=cfg.user_agent)
        # Light stealth: tweak navigator properties where safe
        if cfg.browser_stealth_light:
            try:
                context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                """)
            except Exception:
                pass
        page = context.new_page()
        network_logs: list[dict] = []
        # Build block list for resource types
        _block_types = set()
        try:
            for t in (cfg.browser_block_resources or '').split(','):
                t = t.strip().lower()
                if t:
                    _block_types.add(t)
        except Exception:
            _block_types = set()
        def _route_guard(route):
            try:
                req = route.request
                rtype = (getattr(req, 'resource_type', None) or '').lower()
                url = req.url or ''
                network_logs.append({'url': url, 'method': req.method})
                # Block heavy/analytics resources
                if rtype in _block_types:
                    return route.abort()
                lower = url.lower()
                if any(k in lower for k in ['analytics', 'doubleclick', 'googletagmanager', 'adservice', 'pixel', 'mixpanel', 'segment.io', 'facebook.net']):
                    return route.abort()
            except Exception:
                pass
            try:
                route.continue_()
            except Exception:
                try:
                    route.fallback()
                except Exception:
                    pass
        try:
            page.route("**/*", _route_guard)
        except Exception:
            pass

        try:
            page.goto(url, wait_until="load", timeout=int(cfg.timeout_read * 1000))
            # Auto-detect infinite scroll (bounded)
            for _ in range(max(1, cfg.browser_max_pages)):
                page.wait_for_timeout(cfg.browser_wait_ms)
                before = page.content()
                page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
                page.wait_for_timeout(cfg.browser_wait_ms)
                after = page.content()
                if before == after:
                    break
            html = page.content()
            final_url = page.url
            try:
                headers['content-type'] = 'text/html; charset=utf-8'
            except Exception:
                pass
            # Screenshot for debugging
            try:
                shot_path = os.path.join(cfg.cache_root, 'last_screenshot.png')
                page.screenshot(path=shot_path)
            except Exception:
                pass
        finally:
            try:
                context.close(); browser.close()
            except Exception:
                pass

    raw = html.encode('utf-8', errors='ignore')
    # Save
    try:
        meta = {
            'ts': time.time(),
            'status': status,
            'final_url': final_url,
            'headers': headers,
            'content_type': headers.get('content-type', ''),
            'browser_used': True,
        }
        open(body_path, 'wb').write(raw)
        open(meta_path, 'w', encoding='utf-8').write(json.dumps(meta))
    except Exception:
        pass
    return FetchResult(True, status, url, final_url, headers, headers.get('content-type', ''), len(raw), body_path, meta_path, False, True)



================================================================================
FILE: src/web/extract.py
================================================================================

from __future__ import annotations
import os
import re
import io
import json
from dataclasses import dataclass
from typing import Optional, Dict, Any

from .config import WebConfig
from .fetch import _httpx_client


@dataclass
class ExtractResult:
    ok: bool
    kind: str  # html|pdf|text|binary
    markdown: str
    title: str
    date: Optional[str]
    meta: Dict[str, Any]
    used: Dict[str, bool]  # which extractors were used
    risk: str
    risk_reasons: list[str]


def _assess_risk(text: str) -> tuple[str, list[str]]:
    reasons: list[str] = []
    t = text.lower()
    flags = [
        ("ignore previous", 5),
        ("system prompt", 4),
        ("developer message", 3),
        ("hidden", 2),
        ("data:text", 3),
        ("base64", 2),
        ("#instructions", 2),
        ("prompt injection", 5),
        ("instructions: ", 2),
    ]
    score = 0
    for phrase, w in flags:
        if phrase in t:
            reasons.append(f"found '{phrase}'")
            score += w
    # Long data URIs
    if re.search(r"data:[^;]+;base64,[a-z0-9/+]{200,}", t):
        reasons.append("long data URI")
        score += 5
    risk = 'LOW'
    if score >= 8:
        risk = 'HIGH'
    elif score >= 4:
        risk = 'MED'
    return risk, reasons


def _html_to_markdown(html: str) -> tuple[str, Dict[str, Any], Dict[str, bool]]:
    used = {"trafilatura": False, "readability": False, "jina": False}
    meta: Dict[str, Any] = {"title": "", "date": None}
    markdown = ""
    # Try trafilatura
    try:
        import trafilatura  # type: ignore
        art = trafilatura.extract(html, include_formatting=True, include_links=True, output_format='markdown')
        if art:
            markdown = art
            used["trafilatura"] = True
            meta["title"] = trafilatura.bare_extraction(html).get('title', '') if hasattr(trafilatura, 'bare_extraction') else ''
    except Exception:
        pass
    # Fallback readability-lxml
    if not markdown:
        try:
            from readability import Document  # type: ignore
            doc = Document(html)
            title = doc.short_title() or ''
            content_html = doc.summary()
            # naive convert: strip tags
            text = re.sub(r"<[^>]+>", "\n", content_html)
            text = re.sub(r"\n{2,}", "\n\n", text)
            markdown = f"# {title}\n\n{text.strip()}\n"
            used["readability"] = True
            meta["title"] = title
        except Exception:
            pass
    return markdown or "", meta, used


def _pdf_to_text(bin_data: bytes) -> tuple[str, Dict[str, Any], Dict[str, bool]]:
    used = {"pdfminer": False, "pymupdf": False, "ocrmypdf": False}
    text = ""
    meta: Dict[str, Any] = {}
    pages: list[str] = []
    # Prefer PyMuPDF for speed/robustness
    try:
        import fitz  # PyMuPDF type: ignore
        doc = fitz.open(stream=bin_data, filetype="pdf")
        parts: list[str] = []
        for page in doc:
            parts.append(page.get_text())
        text = "\n\n".join(parts)
        pages = parts[:]
        used["pymupdf"] = True
    except Exception:
        pass
    # Fallback to pdfminer if PyMuPDF failed or yielded empty
    if not text:
        try:
            from pdfminer.high_level import extract_text  # type: ignore
            with io.BytesIO(bin_data) as fp:
                text = extract_text(fp) or ""
            used["pdfminer"] = True
        except Exception:
            pass
    # OCR fallback if text density is very low
    if len((text or '').strip()) < 40:
        # Attempt OCR if ocrmypdf exists in PATH
        try:
            import shutil, subprocess, tempfile
            if shutil.which('ocrmypdf'):
                used["ocrmypdf"] = True
                with tempfile.TemporaryDirectory() as td:
                    in_path = os.path.join(td, 'in.pdf')
                    out_path = os.path.join(td, 'out.pdf')
                    with open(in_path, 'wb') as f:
                        f.write(bin_data)
                    subprocess.run(['ocrmypdf', '--skip-text', in_path, out_path], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    # Re-extract
                    try:
                        from pdfminer.high_level import extract_text as ex2  # type: ignore
                        with open(out_path, 'rb') as f:
                            text = ex2(f) or text
                    except Exception:
                        pass
        except Exception:
            pass
    # Derive pages and page_start_lines from text if not obtained via PyMuPDF
    if not pages:
        # pdfminer typically uses form-feed '\f' as page separator
        if '\f' in text:
            pages = text.split('\f')
        else:
            pages = [text] if text else []
    page_start_lines: list[int] = []
    cur = 1
    for ptxt in pages:
        page_start_lines.append(cur)
        cur += len(ptxt.splitlines())
    meta["page_start_lines"] = page_start_lines
    meta["pages"] = len(pages)
    return text, meta, used


def extract_content(fetch_meta: Dict[str, Any], *, cfg: Optional[WebConfig] = None) -> ExtractResult:
    cfg = cfg or WebConfig()
    ctype = (fetch_meta.get('content_type') or '').lower()
    body_path = fetch_meta.get('body_path')
    raw = b''
    if body_path and os.path.isfile(body_path):
        raw = open(body_path, 'rb').read()
    title = ''
    date = None
    markdown = ''
    meta: Dict[str, Any] = {}
    used: Dict[str, bool] = {}

    if 'pdf' in ctype or (body_path and body_path.lower().endswith('.pdf')):
        text, meta_pdf, used_pdf = _pdf_to_text(raw)
        markdown = text
        meta.update(meta_pdf)
        used.update(used_pdf)
        kind = 'pdf'
    elif 'html' in ctype or ('<html' in raw[:200].decode(errors='ignore').lower() if raw else False):
        md, meta_html, used_html = _html_to_markdown(raw.decode(errors='ignore'))
        markdown = md
        meta.update(meta_html)
        used.update(used_html)
        kind = 'html'
        # If empty, try Jina Reader fallback
        if not markdown:
            try:
                reader_url = f"https://r.jina.ai/{fetch_meta.get('final_url') or fetch_meta.get('url')}"
                with _httpx_client(cfg) as client:
                    jr = client.get(reader_url, timeout=cfg.timeout_read)
                    if jr.status_code == 200:
                        markdown = jr.text
                        used["jina"] = True
            except Exception:
                pass
    else:
        # plain text or binary
        if raw:
            try:
                markdown = raw.decode('utf-8')
                kind = 'text'
            except Exception:
                markdown = ''
                kind = 'binary'
        else:
            markdown = ''
            kind = 'binary'

    # Optional cleanup for common wiki artifacts such as "[edit]" anchors
    try:
        if cfg.clean_wiki_edit_anchors and markdown:
            # Remove markdown links whose visible text is exactly 'edit' (case-insensitive)
            markdown = re.sub(r"\[\s*edit\s*\]\([^)]+\)", "", markdown, flags=re.I)
            # Occasionally appears as nested brackets from HTML â†’ markdown conversions
            markdown = re.sub(r"\[\s*\[?\s*edit\s*\]?\s*\]\([^)]+\)", "", markdown, flags=re.I)
            # Remove stray '[edit]' tokens
            markdown = re.sub(r"\[\s*edit\s*\]", "", markdown, flags=re.I)
            # Trim leftover excess whitespace before newlines
            markdown = re.sub(r"[ \t]+\n", "\n", markdown)
    except Exception:
        pass

    risk, reasons = _assess_risk((markdown or '')[:10000])
    return ExtractResult(
        ok=bool(markdown),
        kind=kind,
        markdown=markdown,
        title=meta.get('title', ''),
        date=meta.get('date'),
        meta=meta,
        used=used,
        risk=risk,
        risk_reasons=reasons,
    )



================================================================================
FILE: src/web/rerank.py
================================================================================

from __future__ import annotations
import re
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict, Any
from .config import WebConfig
from .rerank_adapter_router import rerank as _adapter_rerank


@dataclass
class Chunk:
    id: str
    text: str
    start_line: int
    end_line: int


def _estimate_tokens(text: str) -> int:
    # Simple approximation: 1 token ~ 4 chars avg
    return max(1, int(len(text) / 4))


def chunk_text(markdown: str, *, target_tokens: int = 768) -> List[Chunk]:
    lines = markdown.splitlines()
    chunks: List[Chunk] = []
    buf: list[str] = []
    start_line = 1
    cur_tokens = 0

    def flush(end_line: int):
        nonlocal buf, start_line, cur_tokens
        if not buf:
            return
        txt = "\n".join(buf)
        cid = f"{start_line}-{end_line}"
        chunks.append(Chunk(id=cid, text=txt, start_line=start_line, end_line=end_line))
        buf = []
        cur_tokens = 0
        start_line = end_line + 1

    for i, line in enumerate(lines, start=1):
        # Prefer breaks at headings or blank lines
        if line.strip().startswith('#') and buf:
            flush(i - 1)
        buf.append(line)
        cur_tokens += _estimate_tokens(line + '\n')
        if cur_tokens >= target_tokens and (line.strip() == '' or line.strip().startswith('#')):
            flush(i)
    flush(len(lines))
    return chunks


def _simple_rerank(query: str, chunks: List[Chunk]) -> List[Tuple[Chunk, float]]:
    q = re.findall(r"\w+", query.lower())
    out: List[Tuple[Chunk, float]] = []
    for ch in chunks:
        t = ch.text.lower()
        score = sum(t.count(w) for w in q)
        out.append((ch, float(score)))
    out.sort(key=lambda x: (-x[1], x[0].start_line))
    return out


def rerank_chunks(query: str, chunks: List[Chunk], *, cfg: Optional[WebConfig] = None, top_k: int = 5) -> List[Dict[str, Any]]:
    cfg = cfg or WebConfig()
    if not chunks:
        return []

    docs = [ch.text for ch in chunks]
    indices: List[int] = []
    try:
        indices = _adapter_rerank(cfg, query, docs, top_n=min(top_k, len(docs)))
    except Exception:
        indices = []

    results: List[Tuple[Chunk, float]] = []
    if indices:
        for rank, idx in enumerate(indices):
            if 0 <= idx < len(chunks):
                results.append((chunks[idx], float(len(indices) - rank)))
    else:
        results = _simple_rerank(query, chunks)[:top_k]

    out: List[Dict[str, Any]] = []
    for ch, score in results[:top_k]:
        # Highlight first matching snippet lines with absolute line numbers
        qwords = re.findall(r"\w+", query.lower())
        lines = ch.text.splitlines()
        highlights: list[Dict[str, Any]] = []
        for idx, ln in enumerate(lines, start=0):
            if any(w in ln.lower() for w in qwords):
                abs_line = ch.start_line + idx
                highlights.append({'line': abs_line, 'text': ln.strip()})
            if len(highlights) >= 3:
                break
        out.append({
            'id': ch.id,
            'score': float(score),
            'start_line': ch.start_line,
            'end_line': ch.end_line,
            'highlights': highlights,
            'preview': lines[:3],
        })
    return out



================================================================================
FILE: src/web/archive.py
================================================================================

from __future__ import annotations
import os
import json
import time
from typing import Optional, Dict
from urllib.parse import quote

from .config import WebConfig
from .fetch import _httpx_client


def get_memento(url: str, *, cfg: Optional[WebConfig] = None) -> Dict[str, str]:
    """Query Wayback availability API for an existing snapshot.

    Returns a dict with keys: {'archive_url': str, 'timestamp': str}
    If none found, values are empty strings.
    """
    cfg = cfg or WebConfig()
    api = f"https://archive.org/wayback/available?url={quote(url, safe='')}"
    out = {'archive_url': '', 'timestamp': ''}
    try:
        headers = {'User-Agent': cfg.user_agent, 'Accept': 'application/json'}
        with _httpx_client(cfg) as c:
            r = c.get(api, headers=headers, timeout=cfg.timeout_read)
            if r.status_code == 200:
                data = r.json()
                closest = (data.get('archived_snapshots') or {}).get('closest') or {}
                if closest.get('available') and closest.get('url'):
                    out['archive_url'] = str(closest.get('url') or '')
                    out['timestamp'] = str(closest.get('timestamp') or '')
    except Exception:
        return {'archive_url': '', 'timestamp': ''}
    return out


def save_page_now(url: str, *, cfg: Optional[WebConfig] = None) -> Dict[str, str]:
    cfg = cfg or WebConfig()
    api_url = f"https://web.archive.org/save/{quote(url, safe='')}"
    # Retry with backoff, respect 429 Retry-After if configured
    attempts = max(1, cfg.retry_attempts)
    backoff = max(0.0, cfg.retry_backoff_base)
    out = {'archive_url': '', 'timestamp': ''}
    try:
        for i in range(attempts):
            try:
                with _httpx_client(cfg) as client:
                    resp = client.post(api_url, headers={'User-Agent': cfg.user_agent}, timeout=cfg.timeout_read, follow_redirects=True)
                # Parse headers
                arch = resp.headers.get('content-location', '') or resp.headers.get('Content-Location', '')
                if arch and arch.startswith('/'):
                    arch = f"https://web.archive.org{arch}"
                mdt = resp.headers.get('memento-datetime', '') or resp.headers.get('Memento-Datetime', '')
                # Some responses include a memento link header
                if not arch:
                    link = resp.headers.get('link') or resp.headers.get('Link')
                    if isinstance(link, str) and 'rel="memento"' in link:
                        # crude parse: <URL>; rel="memento"
                        try:
                            start = link.find('<')
                            end = link.find('>')
                            if start != -1 and end != -1 and end > start:
                                arch = link[start+1:end]
                        except Exception:
                            arch = ''
                if arch:
                    out = {'archive_url': arch, 'timestamp': mdt}
                    break
                # Handle 429 if configured
                if resp.status_code == 429 and cfg.archive_retry_on_429 and i < attempts - 1:
                    ra = resp.headers.get('retry-after') or resp.headers.get('Retry-After')
                    try:
                        delay = float(ra) if ra and str(ra).strip().isdigit() else backoff * (2 ** i)
                    except Exception:
                        delay = backoff * (2 ** i)
                    delay = min(delay, cfg.retry_backoff_max)
                    time.sleep(max(0.0, delay))
                    continue
                # Non-429 without archive location: backoff and retry
                if i < attempts - 1:
                    time.sleep(min(cfg.retry_backoff_max, backoff * (2 ** i)))
            except Exception:
                if i < attempts - 1:
                    time.sleep(min(cfg.retry_backoff_max, backoff * (2 ** i)))
                continue
    except Exception:
        return {'archive_url': '', 'timestamp': ''}
    return out



================================================================================
FILE: src/web/robots.py
================================================================================

from __future__ import annotations
import time
import re
import os
import json
from dataclasses import dataclass
from typing import Dict, Optional
from urllib.parse import urlparse
from urllib import robotparser
from .config import WebConfig
import httpx


@dataclass
class RobotsRecord:
    ts: float
    allow: bool
    crawl_delay: Optional[float]
    sitemaps: list[str]
    raw: str


class RobotsPolicy:
    def __init__(self, cfg: Optional[WebConfig] = None) -> None:
        self.cfg = cfg or WebConfig()
        self._mem: Dict[str, RobotsRecord] = {}
        os.makedirs(self.cfg.cache_root, exist_ok=True)

    def _cache_path(self, host: str) -> str:
        safe = re.sub(r"[^a-zA-Z0-9._-]", "_", host)
        return os.path.join(self.cfg.cache_root, f"robots_{safe}.json")

    def _load_cache(self, host: str) -> Optional[RobotsRecord]:
        path = self._cache_path(host)
        try:
            if os.path.isfile(path):
                data = json.loads(open(path, 'r', encoding='utf-8').read())
                cd = data.get('crawl_delay')
                try:
                    cd_f = float(cd) if cd is not None else None
                except Exception:
                    cd_f = None
                return RobotsRecord(
                    ts=float(data.get('ts', 0)),
                    allow=bool(data.get('allow', True)),
                    crawl_delay=cd_f,
                    sitemaps=list(data.get('sitemaps', []) or []),
                    raw=str(data.get('raw', '')),
                )
        except Exception:
            return None
        return None

    def _save_cache(self, host: str, rec: RobotsRecord) -> None:
        path = self._cache_path(host)
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(rec.__dict__, f)
        except Exception:
            pass

    def _fetch_robots(self, base: str) -> RobotsRecord:
        """Fetch and parse robots.txt using urllib.robotparser and cache it."""
        host = urlparse(base).hostname or ''
        rp = robotparser.RobotFileParser()
        robots_url = f"{urlparse(base).scheme}://{host}/robots.txt"
        try:
            headers = {"User-Agent": self.cfg.user_agent, "Accept": "text/plain,*/*;q=0.1"}
            # Local client avoids circular import on fetch
            with httpx.Client(timeout=self.cfg.timeout_read, headers={"User-Agent": self.cfg.user_agent}, follow_redirects=True) as client:
                resp = client.get(robots_url, headers=headers)
                raw = resp.text if resp.status_code == 200 else ''
        except Exception:
            raw = ''
        rp.parse(raw.splitlines())
        allow = True
        try:
            allow = rp.can_fetch(self.cfg.user_agent, base)
        except Exception:
            allow = True
        crawl_delay = None
        try:
            crawl_delay = rp.crawl_delay(self.cfg.user_agent)
        except Exception:
            crawl_delay = None
        sitemaps: list[str] = []
        try:
            sitemaps = list(getattr(rp, 'site_maps', []) or [])
        except Exception:
            sitemaps = []
        rec = RobotsRecord(ts=time.time(), allow=bool(allow), crawl_delay=crawl_delay, sitemaps=sitemaps, raw=raw)
        return rec

    def check(self, url: str) -> RobotsRecord:
        parsed = urlparse(url)
        host = parsed.hostname or ''
        now = time.time()
        # Memory
        rec = self._mem.get(host)
        if rec and (now - rec.ts) < self.cfg.robots_ttl_seconds:
            return rec
        # Disk
        drec = self._load_cache(host)
        if drec and (now - drec.ts) < self.cfg.robots_ttl_seconds:
            self._mem[host] = drec
            return drec
        # Fetch
        base = f"{parsed.scheme}://{host}/"
        rec = self._fetch_robots(base)
        self._mem[host] = rec
        self._save_cache(host, rec)
        return rec



================================================================================
FILE: src/web/progress.py
================================================================================

from __future__ import annotations

"""
Minimal progress bus for web_research to emit progress events that can be
forwarded as SSE from the API without invasive changes.

Usage:
  - API route registers a channel id -> queue and runs the chat turn in a
    background thread.
  - The background thread attaches to the same channel id before executing
    tools. The web_research pipeline emits progress via emit_current().
  - The API thread polls the queue and relays events as SSE 'progress'.
"""

import queue
import threading
from typing import Dict, Optional, Any

_channels: Dict[str, "queue.Queue[dict]"] = {}
_lock = threading.Lock()
_tls = threading.local()


def register(channel_id: str) -> "queue.Queue[dict]":
    q: "queue.Queue[dict]" = queue.Queue()
    with _lock:
        _channels[channel_id] = q
    return q


def unregister(channel_id: str) -> None:
    with _lock:
        _channels.pop(channel_id, None)


def attach_current(channel_id: str) -> None:
    setattr(_tls, "channel_id", channel_id)


def detach_current() -> None:
    if hasattr(_tls, "channel_id"):
        delattr(_tls, "channel_id")


def emit(channel_id: str, event: dict) -> None:
    with _lock:
        q = _channels.get(channel_id)
    if q is not None:
        try:
            q.put_nowait(event)
        except Exception:
            pass


def emit_current(event: dict) -> None:
    cid = getattr(_tls, "channel_id", None)
    if isinstance(cid, str) and cid:
        emit(cid, event)




================================================================================
FILE: src/core/config.py
================================================================================

from __future__ import annotations

"""
Configuration dataclasses for the modularized Ollama Turbo CLI backend.

Phase A scaffolding: these types mirror the existing runtime knobs in
`src/client.py`. They are not yet wired into the client, but provide a
stable place for future dependency injection without behavior changes.

IMPORTANT: Defaults here match current behavior as implemented in
`OllamaTurboClient`. Environment parsing helpers are provided to allow
non-invasive adoption later.
"""

from dataclasses import dataclass, field
from typing import Any, Dict, Optional
import os


# ----------------------------- Helpers -----------------------------

def _env_bool(name: str, default: bool) -> bool:
    try:
        v = os.getenv(name)
        if v is None:
            return default
        return str(v).strip().lower() not in {"0", "false", "no", "off"}
    except Exception:
        return default


def _env_int(name: str, default: int, *, min_value: Optional[int] = None) -> int:
    try:
        v = os.getenv(name)
        if v is None or str(v).strip() == "":
            return default
        val = int(v)
        if min_value is not None:
            val = max(min_value, val)
        return val
    except Exception:
        return default


def _env_float(name: str, default: float, *, min_value: Optional[float] = None) -> float:
    try:
        v = os.getenv(name)
        if v is None or str(v).strip() == "":
            return default
        val = float(v)
        if min_value is not None:
            val = max(min_value, val)
        return val
    except Exception:
        return default


# ----------------------------- Dataclasses -----------------------------

@dataclass
class RetryConfig:
    enabled: bool = True  # CLI_RETRY_ENABLED
    max_retries: int = 3  # CLI_MAX_RETRIES


@dataclass
class TransportConfig:
    # Engine and host resolution
    engine: Optional[str] = None
    host: Optional[str] = None  # resolved by networking layer
    # Keep-alive
    keep_alive_raw: Optional[str] = field(default_factory=lambda: os.getenv("OLLAMA_KEEP_ALIVE") or None)
    warm_models: bool = field(default_factory=lambda: _env_bool("WARM_MODELS", True))
    # HTTP timeouts
    connect_timeout_s: float = field(default_factory=lambda: _env_float("CLI_CONNECT_TIMEOUT_S", 5.0, min_value=1.0))
    read_timeout_s: float = field(default_factory=lambda: _env_float("CLI_READ_TIMEOUT_S", 600.0, min_value=60.0))


@dataclass
class StreamingConfig:
    idle_reconnect_secs: int = field(default_factory=lambda: _env_int("CLI_STREAM_IDLE_RECONNECT_SECS", 90, min_value=10))


@dataclass
class SamplingConfig:
    reasoning: str = "high"  # low|medium|high
    reasoning_mode: str = "system"  # 'system' | 'request:top' | 'request:options'
    max_output_tokens: Optional[int] = None
    ctx_size: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None


@dataclass
class ReasoningInjectionConfig:
    """Controls request-level reasoning injection placement and style.

    field_path: dot-path in the request payload to place reasoning effort.
      - Typical: 'options.reasoning_effort' (for request:options mode)
      - Or: 'reasoning' (for request:top mode)
      - Leave empty to auto-pick based on reasoning_mode.
    field_style: 'string' | 'object'
      - 'string' â†’ injects a simple string (e.g., 'high').
      - 'object' â†’ injects {object_key: 'high'}.
    object_key: key to use when style is 'object' (default: 'effort').
    """
    field_path: str = ""  # default: auto by mode
    field_style: str = field(default_factory=lambda: (os.getenv("REASONING_FIELD_STYLE") or "string").strip().lower())
    object_key: str = field(default_factory=lambda: os.getenv("REASONING_OBJECT_KEY", "effort"))


@dataclass
class PromptConfig:
    """Prompt presentation preferences.

    verbosity: 'concise' | 'detailed' â€” controls style line in system prompts.
    fewshots: include few-shot section (disabled by default to preserve behavior).
    verbose_after_tools: use the more verbose post-tool reprompt (off by default).
    """
    verbosity: str = field(default_factory=lambda: (os.getenv("PROMPT_VERBOSITY", "concise") or "concise").lower())
    fewshots: bool = field(default_factory=lambda: _env_bool("PROMPT_FEWSHOTS", False))
    verbose_after_tools: bool = field(default_factory=lambda: _env_bool("PROMPT_VERBOSE_AFTER_TOOLS", False))

@dataclass
class ToolingConfig:
    enabled: bool = True
    print_limit: int = 2000
    context_cap: int = field(default_factory=lambda: _env_int("TOOL_CONTEXT_MAX_CHARS", 4000))
    multi_round: bool = field(default_factory=lambda: _env_bool("MULTI_ROUND_TOOLS", True))
    max_rounds: int = field(default_factory=lambda: max(1, _env_int("TOOL_MAX_ROUNDS",10)))
    results_format: str = field(default_factory=lambda: (os.getenv("TOOL_RESULTS_FORMAT") or "string").strip().lower())


@dataclass
class Mem0Config:
    # Static config
    enabled: bool = True
    local: bool = False  # MEM0_USE_LOCAL
    vector_provider: str = "chroma"
    vector_host: str = ":memory:"
    vector_port: int = 0
    ollama_url: Optional[str] = None  # embedder base (compat)
    # Optional explicit bases
    llm_base_url: Optional[str] = None
    embedder_base_url: Optional[str] = None
    llm_model: Optional[str] = None
    embedder_model: str = "embeddinggemma"
    user_id: str = "cli-user"  # unified default across client and config
    agent_id: Optional[str] = None
    app_id: Optional[str] = None
    api_key: Optional[str] = None
    org_id: Optional[str] = None
    project_id: Optional[str] = None
    # Runtime knobs
    debug: bool = False
    max_hits: int = 10
    search_timeout_ms: int = 800
    timeout_connect_ms: int = 1000
    timeout_read_ms: int = 2000
    add_queue_max: int = 256
    breaker_threshold: int = 3
    breaker_cooldown_ms: int = 60000
    search_workers: int = field(default_factory=lambda: _env_int("MEM0_SEARCH_WORKERS", 2))
    in_first_system: bool = field(default_factory=lambda: _env_bool("MEM0_IN_FIRST_SYSTEM", False))
    # Proxy / reranker controls
    proxy_model: Optional[str] = None  # MEM0_PROXY_MODEL
    proxy_timeout_ms: int = 1200      # MEM0_PROXY_TIMEOUT_MS
    rerank_search_limit: int = 10     # MEM0_RERANK_SEARCH_LIMIT
    # Context construction
    context_budget_chars: int = 12000   # MEM0_CONTEXT_BUDGET_CHARS
    # Output format for mem0 client responses (avoid deprecation of v1.0)
    output_format: str = field(default_factory=lambda: os.getenv("MEM0_OUTPUT_FORMAT", "v1.1"))


@dataclass
class ReliabilityConfig:
    ground: bool = False
    k: Optional[int] = None
    cite: bool = False
    check: str = "off"  # off|warn|enforce
    consensus: bool = False
    eval_corpus: Optional[str] = None


@dataclass
class HistoryConfig:
    max_history: int = field(default_factory=lambda: max(2, min(_env_int("MAX_CONVERSATION_HISTORY", 10), 10)))


@dataclass
class RerankProviderSpec:
    name: str = "cohere"  # cohere|voyage
    model: str = ""
    base_url: Optional[str] = None
    top_n: int = 20
    weight: float = 1.0


@dataclass
class WebConfig:
    # Identity
    user_agent: str = field(default_factory=lambda: os.getenv("WEB_UA", "ollama-turbo-cli-web/1.0 (+https://github.com)"))
    # Timeouts (seconds)
    timeout_connect: float = field(default_factory=lambda: _env_float("WEB_TIMEOUT_CONNECT", 5.0))
    timeout_read: float = field(default_factory=lambda: _env_float("WEB_TIMEOUT_READ", 15.0))
    timeout_write: float = field(default_factory=lambda: _env_float("WEB_TIMEOUT_WRITE", 10.0))
    # Retries
    retry_attempts: int = field(default_factory=lambda: _env_int("WEB_RETRY_ATTEMPTS", 3, min_value=0))
    retry_backoff_base: float = field(default_factory=lambda: _env_float("WEB_RETRY_BACKOFF_BASE", 0.4))
    retry_backoff_max: float = field(default_factory=lambda: _env_float("WEB_RETRY_BACKOFF_MAX", 6.0))
    # Concurrency
    max_connections: int = field(default_factory=lambda: _env_int("WEB_MAX_CONNECTIONS", 20, min_value=1))
    max_keepalive: int = field(default_factory=lambda: _env_int("WEB_MAX_KEEPALIVE", 10, min_value=0))
    per_host_concurrency: int = field(default_factory=lambda: _env_int("WEB_PER_HOST_CONCURRENCY", 4, min_value=1))
    # Fetch behavior
    follow_redirects: bool = field(default_factory=lambda: _env_bool("WEB_FOLLOW_REDIRECTS", True))
    head_gating_enabled: bool = field(default_factory=lambda: _env_bool("WEB_HEAD_GATING", True))
    max_download_bytes: int = field(default_factory=lambda: _env_int("WEB_MAX_DOWNLOAD_BYTES", 10 * 1024 * 1024, min_value=1024))
    accept_header_override: str = field(default_factory=lambda: os.getenv("WEB_ACCEPT_HEADER", ""))
    client_pool_size: int = field(default_factory=lambda: _env_int("WEB_CLIENT_POOL_SIZE", 16, min_value=1))
    # Caching and robots
    cache_ttl_seconds: int = field(default_factory=lambda: _env_int("WEB_CACHE_TTL_SECONDS", 86400, min_value=0))
    robots_ttl_seconds: int = field(default_factory=lambda: _env_int("WEB_ROBOTS_TTL_SECONDS", 3600, min_value=0))
    max_crawl_delay_s: int = field(default_factory=lambda: _env_int("WEB_MAX_CRAWL_DELAY_S", 20, min_value=0))
    # Provider keys
    brave_key: Optional[str] = field(default_factory=lambda: os.getenv("BRAVE_API_KEY"))
    tavily_key: Optional[str] = field(default_factory=lambda: os.getenv("TAVILY_API_KEY"))
    exa_key: Optional[str] = field(default_factory=lambda: os.getenv("EXA_API_KEY"))
    google_pse_cx: Optional[str] = field(default_factory=lambda: os.getenv("GOOGLE_PSE_CX"))
    google_pse_key: Optional[str] = field(default_factory=lambda: os.getenv("GOOGLE_PSE_KEY"))
    # Rerank
    cohere_key: Optional[str] = field(default_factory=lambda: os.getenv("COHERE_API_KEY"))
    voyage_key: Optional[str] = field(default_factory=lambda: os.getenv("VOYAGE_API_KEY"))
    rerank_enabled: bool = field(default_factory=lambda: _env_bool("WEB_RERANK_ENABLED", True))
    rerank_mode: str = field(default_factory=lambda: os.getenv("WEB_RERANK_MODE", "sdk"))  # sdk|rest
    rerank_timeout_ms: int = field(default_factory=lambda: _env_int("WEB_RERANK_TIMEOUT_MS", 2000, min_value=100))
    rerank_cache_ttl_s: int = field(default_factory=lambda: _env_int("WEB_RERANK_CACHE_TTL_S", 300, min_value=0))
    rerank_breaker_threshold: int = field(default_factory=lambda: _env_int("WEB_RERANK_BREAKER_THRESHOLD", 3, min_value=1))
    rerank_breaker_cooldown_ms: int = field(default_factory=lambda: _env_int("WEB_RERANK_BREAKER_COOLDOWN_MS", 60000, min_value=1000))
    rerank_providers: list[RerankProviderSpec] = field(default_factory=lambda: [
        RerankProviderSpec(name="cohere", model="rerank-english-v3.0"),
        RerankProviderSpec(name="voyage", model="rerank-2"),
    ])
    # Policies
    respect_robots: bool = field(default_factory=lambda: _env_bool("WEB_RESPECT_ROBOTS", True))
    allow_browser: bool = field(default_factory=lambda: _env_bool("WEB_ALLOW_BROWSER", True))
    # Citation policy
    exclude_citation_domains: list[str] = field(default_factory=lambda: [
        d.strip().lower() for d in (os.getenv("WEB_EXCLUDE_CITATION_DOMAINS", "").split(",") if os.getenv("WEB_EXCLUDE_CITATION_DOMAINS") else []) if d.strip()
    ])
    # Debugging / fallbacks
    emergency_bootstrap: bool = field(default_factory=lambda: _env_bool("WEB_EMERGENCY_BOOTSTRAP", True))
    debug_metrics: bool = field(default_factory=lambda: _env_bool("WEB_DEBUG_METRICS", False))
    # Rate limiting
    rate_tokens_per_host: int = field(default_factory=lambda: _env_int("WEB_RATE_TOKENS_PER_HOST", 4, min_value=1))
    rate_refill_per_sec: float = field(default_factory=lambda: _env_float("WEB_RATE_REFILL_PER_SEC", 0.5, min_value=0.01))
    respect_retry_after: bool = field(default_factory=lambda: _env_bool("WEB_RESPECT_RETRY_AFTER", True))
    # Allowlist integration (reuse sandbox policy)
    sandbox_allow: Optional[str] = field(default_factory=lambda: os.getenv("SANDBOX_NET_ALLOW", "*"))
    sandbox_allow_http: bool = field(default_factory=lambda: _env_bool("SANDBOX_ALLOW_HTTP", False))
    sandbox_allow_proxies: bool = field(default_factory=lambda: _env_bool("SANDBOX_ALLOW_PROXIES", False))
    # Network safety policy
    block_private_ips: bool = field(default_factory=lambda: _env_bool("SANDBOX_BLOCK_PRIVATE_IPS", True))
    # Proxy environment (centralized)
    http_proxy: Optional[str] = field(default_factory=lambda: (os.getenv("HTTP_PROXY") or os.getenv("http_proxy")))
    https_proxy: Optional[str] = field(default_factory=lambda: (os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")))
    all_proxy: Optional[str] = field(default_factory=lambda: (os.getenv("ALL_PROXY") or os.getenv("all_proxy")))
    no_proxy: Optional[str] = field(default_factory=lambda: (os.getenv("NO_PROXY") or os.getenv("no_proxy")))
    # Storage locationsq
    cache_root: str = field(default_factory=lambda: os.getenv("WEB_CACHE_ROOT", ".sandbox/webcache"))
    archive_enabled: bool = field(default_factory=lambda: _env_bool("WEB_ARCHIVE_ENABLED", True))
    archive_check_memento_first: bool = field(default_factory=lambda: _env_bool("WEB_ARCHIVE_CHECK_FIRST", False))
    archive_retry_on_429: bool = field(default_factory=lambda: _env_bool("WEB_ARCHIVE_RETRY_ON_429", True))
    # Browser limits
    browser_max_pages: int = field(default_factory=lambda: _env_int("WEB_BROWSER_MAX_PAGES", 10, min_value=1))
    browser_wait_ms: int = field(default_factory=lambda: _env_int("WEB_BROWSER_WAIT_MS", 1200, min_value=0))
    browser_block_resources: str = field(default_factory=lambda: os.getenv("WEB_BROWSER_BLOCK_RESOURCES", "image,font,media"))
    browser_stealth_light: bool = field(default_factory=lambda: _env_bool("WEB_BROWSER_STEALTH_LIGHT", False))
    # Content post-processing toggles
    clean_wiki_edit_anchors: bool = field(default_factory=lambda: _env_bool("WEB_CLEAN_WIKI_EDIT_ANCHORS", True))
    # Sitemaps
    sitemap_enabled: bool = field(default_factory=lambda: _env_bool("WEB_SITEMAP_ENABLED", False))
    sitemap_max_urls: int = field(default_factory=lambda: _env_int("WEB_SITEMAP_MAX_URLS", 50, min_value=1))
    sitemap_timeout_s: float = field(default_factory=lambda: _env_float("WEB_SITEMAP_TIMEOUT_S", 5.0, min_value=1.0))
    sitemap_include_subs: bool = field(default_factory=lambda: _env_bool("WEB_SITEMAP_INCLUDE_SUBS", True))

@dataclass
class ClientRuntimeConfig:
    model: str = "gpt-oss:120b"
    protocol: str = "auto"
    quiet: bool = False
    show_trace: bool = False

    retry: RetryConfig = field(default_factory=RetryConfig)
    transport: TransportConfig = field(default_factory=TransportConfig)
    streaming: StreamingConfig = field(default_factory=StreamingConfig)
    sampling: SamplingConfig = field(default_factory=SamplingConfig)
    tooling: ToolingConfig = field(default_factory=ToolingConfig)
    reasoning_injection: ReasoningInjectionConfig = field(default_factory=ReasoningInjectionConfig)
    prompt: PromptConfig = field(default_factory=PromptConfig)
    mem0: Mem0Config = field(default_factory=Mem0Config)
    reliability: ReliabilityConfig = field(default_factory=ReliabilityConfig)
    history: HistoryConfig = field(default_factory=HistoryConfig)
    web: WebConfig = field(default_factory=WebConfig)

    @classmethod
    def from_env(
        cls,
        *,
        model: Optional[str] = None,
        protocol: Optional[str] = None,
        quiet: Optional[bool] = None,
        show_trace: Optional[bool] = None,
        engine: Optional[str] = None,
    ) -> "ClientRuntimeConfig":
        """
        Construct a runtime config from environment variables, mirroring
        the defaults used by the current `OllamaTurboClient`.
        """
        cfg = cls()
        # Model and protocol: CLI overrides first, then env, then defaults
        if model is not None:
            cfg.model = model
        else:
            cfg.model = os.getenv("OLLAMA_MODEL", cfg.model)
        if protocol is not None:
            cfg.protocol = protocol
        else:
            cfg.protocol = os.getenv("OLLAMA_PROTOCOL", cfg.protocol)
        if quiet is not None:
            cfg.quiet = quiet
        if show_trace is not None:
            cfg.show_trace = show_trace
        if engine is not None:
            cfg.transport.engine = engine

        # Retry
        cfg.retry.enabled = _env_bool("CLI_RETRY_ENABLED", True)
        cfg.retry.max_retries = _env_int("CLI_MAX_RETRIES", 3, min_value=0)

        # Transport
        cfg.transport.keep_alive_raw = os.getenv("OLLAMA_KEEP_ALIVE") or None
        cfg.transport.warm_models = _env_bool("WARM_MODELS", True)
        cfg.transport.connect_timeout_s = _env_float("CLI_CONNECT_TIMEOUT_S", 5.0, min_value=1.0)
        cfg.transport.read_timeout_s = _env_float("CLI_READ_TIMEOUT_S", 600.0, min_value=60.0)

        # Streaming
        cfg.streaming.idle_reconnect_secs = _env_int("CLI_STREAM_IDLE_RECONNECT_SECS", 90, min_value=10)

        # Tooling
        cfg.tooling.context_cap = _env_int("TOOL_CONTEXT_MAX_CHARS", 4000)
        cfg.tooling.multi_round = _env_bool("MULTI_ROUND_TOOLS", True)
        cfg.tooling.max_rounds = max(1, _env_int("TOOL_MAX_ROUNDS", 6))
        trf = (os.getenv("TOOL_RESULTS_FORMAT") or "string").strip().lower()
        cfg.tooling.results_format = "object" if trf == "object" else "string"

        # Sampling: reasoning, mode, caps, and penalties
        try:
            r_env = os.getenv("REASONING")
            if r_env:
                r = r_env.strip().lower()
                if r in {"low", "medium", "high"}:
                    cfg.sampling.reasoning = r
        except Exception:
            pass
        try:
            rm_env = os.getenv("REASONING_MODE")
            if rm_env:
                rm = rm_env.strip().lower()
                if rm in {"system", "request:top", "request:options"}:
                    cfg.sampling.reasoning_mode = rm
        except Exception:
            pass
        # Numeric sampling envs (optional)
        try:
            v = os.getenv("MAX_OUTPUT_TOKENS")
            if v and v.strip().isdigit():
                cfg.sampling.max_output_tokens = int(v)
        except Exception:
            pass
        try:
            v = os.getenv("CTX_SIZE")
            if v and v.strip().isdigit():
                cfg.sampling.ctx_size = int(v)
        except Exception:
            pass
        try:
            v = os.getenv("TEMPERATURE")
            if v not in (None, ""):
                cfg.sampling.temperature = float(v)
        except Exception:
            pass
        try:
            v = os.getenv("TOP_P")
            if v not in (None, ""):
                cfg.sampling.top_p = float(v)
        except Exception:
            pass
        try:
            v = os.getenv("PRESENCE_PENALTY")
            if v not in (None, ""):
                cfg.sampling.presence_penalty = float(v)
        except Exception:
            pass
        try:
            v = os.getenv("FREQUENCY_PENALTY")
            if v not in (None, ""):
                cfg.sampling.frequency_penalty = float(v)
        except Exception:
            pass

        # Mem0
        cfg.mem0.enabled = _env_bool("MEM0_ENABLED", cfg.mem0.enabled)
        cfg.mem0.local = _env_bool("MEM0_USE_LOCAL", cfg.mem0.local)
        # Static/local embeddings bases
        cfg.mem0.ollama_url = os.getenv("MEM0_OLLAMA_URL") or os.getenv("MEM0_OLLAMA_BASE_URL") or cfg.mem0.ollama_url
        cfg.mem0.llm_base_url = os.getenv("MEM0_LLM_OLLAMA_URL") or cfg.mem0.llm_base_url
        cfg.mem0.embedder_base_url = os.getenv("MEM0_EMBEDDER_OLLAMA_URL") or cfg.mem0.embedder_base_url
        # Vector store settings
        cfg.mem0.vector_provider = os.getenv("MEM0_VECTOR_PROVIDER", cfg.mem0.vector_provider)
        cfg.mem0.vector_host = os.getenv("MEM0_VECTOR_HOST", cfg.mem0.vector_host)
        try:
            cfg.mem0.vector_port = _env_int("MEM0_VECTOR_PORT", cfg.mem0.vector_port)
        except Exception:
            pass
        # Models and identity
        cfg.mem0.llm_model = os.getenv("MEM0_LLM_MODEL") or cfg.mem0.llm_model
        cfg.mem0.embedder_model = os.getenv("MEM0_EMBEDDER_MODEL", cfg.mem0.embedder_model)
        cfg.mem0.search_workers = _env_int("MEM0_SEARCH_WORKERS", 2)
        # Mem0 search timeout (ms)
        try:
            cfg.mem0.search_timeout_ms = _env_int("MEM0_SEARCH_TIMEOUT_MS", cfg.mem0.search_timeout_ms, min_value=50)
        except Exception:
            pass
        cfg.mem0.in_first_system = _env_bool("MEM0_IN_FIRST_SYSTEM", False)
        cfg.mem0.user_id = os.getenv("MEM0_USER_ID", cfg.mem0.user_id)
        cfg.mem0.agent_id = os.getenv("MEM0_AGENT_ID") or None
        cfg.mem0.app_id = os.getenv("MEM0_APP_ID") or None
        cfg.mem0.api_key = os.getenv("MEM0_API_KEY") or None
        cfg.mem0.org_id = os.getenv("MEM0_ORG_ID") or None
        cfg.mem0.project_id = os.getenv("MEM0_PROJECT_ID") or None
        cfg.mem0.proxy_model = os.getenv("MEM0_PROXY_MODEL") or None
        try:
            cfg.mem0.proxy_timeout_ms = _env_int("MEM0_PROXY_TIMEOUT_MS", cfg.mem0.proxy_timeout_ms, min_value=100)
        except Exception:
            pass
        try:
            cfg.mem0.rerank_search_limit = _env_int("MEM0_RERANK_SEARCH_LIMIT", cfg.mem0.rerank_search_limit, min_value=1)
        except Exception:
            pass
        try:
            cfg.mem0.context_budget_chars = _env_int("MEM0_CONTEXT_BUDGET_CHARS", cfg.mem0.context_budget_chars, min_value=100)
        except Exception:
            pass

        # History window
        cfg.history.max_history = max(2, min(_env_int("MAX_CONVERSATION_HISTORY", 10), 10))

        # Reasoning injection (override field_path only; style/object_key picked up via defaults)
        try:
            fp = os.getenv("REASONING_FIELD_PATH")
            if fp is not None:
                cfg.reasoning_injection.field_path = fp.strip()
        except Exception:
            pass

        return cfg



================================================================================
FILE: src/web/config.py
================================================================================

from ..core.config import WebConfig, RerankProviderSpec

__all__ = ["WebConfig", "RerankProviderSpec"]



================================================================================
FILE: CONFIG_REFERENCE.md
================================================================================

# #   C o n f i g u r a t i o n   R e f e r e n c e   ( a u t o - g e n e r a t e d ) 
 
 
 
 R u n :   ` p y t h o n   - m   s c r i p t s . g e n _ c o n f i g _ r e f e r e n c e   >   C O N F I G _ R E F E R E N C E . m d ` 
 
 
 
 # # #   C l i e n t R u n t i m e C o n f i g 
 
 
 
 -   m o d e l :   d e f a u l t = ' g p t - o s s : 1 2 0 b '     V%    t y p e = s t r 
 
 -   p r o t o c o l :   d e f a u l t = ' a u t o '     V%    t y p e = s t r 
 
 -   q u i e t :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   s h o w _ t r a c e :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   r e t r y :   d e f a u l t = < f a c t o r y >     V%    t y p e = R e t r y C o n f i g 
 
 -   t r a n s p o r t :   d e f a u l t = < f a c t o r y >     V%    t y p e = T r a n s p o r t C o n f i g 
 
 -   s t r e a m i n g :   d e f a u l t = < f a c t o r y >     V%    t y p e = S t r e a m i n g C o n f i g 
 
 -   s a m p l i n g :   d e f a u l t = < f a c t o r y >     V%    t y p e = S a m p l i n g C o n f i g 
 
 -   t o o l i n g :   d e f a u l t = < f a c t o r y >     V%    t y p e = T o o l i n g C o n f i g 
 
 -   r e a s o n i n g _ i n j e c t i o n :   d e f a u l t = < f a c t o r y >     V%    t y p e = R e a s o n i n g I n j e c t i o n C o n f i g 
 
 -   p r o m p t :   d e f a u l t = < f a c t o r y >     V%    t y p e = P r o m p t C o n f i g 
 
 -   m e m 0 :   d e f a u l t = < f a c t o r y >     V%    t y p e = M e m 0 C o n f i g 
 
 -   r e l i a b i l i t y :   d e f a u l t = < f a c t o r y >     V%    t y p e = R e l i a b i l i t y C o n f i g 
 
 -   h i s t o r y :   d e f a u l t = < f a c t o r y >     V%    t y p e = H i s t o r y C o n f i g 
 
 -   w e b :   d e f a u l t = < f a c t o r y >     V%    t y p e = W e b C o n f i g 
 
 
 
 # # #   R e t r y C o n f i g 
 
 
 
 -   e n a b l e d :   d e f a u l t = T r u e     V%    t y p e = b o o l 
 
 -   m a x _ r e t r i e s :   d e f a u l t = 3     V%    t y p e = i n t 
 
 
 
 # # #   T r a n s p o r t C o n f i g 
 
 
 
 -   e n g i n e :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   h o s t :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   k e e p _ a l i v e _ r a w :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   w a r m _ m o d e l s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   c o n n e c t _ t i m e o u t _ s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   r e a d _ t i m e o u t _ s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 
 
 # # #   S t r e a m i n g C o n f i g 
 
 
 
 -   i d l e _ r e c o n n e c t _ s e c s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 
 
 # # #   S a m p l i n g C o n f i g 
 
 
 
 -   r e a s o n i n g :   d e f a u l t = ' h i g h '     V%    t y p e = s t r 
 
 -   r e a s o n i n g _ m o d e :   d e f a u l t = ' s y s t e m '     V%    t y p e = s t r 
 
 -   m a x _ o u t p u t _ t o k e n s :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ i n t ] 
 
 -   c t x _ s i z e :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ i n t ] 
 
 -   t e m p e r a t u r e :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ f l o a t ] 
 
 -   t o p _ p :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ f l o a t ] 
 
 -   p r e s e n c e _ p e n a l t y :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ f l o a t ] 
 
 -   f r e q u e n c y _ p e n a l t y :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ f l o a t ] 
 
 
 
 # # #   T o o l i n g C o n f i g 
 
 
 
 -   e n a b l e d :   d e f a u l t = T r u e     V%    t y p e = b o o l 
 
 -   p r i n t _ l i m i t :   d e f a u l t = 2 0 0 0     V%    t y p e = i n t 
 
 -   c o n t e x t _ c a p :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   m u l t i _ r o u n d :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   m a x _ r o u n d s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r e s u l t s _ f o r m a t :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 
 
 # # #   M e m 0 C o n f i g 
 
 
 
 -   e n a b l e d :   d e f a u l t = T r u e     V%    t y p e = b o o l 
 
 -   l o c a l :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   v e c t o r _ p r o v i d e r :   d e f a u l t = ' c h r o m a '     V%    t y p e = s t r 
 
 -   v e c t o r _ h o s t :   d e f a u l t = ' : m e m o r y : '     V%    t y p e = s t r 
 
 -   v e c t o r _ p o r t :   d e f a u l t = 0     V%    t y p e = i n t 
 
 -   o l l a m a _ u r l :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   l l m _ b a s e _ u r l :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   e m b e d d e r _ b a s e _ u r l :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   l l m _ m o d e l :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   e m b e d d e r _ m o d e l :   d e f a u l t = ' e m b e d d i n g g e m m a '     V%    t y p e = s t r 
 
 -   u s e r _ i d :   d e f a u l t = ' c l i - u s e r '     V%    t y p e = s t r 
 
 -   a g e n t _ i d :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   a p p _ i d :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   a p i _ k e y :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   o r g _ i d :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   p r o j e c t _ i d :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   d e b u g :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   m a x _ h i t s :   d e f a u l t = 1 0     V%    t y p e = i n t 
 
 -   s e a r c h _ t i m e o u t _ m s   ( m s ) :   d e f a u l t = 7 0 0     V%    t y p e = i n t 
 
 -   t i m e o u t _ c o n n e c t _ m s   ( m s ) :   d e f a u l t = 1 0 0 0     V%    t y p e = i n t 
 
 -   t i m e o u t _ r e a d _ m s   ( m s ) :   d e f a u l t = 2 0 0 0     V%    t y p e = i n t 
 
 -   a d d _ q u e u e _ m a x :   d e f a u l t = 2 5 6     V%    t y p e = i n t 
 
 -   b r e a k e r _ t h r e s h o l d :   d e f a u l t = 3     V%    t y p e = i n t 
 
 -   b r e a k e r _ c o o l d o w n _ m s   ( m s ) :   d e f a u l t = 6 0 0 0 0     V%    t y p e = i n t 
 
 -   s e a r c h _ w o r k e r s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   i n _ f i r s t _ s y s t e m :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   p r o x y _ m o d e l :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   p r o x y _ t i m e o u t _ m s   ( m s ) :   d e f a u l t = 1 2 0 0     V%    t y p e = i n t 
 
 -   r e r a n k _ s e a r c h _ l i m i t :   d e f a u l t = 1 0     V%    t y p e = i n t 
 
 -   c o n t e x t _ b u d g e t _ c h a r s :   d e f a u l t = 1 2 0 0     V%    t y p e = i n t 
 
 -   o u t p u t _ f o r m a t :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 
 
 # # #   R e l i a b i l i t y C o n f i g 
 
 
 
 -   g r o u n d :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   k :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ i n t ] 
 
 -   c i t e :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   c h e c k :   d e f a u l t = ' o f f '     V%    t y p e = s t r 
 
 -   c o n s e n s u s :   d e f a u l t = F a l s e     V%    t y p e = b o o l 
 
 -   e v a l _ c o r p u s :   d e f a u l t = N o n e     V%    t y p e = O p t i o n a l [ s t r ] 
 
 
 
 # # #   H i s t o r y C o n f i g 
 
 
 
 -   m a x _ h i s t o r y :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 
 
 # # #   W e b C o n f i g 
 
 
 
 -   u s e r _ a g e n t :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 -   t i m e o u t _ c o n n e c t :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   t i m e o u t _ r e a d :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   t i m e o u t _ w r i t e :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   r e t r y _ a t t e m p t s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r e t r y _ b a c k o f f _ b a s e :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   r e t r y _ b a c k o f f _ m a x :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   m a x _ c o n n e c t i o n s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   m a x _ k e e p a l i v e :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   p e r _ h o s t _ c o n c u r r e n c y :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   f o l l o w _ r e d i r e c t s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   h e a d _ g a t i n g _ e n a b l e d :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   m a x _ d o w n l o a d _ b y t e s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   a c c e p t _ h e a d e r _ o v e r r i d e :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 -   c l i e n t _ p o o l _ s i z e :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   c a c h e _ t t l _ s e c o n d s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r o b o t s _ t t l _ s e c o n d s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   m a x _ c r a w l _ d e l a y _ s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   b r a v e _ k e y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   t a v i l y _ k e y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   e x a _ k e y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   g o o g l e _ p s e _ c x :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   g o o g l e _ p s e _ k e y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   c o h e r e _ k e y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   v o y a g e _ k e y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   r e r a n k _ e n a b l e d :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   r e r a n k _ m o d e :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 -   r e r a n k _ t i m e o u t _ m s   ( m s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r e r a n k _ c a c h e _ t t l _ s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r e r a n k _ b r e a k e r _ t h r e s h o l d :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r e r a n k _ b r e a k e r _ c o o l d o w n _ m s   ( m s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r e r a n k _ p r o v i d e r s :   d e f a u l t = < f a c t o r y >     V%    t y p e = l i s t [ R e r a n k P r o v i d e r S p e c ] 
 
 -   r e s p e c t _ r o b o t s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   a l l o w _ b r o w s e r :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   e m e r g e n c y _ b o o t s t r a p :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   d e b u g _ m e t r i c s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   r a t e _ t o k e n s _ p e r _ h o s t :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   r a t e _ r e f i l l _ p e r _ s e c :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   r e s p e c t _ r e t r y _ a f t e r :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   s a n d b o x _ a l l o w :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   s a n d b o x _ a l l o w _ h t t p :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   s a n d b o x _ a l l o w _ p r o x i e s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   h t t p _ p r o x y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   h t t p s _ p r o x y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   a l l _ p r o x y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   n o _ p r o x y :   d e f a u l t = < f a c t o r y >     V%    t y p e = O p t i o n a l [ s t r ] 
 
 -   c a c h e _ r o o t :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 -   a r c h i v e _ e n a b l e d :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   a r c h i v e _ c h e c k _ m e m e n t o _ f i r s t :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   a r c h i v e _ r e t r y _ o n _ 4 2 9 :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   b r o w s e r _ m a x _ p a g e s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   b r o w s e r _ w a i t _ m s   ( m s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   b r o w s e r _ b l o c k _ r e s o u r c e s :   d e f a u l t = < f a c t o r y >     V%    t y p e = s t r 
 
 -   b r o w s e r _ s t e a l t h _ l i g h t :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   c l e a n _ w i k i _ e d i t _ a n c h o r s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   s i t e m a p _ e n a b l e d :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 -   s i t e m a p _ m a x _ u r l s :   d e f a u l t = < f a c t o r y >     V%    t y p e = i n t 
 
 -   s i t e m a p _ t i m e o u t _ s   ( s ) :   d e f a u l t = < f a c t o r y >     V%    t y p e = f l o a t 
 
 -   s i t e m a p _ i n c l u d e _ s u b s :   d e f a u l t = < f a c t o r y >     V%    t y p e = b o o l 
 
 
 
 



================================================================================
FILE: scripts/gen_config_reference.py
================================================================================

from __future__ import annotations

"""
Generate a Configuration Reference markdown from src/core/config.py dataclasses.

Usage:
  python -m scripts.gen_config_reference > CONFIG_REFERENCE.md

Notes:
- This inspects dataclasses and prints field names with default values.
- Units are inferred heuristically: *_ms -> milliseconds, *_s or *_secs -> seconds.
- Environment variable precedence and CLI overlays are handled by code; this output
  focuses on the configuration surface and defaults.
"""
import sys
import os
import importlib.util
from dataclasses import fields, is_dataclass, MISSING
from typing import Any, get_origin, get_args


def _load_config_module():
    """Load src/core/config.py without importing the whole src package."""
    here = os.path.dirname(os.path.abspath(__file__))
    cfg_path = os.path.abspath(os.path.join(here, "..", "src", "core", "config.py"))
    spec = importlib.util.spec_from_file_location("_cfg_model", cfg_path)
    if spec is None or spec.loader is None:
        raise RuntimeError(f"Cannot load config module from {cfg_path}")
    mod = importlib.util.module_from_spec(spec)
    sys.modules["_cfg_model"] = mod
    spec.loader.exec_module(mod)
    return mod


def _fmt_default(v: Any) -> str:
    try:
        if isinstance(v, str):
            return f"'{v}'"
        return str(v)
    except Exception:
        return "<unrepr>"


def _typename(tp: Any) -> str:
    try:
        origin = get_origin(tp)
        if origin is None:
            return getattr(tp, "__name__", str(tp))
        args = ", ".join(_typename(a) for a in get_args(tp))
        return f"{getattr(origin, '__name__', str(origin))}[{args}]"
    except Exception:
        return str(tp)


def _safe_factory_repr(factory: Any) -> str:
    if factory in (list, dict, set, tuple):
        try:
            return _fmt_default(factory())
        except Exception:
            return "<factory>"
    return "<factory>"


def _infer_units(name: str) -> str:
    n = name.lower()
    if n.endswith("_ms"):
        return " (ms)"
    if n.endswith("_s") or n.endswith("_secs") or n.endswith("_seconds"):
        return " (s)"
    return ""


def _dump_dataclass(title: str, dc: Any) -> str:
    out = [f"### {title}", ""]
    for f in fields(dc):
        units = _infer_units(f.name)
        typ = _typename(getattr(f, 'type', Any))
        # Detect if this field is (or contains) a nested dataclass type
        nested = False
        try:
            t = getattr(f, 'type', None)
            origin = get_origin(t)
            if origin is None and isinstance(t, type) and is_dataclass(t):
                nested = True
            else:
                for a in get_args(t) or ():
                    if isinstance(a, type) and is_dataclass(a):
                        nested = True
                        break
        except Exception:
            pass
        # default handling for dataclass fields
        if f.default is not MISSING:
            default_repr = _fmt_default(f.default)
        elif f.default_factory is not MISSING:
            default_repr = _safe_factory_repr(f.default_factory)
        else:
            default_repr = "<none>"
        suffix = " Â· nested" if nested else ""
        out.append(f"- {f.name}{units}: default={default_repr}  Â·  type={typ}{suffix}")
        # Optional help text via metadata={'help': '...'}
        try:
            help_txt = (f.metadata or {}).get('help') if hasattr(f, 'metadata') else None
            if help_txt:
                out.append(f"  â€¢ {help_txt}")
        except Exception:
            pass
    out.append("")
    return "\n".join(out)


def main() -> int:
    print("## Configuration Reference (auto-generated)\n")
    print("Run: `python -m scripts.gen_config_reference > CONFIG_REFERENCE.md`\n")
    # Load config dataclasses directly from file to avoid importing src/__init__.py
    mod = _load_config_module()
    # Top-level client
    print(_dump_dataclass("ClientRuntimeConfig", getattr(mod, "ClientRuntimeConfig")))
    sections = [
        ("RetryConfig", getattr(mod, "RetryConfig")),
        ("TransportConfig", getattr(mod, "TransportConfig")),
        ("StreamingConfig", getattr(mod, "StreamingConfig")),
        ("SamplingConfig", getattr(mod, "SamplingConfig")),
        ("ToolingConfig", getattr(mod, "ToolingConfig")),
        ("Mem0Config", getattr(mod, "Mem0Config")),
        ("ReliabilityConfig", getattr(mod, "ReliabilityConfig")),
        ("HistoryConfig", getattr(mod, "HistoryConfig")),
        ("WebConfig", getattr(mod, "WebConfig")),
    ]
    for name, dc in sections:
        print(_dump_dataclass(name, dc))

    # Operational guidance / notes (static prose)
    print("""
### Environment precedence and loading

- The CLI (`src/cli.py`) and API (`src/api/app.py`) load `.env.local` first (override=True), then `.env` (override=False).
- The web pipeline (`src/web/pipeline.py`) also loads `.env.local` then `.env` on import, so tests and direct imports see the same defaults.
- Programmatic configs set via `src.web.pipeline.set_default_config(cfg.web)` take precedence when callers supply a central `ClientRuntimeConfig`.

### Permissive profile (example .env.local)

```
WEB_RESPECT_ROBOTS=0
WEB_HEAD_GATING=0
SANDBOX_NET_ALLOW=*
WEB_UA=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36
WEB_DEBUG_METRICS=1
```

### Citation policy: exclude certain domains

- Set `WEB_EXCLUDE_CITATION_DOMAINS` to a comma-separated list (e.g., `wikipedia.org,reddit.com`).
- These domains will be used for discovery (search), but they will not be quoted as citations in `run_research()`.

### Wikipedia-guided expansion

- When Wikipedia results appear in search, the pipeline fetches the page, extracts external links from the content, and adds those links as candidates.
- Wikipedia hosts are excluded from citations when `WEB_EXCLUDE_CITATION_DOMAINS` includes `wikipedia.org`.
- Debug counters include `excluded` (filtered candidates) and `wiki_refs_added` (external references added from Wikipedia pages).
""")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())



================================================================================
FILE: src/plugins/web_research.py
================================================================================

from __future__ import annotations
import json
from typing import Optional
from ..web.pipeline import run_research

TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "web_research",
        "description": "Use for multi-hop web research that requires up-to-date facts and citations. Orchestrates Planâ†’Searchâ†’Fetch (robots.txt + crawl-delay enforced; per-host concurrency bounded)â†’Extract (HTML/PDF)â†’Chunkâ†’Rerankâ†’Cite (exact quotes; PDF page mapping)â†’Cache. Prefer this over raw web_fetch when you need sourcing and synthesis across multiple pages. Parameters: site_include/site_exclude narrow scope; top_k controls breadth; freshness_days limits recency; force_refresh=true bypasses caches. Returns compact JSON with results, citations, and archive URLs.",
        "parameters": {
            "type": "object",
            "additionalProperties": False,
            "properties": {
                "query": {"type": "string", "description": "User question or query to research."},
                "top_k": {"type": "integer", "minimum": 1, "maximum": 10, "default": 5},
                "site_include": {"type": "string", "description": "Optional site/domain to include (e.g., 'site:arxiv.org')."},
                "site_exclude": {"type": "string", "description": "Optional domain snippet to exclude."},
                "freshness_days": {"type": "integer", "minimum": 1, "maximum": 3650},
                "force_refresh": {"type": "boolean", "default": False}
            },
            "required": ["query"],
        },
    },
}


def web_research(query: str, top_k: int = 5, site_include: Optional[str] = None, site_exclude: Optional[str] = None, freshness_days: Optional[int] = None, force_refresh: bool = False) -> str:
    # Backward-compatible call shape (tests patch run_research without cfg)
    res = run_research(
        query,
        top_k=int(top_k or 5),
        site_include=site_include,
        site_exclude=site_exclude,
        freshness_days=freshness_days,
        force_refresh=bool(force_refresh),
    )
    # Return compact JSON for injection-safe integration
    return json.dumps(res, ensure_ascii=False)

TOOL_IMPLEMENTATION = web_research
TOOL_AUTHOR = "platform-core"
TOOL_VERSION = "1.0.0"



================================================================================
FILE: src/client.py
================================================================================

"""
Ollama Turbo client implementation with tool calling and streaming support.
Reliability hardening: retries/backoff, idempotency keys, keep-alive pools, and
streaming idle reconnects (all behind env flags with safe defaults).
"""

import json
import sys
import logging
import os
from typing import Dict, Any, List, Optional, Union, Tuple
from ollama import Client
import threading
import queue
import time
import atexit
import uuid
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout

from . import plugin_loader as _plugin_loader
from .utils import with_retry, RetryableError, OllamaAPIError, truncate_text, format_conversation_history
from .prompt_manager import PromptManager
from .harmony_processor import HarmonyProcessor
from .reliability_integration.integration import ReliabilityIntegration
from .protocols import get_adapter
from .transport import networking as _net
from .transport.http import TransportHttpClient
from .transport.policy import RetryPolicy
from .streaming import runner as _runner, standard as _standard
from .tools_runtime.executor import ToolRuntimeExecutor
from .memory.mem0 import Mem0Service
from .core.config import ClientRuntimeConfig
from .web.config import WebConfig  # re-exported wrapper around core WebConfig
from .web.pipeline import set_default_config as _web_set_default_config


class OllamaTurboClient:
    """Client for interacting with gpt-oss:120b via Ollama Turbo."""
    
    def __init__(self, api_key: str, model: str = "gpt-oss:120b", enable_tools: bool = True, show_trace: bool = False, reasoning: str = "high", quiet: bool = False, max_output_tokens: Optional[int] = None, ctx_size: Optional[int] = None, tool_print_limit: int = 200, multi_round_tools: bool = True, tool_max_rounds: Optional[int] = None, *, ground: bool = False, k: Optional[int] = None, cite: bool = False, check: str = 'off', consensus: bool = False, engine: Optional[str] = None, eval_corpus: Optional[str] = None, reasoning_mode: str = 'system', protocol: str = 'auto', temperature: Optional[float] = None, top_p: Optional[float] = None, presence_penalty: Optional[float] = None, frequency_penalty: Optional[float] = None, 
                 # Mem0 configuration
                 mem0_enabled: bool = True,
                 mem0_local: bool = False,
                 mem0_vector_provider: str = 'chroma',
                 mem0_vector_host: str = ':memory:',
                 mem0_vector_port: int = 0,
                 mem0_ollama_url: Optional[str] = None,
                 mem0_llm_model: Optional[str] = None,
                 mem0_embedder_model: str = 'embeddinggemma',
                 mem0_user_id: str = 'cli-user',
                 cfg: Optional[ClientRuntimeConfig] = None):
        """Initialize Ollama Turbo client.
        
        Args:
            api_key: Ollama API key for authentication
            model: Model name to use (default: gpt-oss:120b)
            enable_tools: Whether to enable tool calling capabilities
            show_trace: Whether to collect and print a separated reasoning trace
            reasoning: Reasoning effort directive ('low' | 'medium' | 'high')
            quiet: Reduce CLI noise (suppress helper prints)
            max_output_tokens: Limit on tokens to generate (maps to options.num_predict)
            ctx_size: Context window size (maps to options.num_ctx)
            tool_print_limit: CLI print truncation for tool outputs (characters)
        """
        self.api_key = api_key
        # Prefer centralized config when provided
        self._cfg: Optional[ClientRuntimeConfig] = cfg
        # Ensure web pipeline uses the same centralized WebConfig
        try:
            if cfg is not None and getattr(cfg, 'web', None):
                _web_set_default_config(cfg.web)
            else:
                # Fall back to environment-derived WebConfig
                _web_set_default_config(WebConfig())
        except Exception:
            # Never fail client init due to optional web config propagation
            pass
        if cfg is not None:
            self.model = cfg.model
            self.enable_tools = enable_tools
            self.show_trace = cfg.show_trace
            self.quiet = cfg.quiet
            self.reasoning = cfg.sampling.reasoning
            self.reasoning_mode = cfg.sampling.reasoning_mode
            self.max_output_tokens = cfg.sampling.max_output_tokens
            self.ctx_size = cfg.sampling.ctx_size
            self.temperature = cfg.sampling.temperature
            self.top_p = cfg.sampling.top_p
            self.presence_penalty = cfg.sampling.presence_penalty
            self.frequency_penalty = cfg.sampling.frequency_penalty
            self.tool_print_limit = cfg.tooling.print_limit
            self.tool_context_cap = cfg.tooling.context_cap
        else:
            self.model = model
            self.enable_tools = enable_tools
            self.show_trace = show_trace
            self.quiet = quiet
            self.reasoning = reasoning if reasoning in {"low", "medium", "high"} else "high"
            # How to send reasoning effort to provider: 'system' | 'request:top' | 'request:options'
            rm = str(reasoning_mode or 'system').strip().lower()
            self.reasoning_mode = rm if rm in {'system', 'request:top', 'request:options'} else 'system'
            self.max_output_tokens = max_output_tokens
            self.ctx_size = ctx_size
            # Sampling parameters (may be None; adapter- or model-specific defaults can be applied later)
            self.temperature: Optional[float] = temperature
            self.top_p: Optional[float] = top_p
            self.presence_penalty: Optional[float] = presence_penalty
            self.frequency_penalty: Optional[float] = frequency_penalty
        self.trace: List[str] = []
        self.logger = logging.getLogger(__name__)
        if cfg is None:
            self.tool_print_limit = tool_print_limit
            self.tool_context_cap = int(os.getenv('TOOL_CONTEXT_MAX_CHARS', '4000') or '4000')
        self._last_user_message: Optional[str] = None
        self._mem0_notice_shown: bool = False
        self._skip_mem0_after_turn: bool = False
        # Mem0 runtime flags/state (set in _init_mem0)
        self.mem0_enabled: bool = False
        self.mem0_debug: bool = False
        self.mem0_max_hits: int = 3
        self.mem0_search_timeout_ms: int = 200
        self.mem0_timeout_connect_ms: int = 1000
        self.mem0_timeout_read_ms: int = 2000
        self.mem0_add_queue_max: int = 256
        self._mem0_add_queue: Optional["queue.Queue"] = None
        self._mem0_worker: Optional[threading.Thread] = None
        self._mem0_worker_stop: threading.Event = threading.Event()
        self._mem0_last_sat_log: float = 0.0
        self._mem0_fail_count: int = 0
        self._mem0_breaker_threshold: int = 3
        self._mem0_breaker_cooldown_ms: int = 60000
        self._mem0_down_until_ms: int = 0
        self._mem0_breaker_tripped_logged: bool = False
        self._mem0_breaker_recovered_logged: bool = False
        self._last_mem_hash: Optional[str] = None
        self._mem0_search_workers = cfg.mem0.search_workers if cfg is not None else int(os.getenv('MEM0_SEARCH_WORKERS', '2') or '2')
        # Mem0 search timeout unified here (ms)
        if cfg is not None:
            try:
                self.mem0_search_timeout_ms = int(cfg.mem0.search_timeout_ms)
            except Exception:
                self.mem0_search_timeout_ms = 500
        else:
            try:
                self.mem0_search_timeout_ms: int = int(os.getenv('MEM0_SEARCH_TIMEOUT_MS', '800') or '800')
            except Exception:
                self.mem0_search_timeout_ms = 800
        # Tool-call iteration controls
        if cfg is not None:
            self.multi_round_tools = bool(cfg.tooling.multi_round)
            self.tool_max_rounds = max(1, int(cfg.tooling.max_rounds))
        else:
            env_mrt = os.getenv('MULTI_ROUND_TOOLS')
            if env_mrt is not None:
                self.multi_round_tools = env_mrt.strip().lower() in {'1', 'true', 'yes', 'on'}
            else:
                self.multi_round_tools = bool(multi_round_tools)
            try:
                default_rounds = tool_max_rounds if tool_max_rounds is not None else 6
                parsed_rounds = int(os.getenv('TOOL_MAX_ROUNDS', str(default_rounds)) or str(default_rounds))
                self.tool_max_rounds: int = max(1, parsed_rounds)
            except Exception:
                self.tool_max_rounds = max(1, tool_max_rounds if tool_max_rounds is not None else 6)
        self._mem0_search_pool: Optional[ThreadPoolExecutor] = None
        # CLI/network resilience knobs (env-controlled)
        if cfg is not None:
            self.cli_retry_enabled = bool(cfg.retry.enabled)
            self.cli_max_retries = int(cfg.retry.max_retries)
            self.cli_stream_idle_reconnect_secs = int(cfg.streaming.idle_reconnect_secs)
            self.cli_connect_timeout_s = float(cfg.transport.connect_timeout_s)
            self.cli_read_timeout_s = float(cfg.transport.read_timeout_s)
            self.warm_models = bool(cfg.transport.warm_models)
            self.ollama_keep_alive_raw = cfg.transport.keep_alive_raw
        else:
            self.cli_retry_enabled: bool = os.getenv('CLI_RETRY_ENABLED', 'true').strip().lower() != 'false'
            try:
                self.cli_max_retries: int = max(0, int(os.getenv('CLI_MAX_RETRIES', '3') or '3'))
            except Exception:
                self.cli_max_retries = 3
            try:
                self.cli_stream_idle_reconnect_secs: int = max(10, int(os.getenv('CLI_STREAM_IDLE_RECONNECT_SECS', '90') or '90'))
            except Exception:
                self.cli_stream_idle_reconnect_secs = 90
            try:
                self.cli_connect_timeout_s: float = max(1.0, float(os.getenv('CLI_CONNECT_TIMEOUT_S', '5') or '5'))
            except Exception:
                self.cli_connect_timeout_s = 5.0
            try:
                self.cli_read_timeout_s: float = max(60.0, float(os.getenv('CLI_READ_TIMEOUT_S', '600') or '600'))
            except Exception:
                self.cli_read_timeout_s = 600.0
            self.warm_models: bool = os.getenv('WARM_MODELS', 'true').strip().lower() not in {'0', 'false', 'no', 'off'}
            self.ollama_keep_alive_raw: Optional[str] = os.getenv('OLLAMA_KEEP_ALIVE')
        self._current_idempotency_key: Optional[str] = None
        # Tool results return format (for future API use). Default preserves v1 behavior (strings)
        if cfg is not None:
            self.tool_results_format = 'object' if (str(cfg.tooling.results_format).strip().lower() == 'object') else 'string'
        else:
            trf = (os.getenv('TOOL_RESULTS_FORMAT') or 'string').strip().lower()
            self.tool_results_format: str = 'object' if trf == 'object' else 'string'
        # Reliability mode configuration (no-op placeholders until wired)
        self.engine: Optional[str] = (cfg.transport.engine if cfg is not None else engine)
        self.reliability = {
            'ground': bool(ground),
            'k': k,
            'cite': bool(cite),
            'check': check if check in {'off', 'warn', 'enforce'} else 'off',
            'consensus': bool(consensus),
            'eval_corpus': eval_corpus,
        }
        # Split retrieval vs consensus k to avoid coupling
        try:
            rag_k_env = os.getenv('RAG_TOPK', '5')
            cons_k_env = os.getenv('CONSENSUS_K', '')
            rag_k_val = int(self.reliability.pop('k', None) or (rag_k_env if rag_k_env.isdigit() else 5))
        except Exception:
            rag_k_val = 5
        try:
            consensus_k_val = int(cons_k_env) if cons_k_env.isdigit() else None
        except Exception:
            consensus_k_val = None
        self.reliability.update({'rag_k': rag_k_val, 'consensus_k': consensus_k_val})
        # Reliability runtime state
        self._last_context_blocks: List[Dict[str, Any]] = []
        self._last_citations_map: Dict[str, Any] = {}
        self._system_cited_cache: Optional[str] = None

        # Initialize SDK client and wrap with transport policy (retries, idempotency, keep-alive)
        # Note: Ollama Turbo uses Authorization header without 'Bearer' prefix
        resolved_host = self._resolve_host(self.engine)
        self.host = resolved_host
        _sdk_client = Client(
            host=resolved_host,
            headers={'Authorization': api_key}
        )
        # Retry/backoff policy (transport owns retries now)
        rp = RetryPolicy(
            max_retries=(int(cfg.retry.max_retries) if (cfg is not None) else int(self.cli_max_retries)),
        )
        self.client = TransportHttpClient(
            _sdk_client,
            host=resolved_host,
            connect_timeout_s=float(self.cli_connect_timeout_s),
            read_timeout_s=float(self.cli_read_timeout_s),
            warm_models=bool(self.warm_models),
            keep_alive_raw=self.ollama_keep_alive_raw,
            retry_policy=rp,
            logger=self.logger,
            trace_hook=self._trace,
        )
        # Disable client-level retry wrappers to avoid double retrying; transport handles it
        self.cli_retry_enabled = False
        
        # Prompt management (centralized via cfg.prompt)
        try:
            if cfg is not None and getattr(cfg, 'prompt', None):
                self.prompt = PromptManager(
                    self.reasoning,
                    verbosity=str(cfg.prompt.verbosity or 'concise'),
                    verbose_after_tools=bool(cfg.prompt.verbose_after_tools),
                    fewshots=bool(cfg.prompt.fewshots),
                )
            else:
                self.prompt = PromptManager(self.reasoning)
        except Exception:
            self.prompt = PromptManager(self.reasoning)
        # Harmony parsing/markup processing
        self.harmony = HarmonyProcessor()
        # Reliability integration facade (Phase F)
        self.reliability_integration = ReliabilityIntegration()
        # Protocol adapter selection (default: auto -> harmony unless detected otherwise)
        try:
            if cfg is not None:
                self.protocol = str(cfg.protocol or 'auto').strip().lower()
            else:
                self.protocol = str(protocol or os.getenv('OLLAMA_PROTOCOL') or 'auto').strip().lower()
        except Exception:
            self.protocol = 'auto'
        self.adapter = get_adapter(model=self.model, protocol=self.protocol)
        # Apply DeepSeek-specific defaults and minimal system prompt
        try:
            adapter_name = getattr(self.adapter, 'name', '')
        except Exception:
            adapter_name = ''
        # Resolve DeepSeek defaults only if not provided explicitly
        if adapter_name == 'deepseek':
            def _env_float(name: str, default: float) -> float:
                try:
                    v = os.getenv(name)
                    return float(v) if v is not None and str(v).strip() != '' else default
                except Exception:
                    return default
            if self.temperature is None:
                self.temperature = _env_float('DEEPSEEK_TEMP', 0.6)
            if self.top_p is None:
                self.top_p = _env_float('DEEPSEEK_TOP_P', 0.95)
            if self.presence_penalty is None:
                self.presence_penalty = _env_float('DEEPSEEK_PRESENCE_PENALTY', 0.0)
            if self.frequency_penalty is None:
                self.frequency_penalty = _env_float('DEEPSEEK_FREQUENCY_PENALTY', 0.2)
            sys_prompt = self.prompt.deepseek_system_prompt()
        else:
            sys_prompt = self.prompt.initial_system_prompt()
        # Initialize conversation history with a system directive
        self.conversation_history = [
            {
                'role': 'system',
                'content': sys_prompt
            }
        ]
        # Enforce local history window <= 10 turns (excluding initial system)
        # Prefer centralized config for history window; fallback to env only when cfg is absent
        try:
            if self._cfg is not None and getattr(self._cfg, 'history', None):
                self.max_history = max(2, min(int(self._cfg.history.max_history), 10))
            else:
                raw_hist = os.getenv('MAX_CONVERSATION_HISTORY', '10')
                parsed_hist = int(raw_hist) if str(raw_hist).isdigit() else 10
                self.max_history = max(2, min(parsed_hist, 10))
        except Exception:
            self.max_history = 10
        
        # Set up tools if enabled (use copies to avoid global mutation leaks).
        # Access plugin aggregates lazily to avoid import-time plugin loading.
        if enable_tools:
            schemas = _plugin_loader.TOOL_SCHEMAS  # triggers load only now
            funcs = _plugin_loader.TOOL_FUNCTIONS
            self.tools = list(schemas)
            self.tool_functions = dict(funcs)
        else:
            self.tools = []
            self.tool_functions = {}
        
        # Mem0 configuration
        # Default: when using local Mem0 and no explicit Mem0 Ollama URL is provided,
        # point the embedder at local Ollama by default.
        mem0_ollama_default = (cfg.mem0.ollama_url if cfg is not None else (os.getenv('MEM0_OLLAMA_URL') or mem0_ollama_url))
        if mem0_local and not mem0_ollama_default:
            mem0_ollama_default = 'http://localhost:11434'
        self.mem0_config = {
            'enabled': (cfg.mem0.enabled if cfg is not None else mem0_enabled),
            'local': (cfg.mem0.local if cfg is not None else mem0_local),
            'vector_provider': (cfg.mem0.vector_provider if cfg is not None else mem0_vector_provider),
            'vector_host': (cfg.mem0.vector_host if cfg is not None else mem0_vector_host),
            'vector_port': (cfg.mem0.vector_port if cfg is not None else mem0_vector_port),
            # Base URLs (compat + explicit)
            'ollama_url': (mem0_ollama_default or self.host),
            'llm_base_url': (cfg.mem0.llm_base_url if cfg is not None else None),
            'embedder_base_url': (cfg.mem0.embedder_base_url if cfg is not None else None),
            # Models
            'llm_model': (cfg.mem0.llm_model if (cfg is not None and cfg.mem0.llm_model) else (mem0_llm_model or self.model)),
            'embedder_model': ((cfg.mem0.embedder_model if cfg is not None else mem0_embedder_model) or ('embeddinggemma' if (cfg.mem0.local if cfg is not None else mem0_local) else (mem0_embedder_model))),
            # Identity / auth
            'user_id': (cfg.mem0.user_id if cfg is not None else mem0_user_id),
            'agent_id': (cfg.mem0.agent_id if cfg is not None else None),
            'app_id': (cfg.mem0.app_id if cfg is not None else None),
            'api_key': (cfg.mem0.api_key if cfg is not None else None),
            'org_id': (cfg.mem0.org_id if cfg is not None else None),
            'project_id': (cfg.mem0.project_id if cfg is not None else None),
            # Runtime knobs
            'debug': (cfg.mem0.debug if cfg is not None else False),
            'max_hits': (cfg.mem0.max_hits if cfg is not None else 3),
            'timeout_connect_ms': (cfg.mem0.timeout_connect_ms if cfg is not None else 1000),
            'timeout_read_ms': (cfg.mem0.timeout_read_ms if cfg is not None else 2000),
            'add_queue_max': (cfg.mem0.add_queue_max if cfg is not None else 256),
            'breaker_threshold': (cfg.mem0.breaker_threshold if cfg is not None else 3),
            'breaker_cooldown_ms': (cfg.mem0.breaker_cooldown_ms if cfg is not None else 60000),
            'in_first_system': (
                cfg.mem0.in_first_system if cfg is not None else (
                    (os.getenv('MEM0_IN_FIRST_SYSTEM') or '').strip().lower() in {'1','true','yes','on'}
                )
            ),
            # Output format for mem0 client (centralized)
            'output_format': (cfg.mem0.output_format if cfg is not None else (os.getenv('MEM0_OUTPUT_FORMAT') or 'v1.1')),
            # Proxy / reranker
            'proxy_model': (cfg.mem0.proxy_model if cfg is not None else (os.getenv('MEM0_PROXY_MODEL') or None)),
            'proxy_timeout_ms': (cfg.mem0.proxy_timeout_ms if cfg is not None else 1200),
            'rerank_search_limit': (cfg.mem0.rerank_search_limit if cfg is not None else 10),
        }

        # Initialize Mem0 memory system (optional)
        self.mem0_service = Mem0Service(self.mem0_config)
        self._init_mem0()

        # Reasoning injection config (centralized)
        try:
            default_field_path = 'options.reasoning_effort' if (self.reasoning_mode == 'request:options') else 'reasoning'
            if cfg is not None and getattr(cfg, 'reasoning_injection', None):
                inj = cfg.reasoning_injection
                self.reasoning_field_path = (inj.field_path.strip() or default_field_path)
                self.reasoning_field_style = (inj.field_style or 'string').strip().lower()
                self.reasoning_object_key = (inj.object_key or 'effort').strip()
            else:
                # Defaults when no cfg provided â€” honor env overrides
                try:
                    rf_env = os.getenv('REASONING_FIELD_PATH')
                    rs_env = os.getenv('REASONING_FIELD_STYLE')
                    ro_env = os.getenv('REASONING_OBJECT_KEY')
                except Exception:
                    rf_env = rs_env = ro_env = None
                self.reasoning_field_path = (rf_env.strip() if rf_env else default_field_path)
                self.reasoning_field_style = ((rs_env or 'string').strip().lower())
                self.reasoning_object_key = ((ro_env or 'effort').strip())
        except Exception:
            self.reasoning_field_path = 'reasoning'
            self.reasoning_field_style = 'string'
            self.reasoning_object_key = 'effort'

        self.logger.info(f"Initialized client with model: {self.model}, host: {self.host}, tools enabled: {enable_tools}, reasoning={self.reasoning}, mode={self.reasoning_mode}, quiet={self.quiet}")
        # Initial trace state
        if self.show_trace:
            self.trace.append(f"client:init model={self.model} host={self.host} tools={'on' if enable_tools else 'off'} reasoning={self.reasoning} mode={self.reasoning_mode} quiet={'on' if self.quiet else 'off'}")

        # Centralize WebConfig for the web pipeline (set once per client)
        try:
            if cfg is not None and getattr(cfg, 'web', None):
                from .web import pipeline as _web_pipeline
                _web_pipeline.set_default_config(cfg.web)
        except Exception:
            pass

    # ---------- Reasoning Injection Helpers ----------
    def _nested_set(self, d: Dict[str, Any], path: str, value: Any) -> None:
        try:
            parts = [p for p in str(path).split('.') if p]
            cur = d
            for p in parts[:-1]:
                if p not in cur or not isinstance(cur[p], dict):
                    cur[p] = {}
                cur = cur[p]
            cur[parts[-1]] = value
        except Exception:
            # Do not fail request due to optional reasoning injection
            pass

    def _maybe_inject_reasoning(self, kwargs: Dict[str, Any]) -> None:
        """Optionally inject request-level reasoning effort based on configuration.

        Controlled by self.reasoning_mode. Field name and style are env-configurable:
        - REASONING_FIELD_PATH: dot-path for target (default depends on mode)
        - REASONING_FIELD_STYLE: 'string' (default) or 'object'
        - REASONING_OBJECT_KEY: key name when style is 'object' (default: 'effort')
        """
        try:
            if self.reasoning_mode == 'system':
                return
            # Resolve from centralized config prepared at init
            field_path = getattr(self, 'reasoning_field_path', None) or ('options.reasoning_effort' if self.reasoning_mode == 'request:options' else 'reasoning')
            style = (getattr(self, 'reasoning_field_style', 'string') or 'string').strip().lower()
            obj_key = (getattr(self, 'reasoning_object_key', 'effort') or 'effort').strip()

            if style == 'object':
                value: Any = {obj_key: self.reasoning}
            else:
                value = self.reasoning

            self._nested_set(kwargs, field_path, value)
            self._trace(f"reasoning:inject path={field_path} style={style} val={self.reasoning}")
        except Exception:
            # Never raise on optional reasoning injection
            pass

    def _prepare_initial_messages_for_adapter(self, *, include_tools: bool, adapter_opts: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """Prepare initial-turn messages via adapter, merging Mem0 into first system if needed.

        Behavior
        - If Mem0 is already merged into the first system message, pass through.
        - If Mem0 exists as a separate system block and MEM0_IN_FIRST_SYSTEM is enabled,
          remove the separate block and pass it to the adapter as ``mem0_block`` so the
          adapter can merge it into the first system message deterministically.
        - If MEM0_IN_FIRST_SYSTEM is disabled, the separate Mem0 block remains as-is,
          and no merge hint (``mem0_block``) is provided to the adapter.
        - Returns (normalized_messages, payload_overrides) where overrides may include
          provider-specific options and tools.

        Tracing
        - Emits ``mem0:inject:first:adapter`` when a Mem0 block is merged into the first
          system message by this method (merge enabled and a Mem0 block was present).
        """
        try:
            msgs: List[Dict[str, Any]] = list(self.conversation_history or [])
            # Determine Mem0 prefixes
            try:
                prefixes = self.prompt.mem0_prefixes()
            except Exception:
                prefixes = [
                    "Previous context from user history (use if relevant):",
                    "Relevant information:",
                    "Relevant user memories",
                ]
            # Check if first system already contains Mem0
            mem0_in_first = False
            if msgs and (msgs[0] or {}).get('role') == 'system':
                first_c = str((msgs[0] or {}).get('content') or '')
                mem0_in_first = any((p and (p in first_c)) for p in prefixes)

            # Detect Mem0 block if present (latest occurrence)
            mem0_block: Optional[str] = None
            latest_idx: Optional[int] = None
            for i in range(len(msgs) - 1, -1, -1):
                m = msgs[i]
                if m.get('role') == 'system':
                    c = str(m.get('content') or '')
                    if any(c.startswith(p) for p in prefixes):
                        latest_idx = i
                        mem0_block = c
                        break
            # Respect env flag: only remove and merge when explicitly enabled
            # Centralized control: prefer cfg-provided switch; fall back to env only when cfg missing
            try:
                prefer_merge = bool(self.mem0_config.get('in_first_system', False))
            except Exception:
                prefer_merge = str(os.getenv('MEM0_IN_FIRST_SYSTEM', '0')).strip().lower() in {'1', 'true', 'yes', 'on'}
            if prefer_merge and (not mem0_in_first) and (latest_idx is not None and mem0_block):
                # Remove that block so the adapter can merge into first system
                msgs = msgs[:latest_idx] + msgs[latest_idx + 1:]
                # Mirror the merge in our persistent history to keep tests and callers consistent
                try:
                    if self.conversation_history and (self.conversation_history[0] or {}).get('role') == 'system':
                        base0 = str((self.conversation_history[0] or {}).get('content') or '').rstrip()
                        merged0 = (base0 + "\n\n" + mem0_block).strip()
                        self.conversation_history[0]['content'] = merged0
                        # Remove the separate Mem0 block from history at the same index
                        if 0 <= int(latest_idx) < len(self.conversation_history):
                            del self.conversation_history[int(latest_idx)]
                        # Trace adapter-side merge for visibility
                        self._trace("mem0:inject:first:adapter")
                except Exception:
                    pass

            # Delegate to adapter for initial formatting. Only pass mem0_block
            # when we intentionally removed it to merge into the first system.
            mem0_for_adapter = mem0_block if (prefer_merge and (not mem0_in_first) and (latest_idx is not None and mem0_block)) else None
            norm_msgs, overrides = self.adapter.format_initial_messages(
                messages=msgs,
                tools=(self.tools if (include_tools and bool(self.tools)) else None),
                options=(adapter_opts or None),
                mem0_block=mem0_for_adapter,
            )
            return norm_msgs, (overrides or {})
        except Exception:
            # Fallback: pass-through and attempt minimal option/tool mapping
            out_msgs = list(self.conversation_history or [])
            overrides: Dict[str, Any] = {}
            try:
                mapped = self.adapter.map_options(adapter_opts) if adapter_opts else {}
                if mapped:
                    overrides['options'] = mapped
            except Exception:
                # Secondary fallback to Ollama-compatible fields
                opts: Dict[str, Any] = {}
                if self.max_output_tokens is not None:
                    opts['num_predict'] = self.max_output_tokens
                if self.ctx_size is not None:
                    opts['num_ctx'] = self.ctx_size
                if opts:
                    overrides['options'] = opts
            if include_tools and self.tools:
                overrides['tools'] = self.tools
            return out_msgs, overrides

    def _trace(self, event: str):
        """Record a structured, non-sensitive trace event."""
        if self.show_trace:
            self.trace.append(event)

    def _print_trace(self):
        """Print a separated reasoning trace section."""
        if self.show_trace and self.trace:
            # Send to stderr to avoid mixing with streamed stdout
            print("\n\n--- Reasoning Trace ---", file=sys.stderr, flush=True)
            for item in self.trace:
                print(f" â€¢ {item}", file=sys.stderr, flush=True)
            # Do NOT clear here; chat() resets at the start of each turn.
            # self.trace = []
    
    def _trace_mem0_presence(self, messages: Optional[List[Dict[str, Any]]], where: str) -> None:
        """Record whether Mem0 context is present in the first system message or as a separate system block.

        The trace is non-sensitive and only records booleans/counts. Used to verify that providers that
        only honor the first system message still receive Mem0 context when enabled.

        Canonical Mem0 trace keys (for tests and diagnostics)
        - ``mem0:present:{where} first={0|1} blocks={N}``
          Emitted by this method to indicate whether the first system contains Mem0 (``first``)
          and how many Mem0 system blocks are present in the message list (``blocks``).
          Typical ``where`` values include ``standard:r0``, ``stream:init``, and ``stream:r0``.

        - ``mem0:inject:first:adapter``
          Emitted by ``_prepare_initial_messages_for_adapter`` when a Mem0 block is merged into the
          first system message (i.e., MEM0_IN_FIRST_SYSTEM enabled and a separate Mem0 block existed).

        - ``mem0:search:hits={N}``
          Emitted by the Mem0 service after a search to record how many related memories were found
          before constructing the final Mem0 context block.
        """
        if not self.show_trace:
            return
        try:
            msgs = messages or []
            prefixes: List[str] = []
            try:
                prefixes = self.prompt.mem0_prefixes()
            except Exception:
                prefixes = [
                    "Previous context from user history (use if relevant):",
                    "Relevant information:",
                    "Relevant user memories",
                ]
            in_first = False
            if msgs and (msgs[0] or {}).get('role') == 'system':
                first_c = str((msgs[0] or {}).get('content') or '')
                in_first = any((p and (p in first_c)) for p in prefixes)
            blocks = 0
            for m in msgs:
                if m.get('role') == 'system':
                    c = str(m.get('content') or '')
                    if any((p and (p in c)) for p in prefixes):
                        blocks += 1
            self._trace(f"mem0:present:{where} first={'1' if in_first else '0'} blocks={blocks}")
        except Exception:
            # Never fail request due to tracing
            pass
    
    def chat(self, message: str, stream: bool = False) -> str:
        """Send a message to the model and get a response.
        
        Args:
            message: User message to send
            stream: Whether to stream the response
            
        Returns:
            Model response as string
        """
        # Reset trace for this turn (only when tracing is enabled)
        if self.show_trace:
            self.trace = []
        self._skip_mem0_after_turn = False
        # Generate a stable Idempotency-Key for this turn; transport will reuse
        # it across any internal retries/reconnections. Downstream call sites
        # (standard/streaming) will pass this in kwargs.
        try:
            self._current_idempotency_key = str(uuid.uuid4())
            self._trace(f"idempotency:set {self._current_idempotency_key}")
        except Exception:
            self._current_idempotency_key = None
        self._trace(f"chat:start stream={'on' if stream else 'off'}")

        # Inject relevant memories BEFORE user message (one system block per turn)
        self._inject_mem0_context(message)

        # Add user message to history
        self.conversation_history.append({
            'role': 'user',
            'content': message
        })
        # Track last user message for Mem0 capture
        self._last_user_message = message
        # Reliability: clear per-request state to avoid cross-call bleed
        self._last_context_blocks = []
        self._last_citations_map = {}
        # Reliability: optional retrieval/grounding/citation system additions
        if self.reliability.get('ground'):
            try:
                self._prepare_reliability_context(message)
            except Exception as e:
                self.logger.debug(f"reliability:context skipped: {e}")
        
        # Trim history if needed
        self._trim_history()
        
        try:
            if stream:
                result = self._handle_streaming_chat()
            else:
                result = self._handle_standard_chat()
            # Print separated trace after output
            self._print_trace()
            return result
        except Exception as e:
            self.logger.error(f"Chat error: {e}")
            error_msg = f"Error during chat: {str(e)}"
            self.conversation_history.append({
                'role': 'assistant',
                'content': error_msg
            })
            self._trace(f"chat:error {type(e).__name__}")
            self._print_trace()
            return error_msg
        finally:
            pass
    
    def _handle_standard_chat(self, *, _suppress_errors: bool = False) -> str:
        """Handle non-streaming chat interaction (delegated) with dynamic retries."""
        if not self.cli_retry_enabled:
            return _standard.handle_standard_chat(self, _suppress_errors=_suppress_errors)
        @with_retry(max_retries=self.cli_max_retries)
        def _call():
            return _standard.handle_standard_chat(self, _suppress_errors=_suppress_errors)
        return _call()
    
    def _handle_streaming_chat(self) -> str:
        """Handle streaming chat interaction with tool support."""
        try:
            init_stream = self._create_streaming_response()
        except Exception as e:
            # Silent fallback: do not surface errors to CLI output
            self.logger.debug(f"Streaming init failed; falling back to non-streaming: {e}")
            self._trace("stream:init:error -> fallback")
            try:
                final = self._handle_standard_chat(_suppress_errors=True)
                # Print final response so the user sees output even when streaming init fails
                if final and not str(final).startswith("Error during chat:") and not self.quiet:
                    print(final)
                self._trace("stream:init:fallback:success")
                return final
            except Exception as e2:
                # Still suppress to avoid leaking error text in streaming mode
                self.logger.debug(f"Non-streaming fallback also failed: {e2}")
                self._trace("stream:init:fallback:error")
                return ""
        return self.handle_streaming_response(
            init_stream,
            tools_enabled=self.enable_tools
        )
    
    def _create_streaming_response(self):
        """Create a streaming response from the API with dynamic retries."""
        if not self.cli_retry_enabled:
            return _runner.create_streaming_response(self)
        @with_retry(max_retries=self.cli_max_retries)
        def _call():
            return _runner.create_streaming_response(self)
        return _call()
    
    def handle_streaming_response(self, response_stream, tools_enabled: bool = True) -> str:
        """Complete streaming response handler with tool call support (delegated)."""
        return _runner.handle_streaming_response(self, response_stream, tools_enabled=tools_enabled)

    # ------------------ Harmony parsing helpers ------------------
    def _strip_harmony_markup(self, text: str) -> str:
        """Remove Harmony channel/control tokens from text, preserving natural content.

        This strips tokens like <|channel|>commentary, <|channel|>final, <|message|>, <|call|>, <|end|>.
        """
        try:
            return self.harmony.strip_markup(text)
        except Exception:
            return text

    def _parse_harmony_tokens(self, text: str):
        """Parse Harmony tool-call and final-channel tokens from text.

        Returns (cleaned_text, tool_calls, final_text)
        - cleaned_text: input with tool-call segments removed and markup stripped
        - tool_calls: list of OpenAI-style tool_call dicts
        - final_text: last final-channel message content if present
        """
        return self.harmony.parse_tokens(text)

    # ------------------ Internal helpers ------------------
    def _set_idempotency_key(self, key: Optional[str]) -> None:
        """Set Idempotency-Key header on both clients for this turn."""
        try:
            _net.set_idempotency_key(self.client, key, trace_hook=self._trace)
        except Exception:
            # Never fail request due to optional header injection
            pass

    def _clear_idempotency_key(self) -> None:
        """Remove Idempotency-Key header after request completion."""
        try:
            _net.clear_idempotency_key(getattr(self, 'client', None))
            self._current_idempotency_key = None
        except Exception:
            # Never fail cleanup
            pass

    def _resolve_host(self, engine: Optional[str]) -> str:
        """Resolve the Ollama host to use based on engine flag or env.

        Priority:
        1) Explicit --engine flag
           - 'cloud' -> https://ollama.com
           - 'local' -> http://localhost:11434
           - Full URL (http/https) -> use as-is
           - Bare hostname -> prefix with https://
        2) OLLAMA_HOST env var
        3) Default https://ollama.com
        """
        return _net.resolve_host(engine)

    def _resolve_keep_alive(self) -> Optional[Union[float, str]]:
        """Resolve a valid keep_alive value or None.
        Accepts env `OLLAMA_KEEP_ALIVE` as:
        - duration string with units, e.g., '10m', '1h', '30s'
        - numeric seconds (int/float), converted to '<seconds>s'
        If unset and warming is enabled, defaults to '10m'.
        """
        try:
            return _net.resolve_keep_alive(
                warm_models=bool(getattr(self, 'warm_models', True)),
                host=getattr(self, 'host', None),
                keep_alive_raw=getattr(self, 'ollama_keep_alive_raw', None),
                logger=getattr(self, 'logger', None),
            )
        except Exception:
            # match docstring + avoid cold starts when warming is on
            return '10m' if bool(getattr(self, 'warm_models', True)) else None
    
    def _execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict[str, Any]]:
        """Execute tool calls and return results (delegated to ToolRuntimeExecutor)."""
        return ToolRuntimeExecutor.execute(self, tool_calls)
    
    def _serialize_tool_result_to_string(self, tr: Dict[str, Any]) -> str:
        """Serialize a structured tool result to a safe string (delegated)."""
        return ToolRuntimeExecutor.serialize_to_string(self, tr)
    
    def _payload_for_tools(self, tool_results: List[Dict[str, Any]], tool_calls: List[Dict[str, Any]]):
        """
        Returns a tuple (payload_for_adapter, prebuilt_tool_messages_or_None) based on self.tool_results_format.
        - If 'object': (tool_results, None) â€” adapters receive list[dict].
        - If 'string': (tool_strings, prebuilt_msgs) â€” adapters receive list[str]; fallback always uses strings.
          prebuilt_msgs is a list of {'role': 'tool', 'tool_call_id': <matching id if available>, 'content': <string>}
          one per tool call/result (mapped by position when present).
        Also updates:
          - self._last_tool_results_structured
          - self._last_tool_results_strings
        """
        tool_strings = [self._serialize_tool_result_to_string(tr) for tr in tool_results]
        # Bookkeep both views for diagnostics/tests
        try:
            self._last_tool_results_structured = tool_results
            self._last_tool_results_strings = tool_strings
        except Exception:
            pass
        fmt = getattr(self, 'tool_results_format', 'string')
        if fmt == 'object':
            return tool_results, None
        # Build per-call tool messages with tool_call_id mapping by position
        prebuilt_msgs: List[Dict[str, Any]] = []
        for i, s in enumerate(tool_strings):
            tc_id = None
            try:
                if i < len(tool_calls):
                    tc = tool_calls[i] or {}
                    raw = tc.get('id') or ((tc.get('function') or {}).get('id'))
                    # Coerce to string for strict adapters; treat empty string as None
                    tc_id = (str(raw).strip() or None) if raw is not None else None
            except Exception:
                tc_id = None
            msg: Dict[str, Any] = {'role': 'tool', 'content': s}
            if tc_id:
                msg['tool_call_id'] = tc_id
            prebuilt_msgs.append(msg)
        return tool_strings, prebuilt_msgs
    
    def _trim_history(self):
        """Trim conversation history while preserving key system blocks.

        Guarantees:
        - Preserve the very first system directive (if present)
        - Preserve the latest Mem0 "Relevant information:" system message (if present)
        - Preserve the last N messages (N = self.max_history, capped at 10)
        """
        if len(self.conversation_history) <= self.max_history:
            return

        first_system = self.conversation_history[0] if self.conversation_history and self.conversation_history[0].get('role') == 'system' else None
        # Find latest memory system block
        latest_mem_idx = None
        for i in range(len(self.conversation_history) - 1, -1, -1):
            msg = self.conversation_history[i]
            if msg.get('role') == 'system':
                c = msg.get('content') or ''
                if (
                    c.startswith("Previous context from user history (use if relevant):")
                    or c.startswith("Relevant information:")
                    or c.startswith("Relevant user memories")
                ):
                    latest_mem_idx = i
                    break

        last_N = self.conversation_history[-self.max_history:]
        new_hist: List[Dict[str, Any]] = []
        if first_system is not None:
            new_hist.append(first_system)
        if latest_mem_idx is not None and (latest_mem_idx < len(self.conversation_history)):
            mem_msg = self.conversation_history[latest_mem_idx]
            # Avoid duplication if already included in last_N or identical to first_system
            if mem_msg is not first_system and mem_msg not in last_N:
                new_hist.append(mem_msg)
        # Extend with last N (may include the memory block already)
        new_hist.extend(last_N)
        # De-duplicate while preserving order. For Mem0 system blocks, dedupe by content.
        seen = set()
        deduped: List[Dict[str, Any]] = []
        # Prepare optional Mem0 prefixes
        mem0_prefixes: List[str] = []
        try:
            mem0_prefixes = self.prompt.mem0_prefixes()
        except Exception:
            mem0_prefixes = [
                "Previous context from user history (use if relevant):",
                "Relevant information:",
                "Relevant user memories",
            ]
        def _is_mem0_block(msg: Dict[str, Any]) -> bool:
            try:
                if msg.get('role') != 'system':
                    return False
                c = str(msg.get('content') or '')
                return any(c.startswith(p) for p in mem0_prefixes if p)
            except Exception:
                return False
        for m in new_hist:
            try:
                if _is_mem0_block(m):
                    c = str(m.get('content') or '')
                    key = ("mem0_block", hash(c))
                else:
                    key = ("id", id(m))
            except Exception:
                key = ("id", id(m))
            if key in seen:
                continue
            seen.add(key)
            deduped.append(m)
        self.conversation_history = deduped
    
    def clear_history(self):
        """Clear conversation history."""
        # Preserve the system directive when clearing, respecting adapter-specific defaults
        try:
            adapter_name = getattr(self.adapter, 'name', '')
        except Exception:
            adapter_name = ''
        sys_prompt = self.prompt.deepseek_system_prompt() if adapter_name == 'deepseek' else self.prompt.initial_system_prompt()
        self.conversation_history = [
            {
                'role': 'system',
                'content': sys_prompt
            }
        ]
        self.logger.info("Conversation history cleared")
    
    def get_history(self) -> str:
        """Get formatted conversation history.
        
        Returns:
            Formatted conversation history string
        """
        return format_conversation_history(self.conversation_history)
    
    # ------------------ Reliability integration helpers ------------------
    def _load_system_cited(self) -> str:
        """Delegated to ReliabilityIntegration (Phase F)."""
        return self.reliability_integration.load_system_cited(self)

    def _prepare_reliability_context(self, user_message: str) -> None:
        """Delegated to ReliabilityIntegration (Phase F)."""
        return self.reliability_integration.prepare_context(self, user_message)
    
    # ------------------ Mem0 Integration ------------------
    def _init_mem0(self) -> None:
        """Initialize Mem0 client and runtime settings from configuration (delegated)."""
        return self.mem0_service.initialize(self)

    def _inject_mem0_context(self, user_message: str) -> None:
        """Search Mem0 for relevant memories and inject as a system message (delegated)."""
        return self.mem0_service.inject_context(self, user_message)

    def _mem0_llm_generate(self, *, model: str, system: str, user: str) -> str:
        """Low-level reranker generation (delegated)."""
        return self.mem0_service.llm_generate(self, model=model, system=system, user=user)

    def _mem0_rerank_with_proxy(self, query: str, candidates: List[str], *, model: str, k: Optional[int] = None) -> List[int]:
        """Rerank candidate memory snippets for the current query (delegated)."""
        return self.mem0_service.rerank_with_proxy(self, query, candidates, model=model, k=k)

    def _mem0_add_after_response(self, user_message: Optional[str], assistant_message: Optional[str]) -> None:
        """Queue the interaction to Mem0 for persistence (delegated)."""
        return self.mem0_service.persist_turn(self, user_message, assistant_message)

    def _mem0_enqueue_add(self, messages: List[Dict[str, str]], metadata: Dict[str, Any]) -> None:
        """Enqueue an add job (delegated)."""
        return self.mem0_service.enqueue_add(self, messages, metadata)

    def _mem0_worker_loop(self) -> None:
        return self.mem0_service.worker_loop(self)

    def _mem0_execute_add(self, messages: List[Dict[str, str]], metadata: Dict[str, Any]) -> List[str]:
        return self.mem0_service.execute_add(self, messages, metadata)

    def _mem0_enforce_metadata(self, ids: List[str], metadata: Dict[str, Any]) -> None:
        return self.mem0_service.enforce_metadata(self, ids, metadata)

    def _mem0_search_api(self, query: str, filters: Optional[Dict[str, Any]] = None, limit: Optional[int] = None):
        """Search wrapper (delegated)."""
        return self.mem0_service.search_api(self, query, filters=filters, limit=limit)

    def _mem0_get_all_api(self, filters: Optional[Dict[str, Any]] = None):
        """get_all wrapper (delegated)."""
        return self.mem0_service.get_all_api(self, filters=filters)

    def _normalize_fact(self, text: str) -> str:
        t = ' '.join(text.strip().split())
        return t.lower()
    
    
    def interactive_mode(self):
        """Run interactive chat mode."""
        if not self.quiet:
            print("ðŸš€ Ollama Turbo CLI - Interactive Mode")
            print(f"ðŸ“ Model: {self.model}")
            print(f"ðŸ”§ Tools: {'Enabled' if self.enable_tools else 'Disabled'}")
            print("ðŸ’¡ Commands: 'quit'/'exit' to exit, 'clear' to clear history, 'history' to show history, '/mem ...' for memory ops")
            if not getattr(self, 'mem0_enabled', False):
                print("Mem0: disabled (set MEM0_USE_LOCAL=1 for local OSS or provide MEM0_API_KEY for remote)")
            else:
                mode = getattr(self, 'mem0_mode', 'unknown')
                print(f"Mem0: enabled ({mode}, user: {self.mem0_user_id})")
            print("-" * 60)
        
        while True:
            try:
                # Get user input
                user_input = input("\nðŸ‘¤ You: ").strip()
                
                if not user_input:
                    continue
                
                # Handle /mem commands
                if user_input.lower().startswith('/mem'):
                    self.mem0_service.handle_command(self, user_input)
                    continue

                # Natural language memory handlers
                if self.mem0_service.handle_nlu(self, user_input):
                    continue

                # Handle commands
                if user_input.lower() in ['quit', 'exit']:
                    if not self.quiet:
                        print("ðŸ‘‹ Goodbye!")
                    break
                elif user_input.lower() == 'clear':
                    self.clear_history()
                    if not self.quiet:
                        print("âœ… History cleared")
                    continue
                elif user_input.lower() == 'history':
                    if not self.quiet:
                        print("\nðŸ“œ Conversation History:")
                        print(self.get_history())
                    continue
                
                # Send message to model
                if not self.quiet:
                    print()  # Empty line for better formatting
                response = self.chat(user_input, stream=True)
                
                # Response is already printed during streaming
                
            except KeyboardInterrupt:
                if not self.quiet:
                    print("\n\nâš ï¸ Use 'quit' or 'exit' to leave the chat")
                continue
            except Exception as e:
                self.logger.error(f"Interactive mode error: {e}")
                if not self.quiet:
                    print(f"\nâŒ Error: {str(e)}")
                continue

    # ----- Mem0 shutdown -----
    def _mem0_shutdown(self) -> None:
        """Legacy shutdown hook. Mem0Service owns flushing and atexit now."""
        try:
            self.mem0_service.shutdown(self)
        except Exception:
            pass



================================================================================
FILE: .env.local
================================================================================

<MISSING: .env.local>



================================================================================
FILE: .env.example
================================================================================

<MISSING: .env.example>



================================================================================
FILE: scripts/net_check.py
================================================================================

import sys
from pathlib import Path
import json

# Ensure project root on sys.path
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# Import pipeline first to trigger dotenv loading inside pipeline module
from src.web.pipeline import WebConfig, _httpx_client  # type: ignore


def main():
    cfg = WebConfig()
    print(json.dumps({
        'ua': cfg.user_agent,
        'allow': cfg.sandbox_allow,
        'respect_robots': cfg.respect_robots,
        'allow_browser': cfg.allow_browser,
        'sandbox_allow_proxies': cfg.sandbox_allow_proxies,
        'http_proxy': cfg.http_proxy,
        'https_proxy': cfg.https_proxy,
        'all_proxy': cfg.all_proxy,
        'no_proxy': cfg.no_proxy,
    }, ensure_ascii=False))

    endpoints = [
        ("duckduckgo_api", "https://api.duckduckgo.com/", {"q": "test", "format": "json", "no_html": "1", "no_redirect": "1", "t": "net_check"}),
        ("duckduckgo_lite", "https://duckduckgo.com/lite/", {"q": "test"}),
        ("duckduckgo_html", "https://html.duckduckgo.com/html/", {"q": "test"}),
    ]

    with _httpx_client(cfg) as c:
        for name, url, params in endpoints:
            try:
                r = c.get(url, params=params, timeout=cfg.timeout_read)
                # Limit body preview
                body = (r.text or "")[:200].replace("\n", " ")
                print(json.dumps({
                    'name': name,
                    'url': str(r.request.url),
                    'status': r.status_code,
                    'content_type': r.headers.get('content-type', ''),
                    'preview': body,
                }, ensure_ascii=False))
            except Exception as e:
                print(json.dumps({
                    'name': name,
                    'url': url,
                    'error': str(e),
                }, ensure_ascii=False))


if __name__ == '__main__':
    main()



================================================================================
FILE: tests/test_web_pipeline_exhaustive.py
================================================================================

import os
import json
from types import SimpleNamespace

import pytest

from src.web.pipeline import run_research
import src.web.pipeline as pipeline_mod


@pytest.fixture(autouse=True)
def _env(monkeypatch, tmp_path):
    # Permissive profile
    monkeypatch.setenv("WEB_RESPECT_ROBOTS", "0")
    monkeypatch.setenv("WEB_HEAD_GATING", "0")
    monkeypatch.setenv("WEB_DEBUG_METRICS", "1")
    monkeypatch.setenv("SANDBOX_NET_ALLOW", "*")
    # Exclude Wikipedia from citations
    monkeypatch.setenv("WEB_EXCLUDE_CITATION_DOMAINS", "wikipedia.org")
    # Isolate cache dir per test
    monkeypatch.setenv("WEB_CACHE_ROOT", str(tmp_path / ".webcache"))
    yield


def _mk_search_result(title, url, snippet="", source="ddg", published=None):
    return SimpleNamespace(title=title, url=url, snippet=snippet, source=source, published=published)


def test_pipeline_end_to_end_discovery_only_wikipedia(monkeypatch, tmp_path):
    # 1) Search returns one wiki page and one external link
    # Patch the search function used inside the pipeline module

    wiki_url = "https://en.wikipedia.org/wiki/Redistricting_in_the_United_States"
    ext_url1 = "https://www.reuters.com/world/us/example-redistricting-article/"

    def fake_search(query, *, cfg=None, site=None, freshness_days=None):
        return [
            _mk_search_result("Wikipedia Redistricting", wiki_url, source="duckduckgo"),
            _mk_search_result("Reuters Coverage", ext_url1, source="duckduckgo"),
        ]

    monkeypatch.setattr(pipeline_mod, "search", fake_search, raising=True)

    # 2) fetch_url and extract_content
    import src.web.fetch as fetch_mod  # only for types; we patch pipeline_mod symbols
    import src.web.extract as extract_mod  # only for types
    import src.web.rerank as rerank_mod  # only for types
    import src.web.archive as archive_mod  # only for types

    def fake_fetch_url(url, *, cfg=None, robots=None, force_refresh=False, use_browser_if_needed=True):
        # Minimal fields used by pipeline
        return SimpleNamespace(
            ok=True,
            status=200,
            url=url,
            final_url=url,
            headers={"content-type": "text/html"},
            content_type="text/html",
            body_path=None,
            meta_path=None,
            cached=False,
            browser_used=False,
            reason=None,
        )

    monkeypatch.setattr(pipeline_mod, "fetch_url", fake_fetch_url, raising=True)

    def fake_extract(meta, *, cfg=None):
        url = meta.get("final_url")
        if url == wiki_url:
            # Include two external references plus a wiki self-link (filtered)
            md = (
                "This is a Wikipedia page about redistricting.\n"
                "Sources: https://www.cnbc.com/article-x and https://primary.example.com/ref1 \n"
                "See also https://en.wikipedia.org/wiki/Another_Page\n"
            )
            return SimpleNamespace(
                ok=True,
                kind="html",
                markdown=md,
                title="Wikipedia",
                date=None,
                meta={},
                used={},
                risk="LOW",
                risk_reasons=[],
            )
        # External page bodies
        md = "Key facts about lawsuits and timelines.\nLine 2 with details.\nLine 3 reference.\n"
        return SimpleNamespace(
            ok=True,
            kind="html",
            markdown=md,
            title="Primary Source",
            date=None,
            meta={},
            used={},
            risk="LOW",
            risk_reasons=[],
        )

    monkeypatch.setattr(pipeline_mod, "extract_content", fake_extract, raising=True)

    def fake_chunk_text(md: str):
        # 3 simple chunks (start/end lines are inferred by reranker stub)
        return [md]

    monkeypatch.setattr(pipeline_mod, "chunk_text", fake_chunk_text, raising=True)

    def fake_rerank(query: str, chunks, *, cfg=None, top_k=3):
        # Return highlights mapping to specific lines in the markdown
        return [
            {"id": "1", "score": 0.9, "start_line": 1, "end_line": 3, "highlights": [
                {"line": 1, "text": "Key facts about lawsuits"},
                {"line": 2, "text": "timelines"},
            ]}
        ]

    monkeypatch.setattr(pipeline_mod, "rerank_chunks", fake_rerank, raising=True)

    # 3) Disable archive network
    monkeypatch.setattr(pipeline_mod, "save_page_now", lambda *a, **k: {"archive_url": "", "timestamp": ""}, raising=True)
    monkeypatch.setattr(pipeline_mod, "get_memento", lambda *a, **k: {"archive_url": "", "timestamp": ""}, raising=True)

    out = run_research("verify discovery-only wikipedia", top_k=3, force_refresh=True)

    # Assertions
    assert "citations" in out and isinstance(out["citations"], list)
    # Wikipedia never appears in citations
    for cit in out["citations"]:
        assert "wikipedia.org" not in (cit.get("canonical_url") or "")
    # Primary sources should be present (either from search or expanded refs)
    urls = [c.get("canonical_url") for c in out["citations"]]
    # Accept common primary outlets from the test setup (Reuters) or references expanded from Wikipedia (primary.example.com, cnbc.com)
    primary_ok = any(any(dom in (u or "") for dom in ("reuters.com", "primary.example.com", "cnbc.com")) for u in urls)
    assert primary_ok

    # Debug metrics validate the path
    dbg = out.get("debug", {}).get("fetch", {})
    assert isinstance(dbg, dict)
    # Excluded wiki citations counted
    assert dbg.get("excluded", 0) >= 0
    # Wikipedia refs added recorded (non-strict: 0 is fine if rerank drops them)
    assert dbg.get("wiki_refs_added", 0) >= 0


def test_exclusion_policy_applies(monkeypatch, tmp_path):
    # Extend exclusion to example.com and ensure it is not cited
    monkeypatch.setenv("WEB_EXCLUDE_CITATION_DOMAINS", "wikipedia.org,example.com")

    import src.web.search as search_mod
    import src.web.extract as extract_mod
    import src.web.rerank as rerank_mod

    target = "https://example.com/article"
    monkeypatch.setattr(pipeline_mod, "search", lambda *a, **k: [_mk_search_result("Ex", target)], raising=True)
    monkeypatch.setattr(
        extract_mod,
        "extract_content",
        lambda meta, **k: SimpleNamespace(ok=True, kind="html", markdown="x\n", title="T", date=None, meta={}, used={}, risk="LOW", risk_reasons=[]),
        raising=True,
    )
    monkeypatch.setattr(rerank_mod, "chunk_text", lambda s: [s], raising=True)
    monkeypatch.setattr(rerank_mod, "rerank_chunks", lambda q, c, **k: [{"id": "1", "score": 0.8, "start_line": 1, "end_line": 1, "highlights": [{"line": 1, "text": "x"}]}], raising=True)

    # Bypass network in fetch_url
    import src.web.fetch as fetch_mod
    monkeypatch.setattr(pipeline_mod, "fetch_url", lambda url, **k: SimpleNamespace(ok=True, status=200, url=url, final_url=url, headers={}, content_type="text/html", body_path=None, meta_path=None, cached=False, browser_used=False, reason=None), raising=True)

    out = run_research("exclude example", top_k=1, force_refresh=True)
    urls = [c.get("canonical_url") for c in out.get("citations", [])]
    assert all("example.com" not in (u or "") for u in urls)



================================================================================
FILE: README.md
================================================================================

# Ollama Turbo CLI

A production-ready CLI application for interacting with gpt-oss:120b model through Ollama Turbo cloud service, featuring advanced tool calling capabilities and streaming responses.

## Features

- ðŸš€ **Ollama Turbo Integration**: Connect to gpt-oss:120b on datacenter-grade hardware
- ðŸ”§ **Advanced Tool Calling**: Weather, calculator, file operations, system info
- ðŸ“¡ **Streaming Responses**: Real-time response streaming with tool execution
- ðŸ§  **Multi-Round Tool Chaining (Streaming)**: Iteratively call multiple tools in one turn; enabled by default
- ðŸ’¬ **Interactive Mode**: Continuous conversation with history management
- ðŸ›¡ï¸ **Error Handling**: Robust retry logic and graceful error recovery
- âš™ï¸ **Configurable**: Environment variables and command-line options

## Prompting Strategy (Centralized)

Prompts are centralized in `src/prompt_manager.py` to ensure consistent guidance across the app:

- Initial system directive steers tool selection, chaining, and synthesis.
- Post-tool reprompt instructs the model to synthesize a final textual answer.
- Mem0 context is injected as a single, hygienic system block each turn.
  - The tool selection guide is auto-generated from discovered plugins (from `src/plugins/` and `tools/`), so newly added tools like `web_research` appear automatically.

Environment flags:

- `PROMPT_VERSION` (default: `v1`)
- `PROMPT_INCLUDE_TOOL_GUIDE` (default: `1`) â€” include a concise tool selection guide in the system prompt.
- `PROMPT_FEWSHOTS` (default: `0`) â€” append few-shot examples to nudge tool use.

## Plugin Architecture (Dynamic Tools)

Tools are now loaded dynamically via a plugin system.

- Built-in plugins live in `src/plugins/` and wrap the legacy implementations in `src/tools.py` for backward compatibility.
- Third-party plugins can be dropped into the top-level `tools/` directory without modifying core code.
- Plugins are validated using JSON Schema and auto-registered at runtime.
- Optional env: `OLLAMA_TOOLS_DIR` can point to additional plugin directories (supports multiple paths separated by your OS path separator).

### Plugin Contract

Each plugin is a Python module that must define:

- `TOOL_SCHEMA`: OpenAI-style function tool schema with fields `type: "function"`, `function.name`, `function.description`, and `function.parameters` (an object schema or `true` for no-arg tools).
- An implementation callable, provided as one of:
  - `TOOL_IMPLEMENTATION` (callable), or
  - a function named the same as `function.name`, or
  - a function named `execute`.

Minimal example placed at `tools/hello.py`:

```python
TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "hello",
        "description": "Return a friendly greeting message.",
        "parameters": {
            "type": "object",
            "properties": {"name": {"type": "string", "default": "world"}},
            "required": []
        }
    }
}

def execute(name: str = "world") -> str:
    return f"Hello, {name}! ðŸ‘‹"
```

### Backward Compatibility

- Existing tools in `src/tools.py` remain as-is and are exposed via built-in plugins in `src/plugins/`.
- The client now imports dynamic aggregates from `src/plugin_loader.py` to execute tools discovered at runtime.

### Validation and Safety

- Schemas and call arguments are validated using `jsonschema`. Ensure `jsonschema` is installed (see `requirements.txt`).
- Duplicate tool names are skipped with a warning; the first loaded plugin wins.

## Quick Start

### 1. Installation

```bash
git clone <repository-url>
cd ollama-turbo-cli
pip install -r requirements.txt
```

### 2. Get API Key

1. Visit [ollama.com/turbo](https://ollama.com/turbo)
2. Sign up for Ollama Turbo ($20/month)
3. Get your API key from [ollama.com/settings/keys](https://ollama.com/settings/keys)

### 3. Setup Environment

```bash
cp .env.example .env
# Edit .env and add your API key
```

### 4. Run

```bash
# Interactive mode
python -m src.cli --api-key YOUR_API_KEY

# Single message
python -m src.cli --api-key YOUR_API_KEY --message "What's 15 * 8?"

# With streaming
python -m src.cli --message "Weather in London" --stream
```

## Important: Security Defaults

By default, this repo is configured for an easy research/development experience, not a hardened production posture.

- Web tools (duckduckgo, wikipedia, web_fetch) run with liberal defaults via cfg:
  - Allowlist is wildcard `*` (all public hosts allowed)
  - Proxies are permitted
  - Robots.txt is not enforced; HEADâ€‘gating is off; larger byte caps and higher concurrency are used
  - Private/loopback/linkâ€‘local IPs are always blocked (SSRF defense)
- Shell sandbox (execute_shell) is disabled by default and has network egress OFF unless you opt in.

Harden these defaults before production by setting a restrictive allowlist, disabling proxies, enforcing robots, enabling HEADâ€‘gating, reducing max bytes/concurrency, and leaving shell networking off. See the â€œSecure Execution & Web Accessâ€ section below for a quick profile.

## Available Tools

### ðŸŒ¤ï¸ Weather Service

Get current weather for major cities worldwide.

```text
"What's the weather in Tokyo?"
```

### ðŸ§® Mathematical Calculator

Perform complex mathematical calculations with support for functions.

```text
"Calculate sin(pi/2) + sqrt(16)"
```

### ðŸ“ File Operations

List files and directories with filtering options.

```text
"List all Python files in the current directory"
```

### ðŸ’» System Information

Get comprehensive system information including CPU, memory, and disk usage.

```text
"Show me system information"
```

### ðŸ”Ž DuckDuckGo Search

Keyless web search for sources or quick facts (title, URL, snippet).

When to use: need external info discovery or recent data.
Tips: use focused queries (keywords or quoted phrase), keep `max_results` small (1â€“5).

### ðŸ“š Wikipedia Search

Keyless search of canonical topics via MediaWiki API (title, URL, snippet).

When to use: factual background and canonical entity pages.
Tips: focused term/entity; use `web_fetch` on a returned URL for details.

### ðŸŒ Web Fetch

Fetch specific HTTPS URLs through an allowlist proxy with SSRF protections.

When to use: read a specific page/API without credentials.
Tips: prefer HTTPS, small timeouts, minimal bytes. Only compact summaries are injected and may be truncated.

### âœ… Reliable Chat (Reliability API)

Grounded and validated answers via the internal Reliability API. Supports non-streaming and SSE streaming with a trailing summary event.

When to use: you want citations, validator checks, or consensus heuristics.
Flags: `ground`, `k`, `cite`, `check` (`off|warn|enforce`), `consensus` (int|bool; sent as integer), `engine`, `eval_corpus`.

Notes:

- If `consensus` is provided as `true` and `k` is unset, the client sends `consensus=3` by default.
- `k` controls retrieval breadth; `consensus` controls the number of model votes. They are coupled only for defaults.
- Fail-closed: if `ground=true` and `cite=true` but no sources resolve, the client returns a clear refusal and sets `summary.status = "no_docs"` with `grounded=false` and empty `citations`.
- Streaming `summary` contract: `status` âˆˆ {`ok`,`no_docs`,`timeout`,`http_error`}; `provenance` is `retrieval` when grounded else `none`.

Performance tips:

- When responses are slow: omit `consensus`, set `check="off"`, and leave `k` unset to minimize load.
- For complex queries, consider running `web_research` first to fetch sources, then synthesize with citations using your preferred workflow.

Reliable Chat plugin is currently disabled and unavailable.

Reliable Chat streaming aggregation is disabled.

#### Windows-safe curl (SSE) â€” quick smoke

```powershell
$env:BASE = "http://127.0.0.1:8000"

# Happy path (expect tokens + trailing summary with grounded:true when sources found)
curl.exe --no-buffer $env:BASE/v1/chat/stream -H "content-type: application/json" `
  -d '{"message":"When did Pearl Street Mall open?","ground":true,"cite":true,"k":4}'

# Fail-closed path (expect refusal text + summary.status=no_docs, grounded:false, citations:[])
curl.exe --no-buffer $env:BASE/v1/chat/stream -H "content-type: application/json" `
  -d '{"message":"who won the 2047 denver mayor race","ground":true,"cite":true}'
```

Notes:

- By default, auto-heuristics infer flags from the message (citations â†’ ground+cite, verify â†’ check=warn, consensus â†’ consensus+k).
- Configure base URL via `RELIABILITY_API_BASE` (defaults to `http://127.0.0.1:8000`).
- Requires `API_KEY` if the FastAPI server enforces it; upstream `OLLAMA_API_KEY` is forwarded when present.

### ðŸ”¬ Web Research

Multi-hop web research with citations. Orchestrates Planâ†’Searchâ†’Fetchâ†’Extractâ†’Chunkâ†’Rerankâ†’Citeâ†’Cache.

When to use: you need up-to-date facts from multiple sources and deterministic citations. Prefer over `web_fetch` when synthesizing across pages is needed.
Parameters: `top_k` (breadth, default 5), `site_include`/`site_exclude` (scope), `freshness_days` (recency), `force_refresh` (bypass caches).
Returns: compact JSON with results, quotes, page-mapped citations (for PDFs), and archive URLs.

## Usage Examples

### Interactive Mode

```bash
python -m src.cli --api-key sk-your-key-here

ðŸ‘¤ You: What's the weather in London and calculate 25 * 4?

ðŸ”§ Processing tool calls...
   1. Executing get_current_weather(city=London, unit=celsius)
      âœ… Result: Weather in London: Partly cloudy, 15Â°C, Humidity: 65%, Wind: 12 mph
   2. Executing calculate_math(expression=25 * 4)
      âœ… Result: Result: 25 * 4 = 100

ðŸ¤– Final response: Based on the results, London currently has partly cloudy weather at 15Â°C, and 25 multiplied by 4 equals 100.
```

### Single Message Mode

```bash
python -m src.cli --api-key sk-your-key --message "List files in /home/user/documents"
```

### Streaming Mode

```bash
python -m src.cli --message "Explain quantum computing" --stream
```

In streaming mode, the assistant can perform multiple tool rounds in a single turn and then synthesize a final textual answer. By default, up to 6 tool-call rounds are allowed before finalization.

## HTTP API (v1)

The project includes a versioned FastAPI server for programmatic access.

### Start the server

```bash
uvicorn src.api.app:app --reload
```

Feature flag: set `API_ENABLED=0` to disable. If `API_KEY` is set, clients must send `X-API-Key`.

### Endpoints

- `GET /health` â€” root health (FastAPI app)
- `GET /v1/health` â€” basic health
- `GET /v1/live` â€” liveness probe
- `GET /v1/ready` â€” readiness probe
- `POST /v1/chat` â€” non-streaming chat
- `POST /v1/chat/stream` â€” SSE streaming chat

### Headers

- `X-API-Key: <key>` â€” required if `API_KEY` is configured
- `Idempotency-Key: <uuid>` â€” optional; reserved for future de-duplication

### Request model (POST /v1/chat and /v1/chat/stream)

```json
{
  "message": "Hello",
  "options": {
    "tool_results_format": "string|object"
  }
}
```

`tool_results_format` controls the shape of tool results in responses. Default is `string` for backward compatibility; set `TOOL_RESULTS_FORMAT=object` or pass in `options` to get structured objects.

Reliability fields (all optional):

- `ground` (bool): Enable retrieval + grounding context injection
- `k` (int): General breadth parameter (retrieval top-k and/or consensus runs)
- `cite` (bool): Ask the model to include inline citations when grounded
- `check` ("off"|"warn"|"enforce"): Validator mode
- `consensus` (bool): Enable multi-run consensus (streaming returns trace-only summary)
- `engine` (string): Backend engine alias or URL
- `eval_corpus` (string): Eval corpus identifier for micro-eval

### Responses

Non-streaming (`/v1/chat`):

```json
{
  "content": "Hello there!",
  "tool_results": [
    "calc: 42"
  ]
}
```

With `tool_results_format=object`, `tool_results` becomes a list of objects:

```json
{
  "content": "Hello there!",
  "tool_results": [
    {"tool":"calc","status":"ok","content":42,"metadata":{"args":{"x":40,"y":2}},"error":null}
  ]
}
```

Streaming (`/v1/chat/stream`): Server-Sent Events (SSE). Events are sent as `data: {json}` lines.

- Token chunks: `{ "type": "token", "content": "..." }`
- Final event: `{ "type": "final", "content": "...", "tool_results": [...] }`

Tool-calls or stream errors are silently finalized via a non-streaming fallback, and only a final event is emitted (no tokens), matching CLI behavior.

### Web Research Example

```bash
python -m src.cli --message "Research: What are the latest results on Llama 3.2 vision benchmarks? Include citations."

ðŸ”§ Processing tool calls...
   1. Executing web_research(query="latest Llama 3.2 vision benchmarks", top_k=5, freshness_days=365)
      âœ… Result: { "results": [ { "title": "...", "url": "https://...", "quote": "...", "citation": { "page": 3, "line": 120 } } ], "archives": ["https://archive.is/..." ] }

ðŸ¤– Final response: Summary with quotes and numbered citations.
```

## Configuration

### Environment Variables

- `OLLAMA_API_KEY`: Your Ollama API key
- `OLLAMA_MODEL`: Model name (default: gpt-oss:120b)
- `OLLAMA_HOST`: API endpoint (default: <https://ollama.com>)
- `RELIABILITY_API_BASE`: Reliability router base URL (default: <http://127.0.0.1:8000>)
- `MAX_CONVERSATION_HISTORY`: Maximum messages to keep (default: 10)
- `STREAM_BY_DEFAULT`: Enable streaming by default (default: false)
- `LOG_LEVEL`: Logging level (default: INFO)
- `REASONING`: Default reasoning effort (low|medium|high, default: `high`)
- `PROMPT_VERSION`: Prompt pack version (default: `v1`)
- `PROMPT_INCLUDE_TOOL_GUIDE`: Include tool selection guide in system prompt (default: `1`)
- `PROMPT_FEWSHOTS`: Append few-shot guidance to the system prompt (default: `0`)
- `MULTI_ROUND_TOOLS`: Enable multi-round tool chaining (default: `true`). In non-streaming mode, the client always finalizes with a textual answer after the first tool round for compatibility.
- `TOOL_MAX_ROUNDS`: Maximum number of tool-call rounds in streaming mode before finalizing (default: `6`).

#### Mem0 (optional)

- `MEM0_API_KEY`: Mem0 API key (enables long-term memory)
- `MEM0_USER_ID`: Memory namespace user id (default: "Braden")
- `MEM0_AGENT_ID`: Optional agent id for tagging
- `MEM0_APP_ID`: Optional app id for tagging
- `MEM0_ORG_ID`: Optional organization id
- `MEM0_PROJECT_ID`: Optional project id
- `MEM0_VERSION`: Preferred Mem0 API version (default: "v2"). The client prefers v2 and gracefully falls back if the SDK doesn't support the `version` parameter.

Mem0 runtime knobs (advanced):

- `MEM0_ENABLED`: Enable/disable Mem0 (default: `1`)
- `MEM0_DEBUG`: Enable Mem0 debug logs (default: `0`)
- `MEM0_MAX_HITS`: Max memories injected per turn (default: `3`)
- `MEM0_SEARCH_TIMEOUT_MS`: Time-boxed search timeout in ms (default: `200`)
- `MEM0_TIMEOUT_CONNECT_MS`: HTTP connect timeout in ms (default: `1000`)
- `MEM0_TIMEOUT_READ_MS`: HTTP read timeout in ms (default: `2000`)
- `MEM0_ADD_QUEUE_MAX`: Background add queue size (default: `256`)
- `MEM0_BREAKER_THRESHOLD`: Consecutive failures to trip circuit breaker (default: `3`)
- `MEM0_BREAKER_COOLDOWN_MS`: Breaker cooldown in ms (default: `60000`)
- `MEM0_SHUTDOWN_FLUSH_MS`: Shutdown flush timeout in ms (default: `3000`)
- `MEM0_SEARCH_WORKERS`: Tiny thread pool size for Mem0 searches (default: `2`)

Local OSS/OpenMemory mode (no cloud key required):

- `MEM0_USE_LOCAL`: Set to `1` to enable local OpenMemory/Mem0 OSS client
- Optional config sources (first found wins):
  - `MEM0_LOCAL_CONFIG_PATH`: Path to a JSON config file
  - `MEM0_LOCAL_CONFIG_JSON`: Inline JSON string with config
- If no explicit config is provided, a sensible default is synthesized from env:
  - `MEM0_OLLAMA_BASE_URL`: Base URL for local Ollama (default: resolved client host)
  - `MEM0_LLM_PROVIDER`/`MEM0_LLM_MODEL` (default: `ollama` / current model)
  - `MEM0_EMBEDDER_PROVIDER`/`MEM0_EMBEDDER_MODEL` (default: `ollama` / `nomic-embed-text`)
  - `MEM0_VECTOR_PROVIDER` (default: `qdrant`)
  - `MEM0_VECTOR_HOST`/`MEM0_VECTOR_PORT` (default: `127.0.0.1` / `6333`)
  - `MEM0_HISTORY_DB_PATH` for local SQLite history (optional)
  - Graph store (optional): `MEM0_GRAPH_PROVIDER`, `MEM0_GRAPH_URL`, `MEM0_GRAPH_USERNAME`, `MEM0_GRAPH_PASSWORD`

Notes:

- Remote/platform mode remains fully supported when `MEM0_API_KEY` is provided.
- The client auto-detects which Mem0 implementation is present and adapts API call signatures for search/get_all across versions.

### Command Line Options

```bash
--api-key        Ollama API key
--model          Model name (default: gpt-oss:120b)
--message        Single message mode
--stream         Enable streaming
--reasoning      Reasoning effort: low|medium|high (default: high)
--no-tools       Disable tool calling
--log-level      Set logging level
--version        Show version
--help           Show help
```

## Interactive Commands

While in interactive mode:

- `quit` or `exit`: Exit the application
- `clear`: Clear conversation history
- `history`: Show conversation history
- `/mem ...`: Manage memories (list|search|add|get|update|delete|clear|link|export|import)
- `Ctrl+C`: Exit gracefully

## Mem0 Memory System

Mem0 is integrated as the long-term memory store. It is optional and enabled when `MEM0_API_KEY` is present (remote) or `MEM0_USE_LOCAL=1` is set (local OSS).

- **Initialization**: Loaded silently from environment variables. No runtime prompts.
- **Status banner**: Interactive mode shows `Mem0: enabled (<mode>, user: <id>)` or guidance to enable local/remote.
- **Injection hygiene**: At most one "Relevant information:" system block is injected per turn before your message; previous injection blocks are removed each turn. Local conversation history is capped at 10 turns.
- **Natural language intents**: You can type phrases like:
  - "remember ...", "forget ...", "update X to Y", "list/show memories", "link `<id1>` `<id2>`", "search memories for ...", or "what do you know about me".
- **/mem commands**: `list`, `search <q>`, `add <text>`, `get <id>`, `update <id> <text>`, `delete <id>`, `clear`, `link <id1> <id2>`, `export [path.json]`, `import <path.json>`.
- **Export/Import**: Exports to `mem0_export_*.json` including `id`, `memory`, `metadata`, `created_at`, `updated_at`. Git ignores these files by default.
- **Resilience**: All Mem0 operations are wrapped in try/except; failures never block chat. A one-time notice is shown if Mem0 is unavailable; details are logged at DEBUG level.
- **API versioning**: Prefers Mem0 v2 endpoints (passes `version="v2"`), and falls back automatically if the SDK does not support the `version` kwarg. Configure preferred version via `MEM0_VERSION`.
- **Filters**: Minimal filters are used for recall (`user_id` only) for list/search/export until recall is proven.

## Secure Execution & Web Access

- **Shell tool is disabled by default**: Commands run only inside a locked-down sandbox when enabled. Set `SHELL_TOOL_ALLOW=1` and keep an explicit `SHELL_TOOL_ALLOWLIST` (prefix match) for safe commands like `git status`, `ls`, `cat`.

- **Confirmation prompts**: In TTY, `execute_shell` shows a one-line preview and requires confirmation unless `SHELL_TOOL_CONFIRM=0`.

- **Sandbox guarantees**: CPU/mem/pids/disk/time limits, read-only project mount at `/project`, tmpfs workspace, no host env (only `env_vars` pass-through). Windows requires Docker Desktop/WSL2. If Docker is unavailable, execution fails closed with a clear message.

- **Web tools (cfg-first defaults are liberal)**: By default, the web client uses a wildcard allowlist (`*`) and permits proxies to make research easy. Private/loopback IPs remain blocked. If `SANDBOX_*` envs are set, they override cfg at runtime for strict enforcement.

  - To harden: set a narrow allowlist, disable proxies, enforce robots, enable HEADâ€‘gating, and reduce byte/concurrency limits.
  - To keep permissive behavior: do not set `SANDBOX_*` envs and rely on cfg defaults, or explicitly set `SANDBOX_NET_ALLOW=*`.

- **Summarization before injection**: Tool outputs are summarized and capped (`TOOL_CONTEXT_MAX_CHARS`, default 4000). Full logs and payloads are kept under `.sandbox/sessions/` and `.sandbox/cache/` and are not shared with the model.

- **Mem0 safety**: Outputs from `execute_shell` and sensitive `web_fetch` responses are flagged and not persisted to Mem0.

- **Caching & rate limits**: On-host HTTP cache (TTL 10m, LRU ~200MB) accelerates repeat fetches. Per-host rate-limits prevent abuse.

### Enable a one-off command safely

1. Export envs:
   - `SHELL_TOOL_ALLOW=1`
   - Optional: extend `SHELL_TOOL_ALLOWLIST` with a safe prefix (e.g., `git show`)

2. Run your prompt. When asked, confirm the preview.

### Hardened web profile (recommended for production)

```bash
# Allow only specific hosts (supports wildcards)
export SANDBOX_NET=allowlist
export SANDBOX_NET_ALLOW=api.github.com,*.wikipedia.org

# Enforce HTTPS and disable proxies
export SANDBOX_ALLOW_HTTP=0
export SANDBOX_ALLOW_PROXIES=0

# Tighten fetch behavior
export WEB_RESPECT_ROBOTS=1
export WEB_HEAD_GATING=1
export WEB_MAX_DOWNLOAD_BYTES=1048576   # 1 MB
export WEB_MAX_CONNECTIONS=8
export WEB_PER_HOST_CONCURRENCY=2
```

To keep the permissive research profile, omit `SANDBOX_*` envs (cfg defaults apply) or set `SANDBOX_NET_ALLOW=*` explicitly.

### Environment knobs

See `.env.example` for all variables, including:

- `SHELL_TOOL_ALLOW`, `SHELL_TOOL_CONFIRM`, `SHELL_TOOL_ALLOWLIST`, `SHELL_TOOL_MAX_OUTPUT`, `SHELL_TOOL_ROOT`
- `SANDBOX_NET`, `SANDBOX_NET_ALLOW`, `SANDBOX_ALLOW_HTTP`, `SANDBOX_BLOCK_PRIVATE_IPS`, `SANDBOX_MAX_DOWNLOAD_MB`, `SANDBOX_RATE_PER_HOST`, `SANDBOX_HTTP_CACHE_TTL_S`, `SANDBOX_HTTP_CACHE_MB`
- `TOOL_CONTEXT_MAX_CHARS`

### Tool result payloads

`TOOL_RESULTS_FORMAT` controls how tool outputs are fed back to the model during the reprompt after tool execution. This applies to both streaming and nonâ€‘streaming flows.

- `string` (default): adapters receive `list[str]` (stringified tool results). If the adapter raises, the client injects one `'tool'` message per tool call with `tool_call_id` and string content (or a single aggregated `'tool'` message as a last resort). CLI prints remain unchanged.

- `object`: adapters receive `list[dict]` (structured tool results). If the adapter raises, the client falls back to the same string injection behavior described above.

Note: streaming intentionally passes `options=None` to the adapter to preserve behavior, while nonâ€‘streaming passes any `adapter_opts` established earlier in the call path.

JSON string arguments for tools: When a tool call provides its `function.arguments` as a JSON string (OpenAI-style), the runtime automatically parses it to a dict before invoking the tool implementation.

## Error Handling

The application includes robust error handling:

- **Network Issues**: Automatic retry with exponential backoff
- **API Errors**: Clear error messages and recovery
- **Tool Failures**: Graceful degradation with error reporting
- **Invalid Input**: Input validation and helpful suggestions
- **Streaming fallback**: Streaming errors are not shown in CLI; the client silently falls back to non-streaming and logs details at DEBUG level only.

## Requirements

- Python 3.8+
- Ollama Turbo subscription ($20/month)
- Internet connection for API calls
- 4GB+ RAM recommended

## License

MIT License - see LICENSE file for details.

## Support

For issues and questions:

1. Check the troubleshooting section
2. Review error logs with `--log-level DEBUG`
3. Verify API key and subscription status
4. Open an issue on GitHub

## Troubleshooting

### Common Issues

#### API Key Errors

- Ensure your API key is correctly set in `.env` or via `--api-key`
- Verify the key is active at [ollama.com/settings/keys](https://ollama.com/settings/keys)

#### Connection Issues

- Check your internet connection
- Verify Ollama Turbo service status
- Try with `--log-level DEBUG` for detailed error messages

#### Tool Execution Failures

- Some tools require specific permissions (e.g., file operations)
- System info tool requires `psutil` package
- Weather data is available for major cities only

#### Performance

- Streaming mode provides better perceived performance
- Tool calls may add latency for complex operations
- Consider adjusting `MAX_CONVERSATION_HISTORY` for memory usage

## Development

### Project Structure

```text
ollama-turbo-cli/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py         # Package initialization
â”‚   â”œâ”€â”€ cli.py              # Main CLI application
â”‚   â”œâ”€â”€ client.py           # Ollama Turbo client wrapper
â”‚   â”œâ”€â”€ prompt_manager.py   # Centralized prompt construction and configuration
â”‚   â”œâ”€â”€ tools.py            # Legacy tool implementations (wrapped by plugins)
â”‚   â”œâ”€â”€ plugin_loader.py    # Dynamic plugin discovery and validation
â”‚   â””â”€â”€ plugins/            # Built-in tool plugins (wrapping legacy tools)
â”‚   â””â”€â”€ utils.py            # Utility functions
â”œâ”€â”€ tools/                  # Third-party plugins (auto-discovered)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ setup.py               # Package setup configuration
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ .env.example           # Environment variables template
â””â”€â”€ LICENSE                # MIT license
```

### Web Research Pipeline (Internals)

- **Entry point tool**: `src/plugins/web_research.py` calls `src/web/pipeline.py:run_research()`.
- **Modules under** `src/web/` are internal (not tools). They implement the pipeline stages and safety:
  - Plan â†’ Search â†’ Fetch â†’ Extract â†’ Chunk â†’ Rerank â†’ Cite â†’ Cache
  - Fetch enforces robots.txt crawl-delay and per-host rate/concurrency guards.
  - Extract handles HTML and PDF; citations include exact quotes and PDF page mapping when available.
  - Caching and content-hash dedupe keep repeated runs fast; `force_refresh=true` bypasses caches.
- **When to use**: multi-source questions needing fresh facts and deterministic citations. Prefer over `web_fetch` when synthesizing across multiple pages.
- **Parameters** (tool): `top_k`, `site_include`, `site_exclude`, `freshness_days`, `force_refresh`.
- **Output**: compact JSON with results, citations, and archive URLs suitable for downstream summarization.

### Running Tests

```bash
# Test weather tool
python -m src.cli --message "What's the weather in Paris?"

# Test calculator
python -m src.cli --message "Calculate sqrt(144) + sin(pi/2)"

# Test file operations
python -m src.cli --message "List Python files in the current directory"

# Test system info
python -m src.cli --message "Show system information"

# Test multiple tools
python -m src.cli --message "Weather in London and calculate 15 * 8"

# Run unit tests (includes plugin system tests)
pytest -q
```

### Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## Version History

### v1.1.0 (Current)

- Reliability mode: grounding, citations, validator (off|warn|enforce), consensus (k)
- Streaming reliability with trailing summary (grounded, citations, validator, consensus)
- Heuristic auto-flags based on message intent; CLI/API flags plumbed end-to-end
- Hardened plugin validation and None-stripping for tool args
- Version bump and README/documentation updates

### v1.0.0

- Initial release with gpt-oss:120b support
- Four built-in tools: weather, calculator, files, system
- Streaming and non-streaming modes
- Interactive and single-message modes
- Comprehensive error handling and retry logic


